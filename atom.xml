<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>楸涵の小窝</title>
  
  <subtitle>祝你天天开心！</subtitle>
  <link href="https://himoqiuhan.github.io/atom.xml" rel="self"/>
  
  <link href="https://himoqiuhan.github.io/"/>
  <updated>2024-07-28T13:37:53.031Z</updated>
  <id>https://himoqiuhan.github.io/</id>
  
  <author>
    <name>楸涵</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>技术美术百人计划学习笔记（美术4.5 特效规范及拆分实现）</title>
    <link href="https://himoqiuhan.github.io/2024/07/28/Notes-TA100-A4510/"/>
    <id>https://himoqiuhan.github.io/2024/07/28/Notes-TA100-A4510/</id>
    <published>2024-07-28T09:20:57.000Z</published>
    <updated>2024-07-28T13:37:53.031Z</updated>
    
    <content type="html"><![CDATA[<h2 id="特效制作流程">特效制作流程</h2><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20240728172355474.png" alt="一般特效的制作流程"></p><h2 id="特效文件规范">特效文件规范</h2><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20240728172819822.png" alt="资产结构"></p><p>资源规范视游戏体量而定</p><ul><li>贴图：分辨率、mipmaps是否关，闭节约内存（默认UI和序列帧贴图关闭mipmap）、限制贴图总数并在制作时考虑贴图是否可以复用、提高贴图的填充率以降低overdraw、避免出现相同或相似的贴图</li><li>模型：限制面数、总数，关闭mesh的read/write，并且DCC软件输出时关闭不需要的内容（动画/灯光/贴图等）</li></ul><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20240728173024896.png" alt="资源规范"></p><p>基本上是&lt;类型_角色名称_动画名称_技能名称&gt;的规范，对于贴图如果是4通道的会加一个_a，如果是3通道则不加_a，如果是法线会加入_n</p><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20240728173419465.png" alt="命名规范"></p><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20240728173733281.png" alt="路径规范"></p><h2 id="特效制作规范">特效制作规范</h2><p>大部分是基于性能需求的规范，会考虑不同运用场景进行区分，并且主要关注粒子发射器数量、粒子数量、Batches数量以及Overdraw的情况</p><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20240728173932957.png" alt=""></p><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20240728173949426.png" alt=""></p><h2 id="特效拆分与程序调用原理">特效拆分与程序调用原理</h2><p>特效基本逻辑可以拆分为发生前、进行时、结束后</p><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20240728174121342.png" alt="特效的基本逻辑"></p><p>连入游戏时会基于特效的实际运用情况进行合并和拆分，以下是皮卡丘十万伏特的例子。其中蓄力和释放会基于是否循环来决定是否做到一个特效之中</p><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20240728174455690.png" alt="特效拆分案例:皮卡丘特效"></p><p>技能的调用分为三个情景：技能编辑器、代码控制的特效、以及一些特殊类型的特效表现</p><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20240728174704738.png" alt=""></p>]]></content>
    
    
    <summary type="html">介绍特效制作流程、特效规范，以及程序调用特效的实现，其中规范这一块讲的很好</summary>
    
    
    
    <category term="Notes" scheme="https://himoqiuhan.github.io/categories/Notes/"/>
    
    
    <category term="TA" scheme="https://himoqiuhan.github.io/tags/TA/"/>
    
    <category term="美术理论" scheme="https://himoqiuhan.github.io/tags/%E7%BE%8E%E6%9C%AF%E7%90%86%E8%AE%BA/"/>
    
    <category term="百人计划" scheme="https://himoqiuhan.github.io/tags/%E7%99%BE%E4%BA%BA%E8%AE%A1%E5%88%92/"/>
    
  </entry>
  
  <entry>
    <title>《響け！ユーフォニアム》杂谈</title>
    <link href="https://himoqiuhan.github.io/2024/02/13/Anima-SoundEuphonium/"/>
    <id>https://himoqiuhan.github.io/2024/02/13/Anima-SoundEuphonium/</id>
    <published>2024-02-13T06:14:39.000Z</published>
    <updated>2024-07-27T15:13:59.650Z</updated>
    
    <content type="html"><![CDATA[<p>难得的春节休假，补了补《京吹》的TV版一二季，虽说这段时间也补了很多很多番，但只有《吹响吧！上低音号》让我再次萌生了想要为它写下一点东西的念头。“看《京吹》”的种子是从<a href="https://space.bilibili.com/181766934">白孔雀Aper</a>老师的一期<a href="https://www.bilibili.com/video/BV1kc411c7cS">编排分享</a>中种下的，加之刚看完《玉子市场》，被京阿尼式的温暖所包裹，我便决定开《京吹》的新坑。和之前的杂谈一样，我只是说我自己想说的，带有强烈的主观因素与当时心境的影响，或许会有剧透内容，也无法保证想法不会与观者冲突，但是我所说的都是看完之后一腔热血时自己的所思所想。如果能有人看到这篇杂谈，单是这么设想一下我就会很开心，如果因为我的杂谈去看了《京吹》，甚至愿意来与我讨论这部动画，那便是无上的喜悦！</p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/SoundEuphonium08.jpg" style="zoom:50%;" /><p>先从我最喜欢的人设开始谈起吧，一部番最能吸引我的点就是他的人设树立了，因为人设会伴随着故事，一个好的人设需要好的故事，以及好的讲述手法，才能让观众所喜欢。不过，太多深入的内容，我没有专门学习过，只是说一说自己看番的感受。《京吹》里我最喜欢的是久美子，尤其是喜欢她因为好奇而去了解一起，但又因为不想使任何人受伤，在自己和所有人之间划出一条明确界限的性格，以及因此而带来的懒洋洋的、随随便便的声音。</p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/SoundEuphonium05.jpg" style="zoom:50%;" /><p>TV一二季中，故事的大矛盾有三个，一是小号独奏者的决定，二是霙对希美的心结，三是明日香与母亲的矛盾。前两个矛盾久美子都没有直接的参与，但都是距离事件发生最近的旁观者，只是从侧面的角度或多或少地帮助了问题的解决。但是在这一年当中，久美子也逐渐成长着，学习着用真心和他人交流。在小号独奏决定的事件中，久美子只是给予了丽奈支持；在霙心结解开的过程中，久美子就明确地向想要回到社团地希美表明了自己会帮助她，虽然最后问题真正的解决还是靠的霙和希美互相吐露心声，但是慢慢地表明久美子开始愿意将自己置身于风波之中，去帮助解决问题了。</p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/SoundEuphonium06.jpg" style="zoom:50%;" /><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/SoundEuphonium07.jpg" style="zoom:50%;" /><p>而第三次矛盾，也是第二季的最大矛盾，久美子先只是受大家的期待，去尝试让明日香回到社团一同参加全国大赛。但是在周围多重事件的发生，尤其得知姐姐麻美子其实十分后悔自己高中时装成熟而放弃了想要走的路之后，突破了自己心里的障碍，一把手拉回了明日香。</p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/SoundEuphonium10.jpg" style="zoom:50%;" /><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/SoundEuphonium11.jpg" style="zoom:50%;" /><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/SoundEuphonium09.jpg" style="zoom:50%;" /><p>说到障碍，也顺势聊一下久美子在一开始的心理吧。故事中也很多次地提及，久美子是因为向往姐姐，才决定学习的吹奏乐。起初她想要和姐姐一样，吹长号，但是因为吹长号的人太多，在老师的盛情建议下，她顺应需求选择了吹上低音号。初中时也是因为没有人吹上低音号，她便继续了上低音号的吹奏。高中入学时，她提到想要来到一个没人认识她的地方，重新开始一段旅途，在第一次走进吹奏部时，想要隐瞒自己的经历，去吹长号，因为姐姐是吹长号的，因为这个时候久美子还是为了对姐姐的向往开始吹奏。</p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/SoundEuphonium12.jpg" style="zoom:50%;" /><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/SoundEuphonium13.jpg" style="zoom:50%;" /><p>随着经历的积累，和朋友们、队友们一同演出，尤其是和明日香的接触，她一步一步察觉到了自己不仅仅因为姐姐而喜欢吹奏月，还发现自己很喜欢上低音号，很喜欢和大家一起吹奏的时光。同时，在最后的告别上，她吐露了自己对明日香的心声，自此，成长了一年，足以将明日香传承下来的东西进一步传承下去的久美子便养成了。</p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/SoundEuphonium01.png" alt="" style="zoom:50%;" /><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/SoundEuphonium02.png" style="zoom:50%;" /><p>好的故事就是这样，好的人设的树立，需要无时无刻，通过精心打磨的细节来告诉观众，她是一个怎样的人，她曾经是一个怎样的人，她正在经历着什么样的成长，把一个角色全面地展现在观众面前，让观众知道他在见证着、陪伴着角色的成长，这样塑造出来的角色，在足够细腻到能够融入观众的内心。像是丽奈这样的故事次核心角色，也同样是用这样的概念塑造起来的。</p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/SoundEuphonium04.jpg" style="zoom:50%;" /><p>再有一点我很喜欢的是，《京吹》故事的推进节奏。尤其是在第二部中，后期除去明日香的大矛盾这一条主线外，还有麻美子、丽奈的暗线在发展，制作组对这些内容处理得很恰当，整个叙事的节奏很让人舒适。最主要的原因在于，不同线的时间之间的切换过渡很自然，一条线的故事没有在被关注时，他并不是停滞不前的，而是在慢慢发展，同时为了不让人觉得事件发展得突然，会在当前叙述线的空闲段，用少量的笔墨，描绘其他线故事推进的重点内容。同时，一件事情的发生、一个矛盾的解决也恰如其分地推动了另一个矛盾的处理，让前后有了连贯性。最明显的其实就是刚刚提到的全国大赛前的剧情发展，各个矛盾处理得不会让人觉得突兀，让人觉得是顺应着故事所发展的，或者也是能够让人知道，这是这样的角色所做出的适合于他的决定。</p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/SoundEuphonium14.jpg" style="zoom:50%;" /><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/SoundEuphonium15.jpg" style="zoom:50%;" />]]></content>
    
    
    <summary type="html">そして 次の曲が始まるのです</summary>
    
    
    
    
    <category term="动漫" scheme="https://himoqiuhan.github.io/tags/%E5%8A%A8%E6%BC%AB/"/>
    
    <category term="感想" scheme="https://himoqiuhan.github.io/tags/%E6%84%9F%E6%83%B3/"/>
    
  </entry>
  
  <entry>
    <title>【Unity】仿原神渲染2.0技术文档-角色渲染篇</title>
    <link href="https://himoqiuhan.github.io/2023/12/07/Projects-GenshinLikeRenderingInURP2-Avatar/"/>
    <id>https://himoqiuhan.github.io/2023/12/07/Projects-GenshinLikeRenderingInURP2-Avatar/</id>
    <published>2023-12-07T13:17:37.000Z</published>
    <updated>2024-07-27T15:13:59.657Z</updated>
    
    <content type="html"><![CDATA[<h1>【Unity】URP 中的仿原神渲染2.0-角色篇</h1><h2 id="整体效果展示">整体效果展示</h2><p>【【卡通渲染/MMD】五等分の気持ち】 <a href="https://www.bilibili.com/video/BV1ew41187H5/">https://www.bilibili.com/video/BV1ew41187H5/</a></p><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20231204162907101.png" alt="夜间、白天及加入Bloom后的效果"></p><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/GLR2_Character_GeneralShow.gif" alt=""></p><blockquote><p>代码仓库链接：<a href="https://github.com/himoqiuhan/URPToonRenderReconstruction">https://github.com/himoqiuhan/URPToonRenderReconstruction</a></p></blockquote><h2 id="0-前言">0 前言</h2><p>对于卡通渲染，尤其是赛璐璐风格的卡通渲染的简介和精髓，其实很多大佬都已经写明了，所以我不进行系统的概述。</p><p>每个人心中都有属于自己的卡通渲染，之前和其他同学交流的时候，我对卡通角色渲染的浅薄理解是，好看的卡通角色效果，两分靠建模、五分靠贴图、三分靠渲染，并且需要整个流程中所有人员的相互协调处理。这只是个人的主观认知，其中的每一步都十分重要，都要融入设计师自己的心意，我这样说只是想要进一步强调贴图对于一个角色的重要程度。除去面部的特殊处理外，其他区域只是用简单的二分光照所获得的效果如下：</p><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20231202192854842.png" alt="只使用BaseColor和二分阴影得到的效果"></p><p>整体的效果还是很好看的，在此基础上，好的渲染能让角色锦上添花，接下来我就一步一步分析一下我角色渲染的处理思路。在这里要感谢一下其他大佬复刻后的无私分享，文末有我学习参考的大佬们的文章。</p><p>以下流程中我就以心海为例进行我还原过程中的处理思路的分享。</p><h2 id="1-漫反射">1 漫反射</h2><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20231204163137322.png" alt="处理完基础漫反射后的效果" style="zoom: 67%;" /><h3 id="1-1-二分阴影和原神的Ramp">1.1 二分阴影和原神的Ramp</h3><p>说到赛璐璐风格的卡通渲染，最容易让人想到的就是大色块、二分阴影和描边</p><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20231202200129315.png" alt="为什么会出现千咲？因为刚看完来自风平浪静的明天，以及很喜欢千咲，夹带私货！"></p><p>通常情况下，最直接的二分阴影可以通过step处理NdotL得到，而原神中则是通过<strong>Ramp贴图</strong>来实现的二分效果。基于我之前学习的知识，Ramp一开始是用于高效模拟皮肤SSS效果的（如果说错了还请大家指正），不过随着使用德越来越多，我认为Ramp图可以理解为自定义一系列的ColorTint，以光照计算得到的数据作为采样贴图的U轴坐标，采样不同光照情况下的调色颜色，用以混合BaseColor，得到最终的光影表现，通过Ramp图可以<strong>实现对光影颜色更便利、更美术可控的控制</strong>。基于此，原神通过对Ramp图的二分处理，一方面实现了可由美术自定义颜色的明暗区域二分，另一方面，因为二分效果需要像素之间明显的色阶变化，所以可以用很低分辨率的图片去存储Ramp信息，横向只有256个像素，多少也是省了一些存储和内存</p><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20231202202447174.png" alt="心海身体部分所用到的Ramp图的明暗区分部分"></p><p>从上面的截图可以看出，相隔不远的区域之间有明显的色阶变化，基于此就可以实现如下这样更具风格化的明暗区域二分效果，同时我也很喜欢明暗区域二分区域上那条暖色的分界线。</p><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20231202203900274.png" alt="插画风格的明暗交界处"></p><h3 id="1-2-原神ilm贴图GA通道和Ramp的采样">1.2 原神ilm贴图GA通道和Ramp的采样</h3><p>通常情况我们看到的Ramp图都是一条的，但是为了更加贴近插画的风格，以及更高的美术可控性，原神对Ramp图进行了进一步的升级，实现身体和头发各自<strong>五个区域</strong>、白天和黑夜<strong>两个状态</strong>下更丰富的光影细节效果，所以在上面Ramp的截图中，可以发现原神的一张Ramp图内实际存有10个有关光影变化的颜色信息。</p><p>为了<strong>区分不同光影效果的区域</strong>（即不同Ramp作用的区域、原神Ramp贴图采样的V轴信息），我们需要配合使用一张带有区域划分信息的贴图，通过贴图中的灰度值，来偏移采样RampTexture时的V轴坐标。原神把这个灰度信息整合到了ilmTexture的A通道，ilm这个说法最早源自GGX，不过原神中似乎命名为lightmap，为了区分BakeLight的lightmap，我在本文中暂且使用ilmTexture来称呼。</p><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20231202210110109.png" alt="心海ilmTexture的A通道"></p><p>可以看到，ilmTexture的A通道存储了不同的灰度值，这个值便是用于区分不同区域的信息。但是，在我的平台使用时，对V轴信息的处理有一些特殊，Ramp无法正常出效果的问题困扰了我很久，后来也是一遍一遍地摸索，发现了问题所在：首先，原神的Ramp采样是基于左上角为(0，0)的，原因猜测是网上拿到的Ramp资源是基于PC截帧获取的，DX平台默认的贴图是以左上角为UV空间的原点，而Unity中默认的UV空间原点是左下角，所以需要在Y轴上做翻转；其次，有些角色其实并没有用满10个Ramp区域，会出现如同上方截图那样的留白区域，以及ilmTexture.a存储的A通道的信息也存在一些特殊性，所以基于ilm贴图的A通道和Ramp图的适配性，以及结合游戏内的表现，我加入了一个_RampCount来对ilm贴图A通道读取Ramp列序号的信息进行缩放，使得ilmTexture的A通道为0和1的区域分别是有效Ramp的最上方和最下方，不会采样到纯白的无效区域（像我这样的处理方式或许也还原得不对，但是他似乎刚刚好让我能成功的用到五个角色上面，以及如果要我自己去做Ramp，我还是会更清晰地用更准确的灰度去控制区域，不需要加入缩放的处理）</p><p>相比于此，<strong>白天和黑夜两个状态</strong>的处理就简单了很多，通过一个toggle控制使用白天或黑夜的状态，加上如上处理后的V轴信息，然后再映射回0-1区间即可。代码如下：_UseCoolShadowColorOrTex即为toggle对应的参数，因为toggle获得的值直接是0或1，所以两个值乘0.5即可映射回0-1；同时减去一个0.001防止采样到像素的边缘。</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">rampUV.y = 1.0 - saturate(ilmTexCol.a) / _RampCount * 0.5 + _UseCoolShadowColorOrTex * 0.5 - 0.001;</span><br></pre></td></tr></table></figure><p>前文也提到，Ramp贴图的U轴是光照计算得到的数据。基于对Ramp图的观察可以发现，明暗区分的分界线是比较靠Ramp图右侧的，而贴图UV坐标是再0-1之间的，所以基于此我们可以用halfLambert来作为U轴的坐标来对Ramp进行采样。</p><p>同时，插画中为了增加角色的表现效果，会加入主观的阴影，一方面能处理衣服布料相互遮挡的关系，一方面可以塑造诸如衣服褶皱这样的细节，使得角色表现更加生动。对阴影的笔触感处理（为了叙述方便，以下均用“固有阴影”来描述此类型的阴影），我们同样可以借助遮罩来实现，因为也是单个通道就能存储的灰度信息，原神将其存储在了ilmTexture的G通道中。对此信息通过step处理之后，再乘上我们计算出的halfLambert信息，就可以获取到最基础的带有固有阴影的光照信息。</p><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20231202215851888.png" alt="心海ilmTexture的G通道与处理后的光照信息"></p><p>随后，再加入一个参数对光照区域进行调整，就可以得到Ramp图的U轴坐标，代码如下：其中_LightArea是用于调整光照区域范围的参数，调整后的信息与0.99取最小值，避免采样到像素边缘。</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">float halfLambert = 0.5 * dot(mainLightDirection, input.normalWS) + 0.5;</span><br><span class="line">float AOMask = step(0.02, ilmTexCol.g);</span><br><span class="line">float brightAreaMask = AOMask * halfLambert;</span><br><span class="line"></span><br><span class="line">rampUV.x = min(0.99, smoothstep(0.001, 1.0 - _LightArea, brightAreaMask));</span><br></pre></td></tr></table></figure><p>最终再乘上BaseColor，获取Ramp的运用上去后的效果。</p><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20231202221911368.png" alt="采样Ramp图及Ramp图对BaseColor的作用效果"></p><h3 id="1-3-进一步优化diffuse表现">1.3 进一步优化diffuse表现</h3><p>因为只使用Ramp图处理的固有阴影颜色不够明显，所以我再额外通过一个参数来进一步进行调色，代码如下：其中区分_DarkShadowColor和_CoolDarkShadowColor来分别处理白天或夜晚的两种情况</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">half3 darkShadowColor = lerp(_DarkShadowColor.rgb, _CoolDarkShadowColor.rgb, _UseCoolShadowColorOrTex) * rampTexCol;</span><br></pre></td></tr></table></figure><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20231202233941445.png" alt="加深处理固有阴影后的表现效果"></p><p>在写代码时，想要进一步拓展可控性（源自于前几个版本对Ramp图的处理方法），我额外加入了一个ShaderFeature来额外添加一个“使用自定义亮部调色颜色”的选项，如果使用自定义的亮部调色颜色，则可以获取到额外的二分Ramp条，其调色颜色就是不启用此feature时的亮部颜色。不过由于这一版对Ramp的处理算是比较适宜，所以在最终渲染效果上，我并没有自定义亮部颜色。不过处理的思路还是保留下来。</p><p>其实也就是进一步处理rampTexCol，通过一个参数影响最接近亮部的那条Ramp的宽度，基于此进一步处理brightAreaMask，来获取亮部区域（非Ramp区）的遮罩，然后对rampTexCol进行进一步的插值处理即可，代码如下：其中_ShadowRampWidth用来控制最后一个“Ramp条”的宽度，乘上0.1则是在Shader内对Ramp长度进行一定范围控制，_LightAreaColorTint为自定义的亮部调色颜色</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#if !defined(_USERAMPLIGHTAREACOLOR_ON)</span><br><span class="line">brightAreaMask = step(1.0 - _LightArea, brightAreaMask + (1.0 - _ShadowRampWidth) * 0.1);</span><br><span class="line">rampTexCol = lerp(rampTexCol, _LightAreaColorTint, brightAreaMask);</span><br><span class="line">#endif</span><br></pre></td></tr></table></figure><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/GLR2_ShadowRampWidthAdjust.gif" alt="自定义亮部颜色情况下对Ramp宽度的调节"></p><p>由于原神角色建模时脸和脖子是分开的，脖子属于身体部分，以及脖子区域计算出的光照模型通常怪怪的，所以游戏中这一部分是不参与光照计算的。为了区分出不进行光照计算的区域，我在模型顶点色的B通道中绘制了一张遮罩，区分不进行光照计算的区域，并且为了与面部结合得更融洽，还额外加入了一个参数去控制这一部分的阴影颜色，并在此之后进行最终的diffuse颜色融合，其代码如下：其中_NeckColor是脖子区域的调色颜色。</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ShadowColorTint = lerp(_NeckColor.rgb, ShadowColorTint, input.vertexColor.b);</span><br><span class="line">half3 diffuseColor = ShadowColorTint * mainTexCol.rgb;</span><br></pre></td></tr></table></figure><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20231203001948366.png" alt="模型顶点色B通道信息，以及最终得到的diffuse效果"></p><h3 id="1-4原神Diffuse贴图的A通道">1.4原神Diffuse贴图的A通道</h3><p>原神部分角色身体和头发的漫反射贴图不仅仅只存储BaseColor，其A通道也存有额外信息。我在这次处理流程中，基于A通道存储的不同的信息，将漫反射贴图分为了四类：无额外信息、常亮自发光、闪烁自发光、AlphaTest信息。通常包含神之眼的贴图都是闪烁自发光，特殊一点的像神里皮肤帽子上花瓣的贴图是AlphaTest遮罩，其余的通常进游戏看一下就能区分。面部漫反射贴图A通道存储的是腮红区域的遮罩。</p><p>不同类型贴图的区分通过一个自定义枚举去控制宏，对于自发光的处理，我的方式是统一加入一个自发光系数，通过控控制该系数来实现对应效果，如果是非自发光或AlphaTest，直接设置自发光系数为0，然后再基于自发光系数在最终得到的颜色上叠加自发光。而对于AlphaTest，则是在对应的宏定义情况加入一个Clip来实现，代码如下：其中EmissionScale是用于控制自发光强度的参数。</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">//Properties中</span><br><span class="line">[KeywordEnum(None,Flicker,Emission,AlphaTest)]_MainTexAlphaUse(&quot;Diffuse Texture Alpha Use&quot;, float) = 0.0</span><br><span class="line">//...</span><br><span class="line">//Pass中</span><br><span class="line">#pragma shader_feature_local _ _MAINTEXALPHAUSE_NONE _MAINTEXALPHAUSE_FLICKER _MAINTEXALPHAUSE_EMISSION _MAINTEXALPHAUSE_ALPHATEST</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">float emissionFactor = 1.0;</span><br><span class="line">#if defined(_MAINTEXALPHAUSE_EMISSION)</span><br><span class="line">emissionFactor = _EmissionScaler * mainTexCol.a;</span><br><span class="line">#elif defined(_MAINTEXALPHAUSE_FLICKER)</span><br><span class="line">emissionFactor = _EmissionScaler * mainTexCol.a * (0.5 * sin(_Time.y) + 0.5);</span><br><span class="line">#elif defined(_MAINTEXALPHAUSE_ALPHATEST)</span><br><span class="line">clip(mainTexCol.a - _MainTexCutOff);</span><br><span class="line">emissionFactor = 0;</span><br><span class="line">#else</span><br><span class="line">emissionFactor = 0;</span><br><span class="line">#endif</span><br><span class="line">//...</span><br><span class="line">color *= 1 + emissionFactor;</span><br></pre></td></tr></table></figure><p>心海的神之眼放在身体贴图中，身体贴图A通道存储的是闪烁自发光遮罩，头发贴图的A通道则是存储了自发光强度，</p><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20231203140709167.png" alt="心海Diffuse贴图的A通道，以及自发光强度为0和10时的表现"></p><p>像神里帽子上的花朵这一部分，为了在低顶点数的面片上实现平滑的花瓣效果，在Fragment Shader中进行了Clip</p><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20231203141054240.png" alt="神里帽子上花瓣Diffuse贴图的A通道，以及Clip的处理"></p><h2 id="2-高光反射">2 高光反射</h2><img src="E:/Document/Notes/Render/【Unity】GLR2.0-角色篇.assets/image-20231204163239958.png" alt="加入高光反射的效果" style="zoom:67%;" /><h3 id="2-1-赛璐璐卡渲高光概述">2.1 赛璐璐卡渲高光概述</h3><p>卡通角色的高光最特殊的点在于，对简化后头发天使轮高光的处理，一般可以通过各向异性来处理，但是为了更加便捷地控制，我们可以在贴图和遮罩上下功夫。其他部位的高光，因为赛璐璐风格的大色块上色特点与形状的高度概括性，可以简单地用Blinn-Phong高光模型来处理。同样也可以使用次世代卡通渲染的处理思路，为一些需要强调高光表现的区域处理遮罩和法线贴图，结合使用风格化PBR及Blinn-Phong。基于对插画笔触感的模拟，也有如使用MatCap处理镜面高光的hack。</p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20231203162159734.png" alt="卡通角色头发上高度概括的天使轮高光，为什么是赛马娘？因为我太爱帝王小姐了！" style="zoom: 45%;" /><p>原神中，对高光的处理方式有三种，第一是普通的Blinn-Phong高光，第二是基于MatCap模拟笔触感的镜面高光，第三是须弥版本后陆续引入的次世代卡通角色渲染高光。</p><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20231203162722448.png" alt="MatCap贴图及神里皮肤的法线贴图"></p><p>这一个版本的还原中，我没有使用法线贴图，只是处理BLinn-Phong高光和特定区域的MatCap高光，然后对其叠加，获取最终的高光效果。</p><h3 id="2-2-ilm贴图R通道和Blinn-Phong高光">2.2 ilm贴图R通道和Blinn-Phong高光</h3><p>由于身体上不同物件、衣服布料需要表现不同的材质质感，以及整体统一都表现高光会让渲染效果显得很“油腻”，所以我们需要对高光的区域、以及对应区域内高光的强度进行一定的控制，借此形成对比来加强角色的表现效果。控制方式同样是使用灰度遮罩，基于灰度缩放高光的强度。</p><p>原神把控制高光强度缩放的灰度信息放在了ilmTexture的R通道中，计算出Blinn-Phong高光然后再乘上ilmTexture.a的值即可。我在处理的过程中区分了金属区域和非金属区域，通过不同的Shiness值去分别控制不同的Blinn-Phong幂的指数，区分不同的高光效果。不过对于最终的效果，非金属区域的Blinn-Phong高光我只用在了头发上，并且为了风格化，使用step进行了二分，模拟出了一个头上高光的小圆点。代码如下：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">//Blinn-Phong计算</span><br><span class="line">float3 viewDirectionWS = normalize(_WorldSpaceCameraPos.xyz - input.positionWS.xyz);</span><br><span class="line">float3 halfDirectionWS = normalize(viewDirectionWS + mainLightDirection);</span><br><span class="line">float hdotl = max(dot(halfDirectionWS, input.normalWS.xyz), 0.0);</span><br><span class="line">//非金属高光</span><br><span class="line">float nonMetalSpecular = step(1.0 - 0.5 * _NonMetalSpecArea, pow(hdotl, _Shininess)) * ilmTexCol.r;</span><br><span class="line">//金属高光（MatCap高光）</span><br><span class="line">float metalBlinnPhongSpecular = pow(hdotl, _MTShininess) * ilmTexCol.r;</span><br></pre></td></tr></table></figure><h3 id="2-3-ilm贴图B通道和MatCap高光">2.3 ilm贴图B通道和MatCap高光</h3><p>我不能确切地说像原神这样实现镜面高光的贴图是MatCap图，但是大家都这么称呼，我在文中也统一这样称呼。MatCap基本概念是用于做映射的，将光源、材质信息烘焙到球上，然后存到贴图中，渲染的时候直接使用ViewSpace的法线信息去采样。我真正理解原神MatCap使用方法，源自之前和同学讨论Blender渲染MMD的ToonShader中对toon这张Texture的使用。他基于屏幕空间法线信息采样Toon贴图，制作出会随着镜头上下移动变化，并且具有笔触感的下部阴影效果（模拟光从上方打下来，所以底部是阴影）。然后我顿悟了，MatCap也是基于同样的原理，基于屏幕空间法线去读取这张贴图，在屏幕空间不同朝向的面的高光表现不同，获得具有笔触感的镜面高光表现。</p><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20231203211639492.png" alt="一张MMD中使用的Toon贴图和原神中模拟镜面高光的MatCap贴图"></p><p>明白原理之后，代码就很简单了。我处理流程中使用的是HClipSpcace的法线（与VS是等效的），并且在用于读取贴图前需要将信息映射到0-1之间，不过这个映射的过程也需要一点特殊处理。可以发现，这张MatCap是上下左右对称的，而我们用于去采样贴图的屏幕空间法线的xy信息都在-1到1之间，恰好可以通过中心点(0.5, 0.5)来映射UV到0-1之间。其本源的原因我暂时还没有想到，有知道的大佬希望能在评论区说一说。这样处理后实现的效果是，越是朝向摄像机的面，高光强度越高。之后用MatCap贴图读取到的信息作为Blinn-Phong高光的强度，乘上之前计算出的高光，即可获得带有笔触感、二分感的高光。代码如下：其中_MTMapBrightness适用于控制MatCap贴图的作用强度</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">float2 normalCS = TransformWorldToHClipDir(input.normalWS.xyz).xy * 0.5 + float2(0.5, 0.5);</span><br><span class="line">float metalTexCol = saturate(SAMPLE_TEXTURE2D(_MetalTex, sampler_MetalTex, normalCS)).r * _MTMapBrightness;</span><br><span class="line">float metalBlinnPhongSpecular = pow(hdotl, _MTShininess) * ilmTexCol.r;</span><br><span class="line">float metalSpecular = ilmTexCol.b * metalBlinnPhongSpecular * metalTexCol;</span><br></pre></td></tr></table></figure><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/GLR2_Kokomi_MatCapSpec.gif" alt="MatCap加入后的高光表现，心海胸口蝴蝶结部分较明显"></p><p>最终对两个计算出的高光进行相加，并乘上最终的Diffuse颜色，即可获得高光颜色，代码如下：其中_MTSpecularScale用于控制MatCap高光的整体强度，_SpecMulti用于控制所有高光的整体强度</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">half3 specularColor = (metalSpecular * _MTSpecularScale + nonMetalSpecular) * diffuseColor;</span><br><span class="line">half3 color = specularColor * _SpecMulti + diffuseColor;</span><br></pre></td></tr></table></figure><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20231204161131870.png" alt="高光强度，只计算漫反射及加入高光后的表现"></p><h2 id="3-边缘光">3 边缘光</h2><p>边缘光源自于逆光，用于勾勒出对象的轮廓。因为赛璐璐风格使用大色块铺色，容易使得主体对象与背景区分得不够清楚，尤其是在灯光不强的区域。边缘光的使用可以加大主体对象与背景的区分，同时我认为边缘光可以让卡通角色显得更立体，像原神这样带有二分感的边缘光可以让角色更有“二次元味”。</p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20231204164400856.png" alt="Cygames赛马娘Winning Live制作技术分享，使用边缘光区分次要马娘与背景" style="zoom:67%;" /><p>目前效果中，边缘光使用的是屏幕空间等距边缘光，在适宜的边缘光宽度范围内得到的效果是很不错的，缺点是需要提前渲染、发送、读取深度图。屏幕空间等距边缘光的原理是，对模型沿法线进行一定程度的外扩，对比外扩前后像素的深度差距，如果像素深度差距达到一定值，则判定为边缘光区域，做进一步的边缘光处理。所以，控制边缘光的宽度本质是控制模型外扩的距离。在此基础上，我借用了NiloToon对描边调整的方式，加入了边缘光随距离的消散，代码如下：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">//获取Fragment原本的深度值</span><br><span class="line">float2 originPositionSS = positionCS_XYZ.xy / _ScaledScreenParams.xy;</span><br><span class="line">float originDepth = SAMPLE_TEXTURE2D(_CameraDepthTexture, sampler_CameraDepthTexture, originPositionSS).r;</span><br><span class="line">originDepth = LinearEyeDepth(originDepth, _ZBufferParams);</span><br><span class="line">//基于摄像机距离调整外扩距离，进而控制边缘光宽度</span><br><span class="line">float distanceFixMultiplier = GetRimLightDistanceFixMultiplier(positionCS_XYZ.z);</span><br><span class="line">//获取Fragment外扩后的深度值</span><br><span class="line">float2 offsetPositionSS = originPositionSS + normalCS_XY * rimLightWidth * rimLightCorrection * distanceFixMultiplier;</span><br><span class="line">float offsetDepth = SAMPLE_TEXTURE2D(_CameraDepthTexture, sampler_CameraDepthTexture, offsetPositionSS).r;</span><br><span class="line">offsetDepth = LinearEyeDepth(offsetDepth, _ZBufferParams);</span><br><span class="line">float depthDiff = offsetDepth - originDepth;</span><br></pre></td></tr></table></figure><p>其中GetRimLightDistanceFixMultiplier函数获取基于摄像机距离控制边缘光消散的参数，代码如下：因为边缘光不同于描边，边缘光的宽度更适合基于模型大小进行控制，所以我没有像处理描边那样通过摄像机距离来缩放宽度。</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">float ApplyRimLightDistanceFadeOut(float distance)</span><br><span class="line">&#123;</span><br><span class="line">    return smoothstep(0.0, 0.05, distance);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">float GetRimLightDistanceFixMultiplier(float positionVS_Z)</span><br><span class="line">&#123;</span><br><span class="line">    float distanceFix;</span><br><span class="line">    distanceFix = ApplyRimLightDistanceFadeOut(positionVS_Z);</span><br><span class="line"></span><br><span class="line">    return distanceFix;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后进一步处理深度差，获取边缘光的遮罩。我想要通过不同深度差获取不同的边缘光强度，所以没有简单地通过step去处理深度差。但是如果直接用depthDiff去进行最终边缘光计算的插值，得到的效果不够好，不同区域之间深度diff差异太大，所以对depthDiff进一步处理，获得比较平滑的Rim Light效果</p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20231204190004974.png" alt="不同深度差获得不同的边缘光强度" style="zoom:80%;" /><p>其中的参数是我调试后认为最终效果不错的值，处理后得到的结果是减小了大Diff值与小Diff值之间的差距。然后基于diffuseColor乘上一定强度获得边缘光的颜色，并与之前出来的结果进行融合，代码如下：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">float rimLightMask = 0.75 * (saturate(rimLightDepthDiff) * 0.5 + 0.15 * step(0.1, rimLightDepthDiff));</span><br><span class="line">color = lerp(color, diffuseColor * (1.0 + _RimLightStrength * 2.0), rimLightMask);</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20231204190657917.png" alt="边缘光遮罩，漫反射+高光及加入边缘光的效果"></p><h2 id="4-面部阴影">4 面部阴影</h2><p>卡通面部阴影目前主流实现方案有两个：一个是特殊处理面部法线，因为卡通角色面部阴影的问题来自于面部图元法线朝向无法满足赛璐璐高度概括阴影的计算，所以可以通过修正模型法线来实现，也可以通过计算一个球体的光照信息映射到面部；另一个是使用SDF面部阴影贴图，相比于特殊处理法线，这个方法更易于控制、细化面部阴影的表现效果，例如概括性的伦勃朗光、以及正面打光情况下用于提升面部立体感的鼻翼侧边的阴影。原神中的面部二分阴影是借助SDF面部阴影贴图来实现的。</p><p>SDF是有向距离场的简称，关于SDF面部阴影的原理，网上有很多大佬都有分享，在此我只说一说我自己的理解。SDF面部阴影贴图实质上只是阴影遮罩贴图，以灰度信息作为阈值，去区分光照计算出来的亮面和暗面，距离场的技术也只是在合并遮罩图时用到过。先通过制作某几个特定光照角度下面部的阴影遮罩，然后计算出每个遮罩的有向距离场，最后合并这几张距离场贴图，使得阴影过度变得连续。</p><p>所以处理的重点就在于，如何去获得光照的角度信息，可以通过点乘的值来替代。在目前的项目中，我通过获取光源在xz平面投影的向量，与模型向前的向量，计算他们的点乘，用于作为面部阴影计算的光照信息。不过在我的流程中，我将SDF贴图的信息反过来作为了光照的信息，用计算出的点乘作为阈值去进行step，这样获取到的鼻翼亮光和阴影才更为正确。还有一个特殊的地方，一般SDF面部阴影图只记录一侧的光照情况，所以需要我们获取模型在世界空间中左右方向上的一条向量，来判断光源从那一侧照过来，进一步处理贴图的翻转。处理的代码如下：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">//获取向量信息</span><br><span class="line">//unity_objectToWorld可以获取世界空间的方向信息，构成如下：</span><br><span class="line">// unity_ObjectToWorld = ( right, back, left)</span><br><span class="line">float3 rightDirectionWS = unity_ObjectToWorld._11_21_31;</span><br><span class="line">float3 backDirectionWS = unity_ObjectToWorld._13_23_33;</span><br><span class="line">float rdotl = dot(normalize(mainLightDirection.xz), normalize(rightDirectionWS.xz));</span><br><span class="line">float fdotl = dot(normalize(mainLightDirection.xz), normalize(backDirectionWS.xz));</span><br><span class="line"></span><br><span class="line">//SDF面部阴影</span><br><span class="line">//将ilmTexture看作光源的数值，那么原UV采样得到的图片是光从角色左侧打过来的效果，且越往中间，所需要的亮度越低。lightThreshold作为点亮区域所需的光源强度</span><br><span class="line">float2 ilmTextureUV = rdotl &lt; 0.0 ? input.uv : float2(1.0 - input.uv.x, input.uv.y);</span><br><span class="line">float lightThreshold = 0.5 * (1.0 - fdotl);</span><br><span class="line"></span><br><span class="line">half4 ilmTexCol = SAMPLE_TEXTURE2D(_ilmTex, sampler_ilmTex, ilmTextureUV);</span><br><span class="line">float lightStrength = ilmTexCol.r;</span><br></pre></td></tr></table></figure><p>像眼睛、眉毛这些区域，计算光照后的效果不好，所以将其设置为不计算光照的区域，在最终合并光影时将其视为阴影区域。设置遮罩的方法可以通过贴图，也可以通过顶点颜色。边缘光的叠加与身体的处理方式相同，我在这里就不重复叙述了。最终进行光影效果合并，代码如下：其中做了基于白天、夜晚不同情况下的阴影和亮部颜色调整</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">float brightAreaMask = step(lightThreshold, lightStrength);</span><br><span class="line">half3 brightAreaColor = mainTexCol.rgb * _LightAreaColorTint.rgb;</span><br><span class="line">half3 shadowAreaColor = mainTexCol.rgb * lerp(_DarkShadowColor.rgb, _CoolDarkShadowColor.rgb, _UseCoolShadowColorOrTex);</span><br><span class="line"></span><br><span class="line">half3 lightingColor = lerp(shadowAreaColor, brightAreaColor, brightAreaMask) * _MainTexColoring.rgb;</span><br><span class="line">half3 color = lerp(mainTexCol.rgb, lightingColor, metalTexCol.r);</span><br></pre></td></tr></table></figure><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20231204204938806.png" alt="SDF阴影直接采样及受光照区域遮罩"></p><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/GLR2_Kokomi_FaceShadow.gif" alt="面部的BrightAreaMask"></p><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20231204204328212.png" alt="BrightAreaMask，只是用漫反射贴图及最终运用阴影和颜色调整后的效果"></p><h2 id="5-描边">5 描边</h2><p>描边是赛璐璐风格卡渲的另一大特点，凸显角色的同时，他同样也是使渲染效果更有“二次元味”的一块基石。</p><p>很喜欢崩三里那样带有自发光和透明的描边，加上Bloom后效果很好。顺带一提，12月7号是Kiana生日！庆生视频来不及做完了，只能在这里发一发QAQ</p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20231204211238584.png" alt="Kiana" style="zoom: 33%;" /><p>目前描边的方案大概有三类：第一类是基于屏幕后处理实现的，通过在后处理中根据法线和深度信息进行边缘检测来获取描边区域。其优点是逐像素的描边，精度更高，而缺点是在后处理中无法简单地灵活处理描边颜色这类信息。第二类是通过贴图来实现如膝盖区域的内描边，并且可以通过“本村线”这样的处理流程来减少锯齿，不过此方案只能处理内描边。第三类是在模型渲染过程中实现描边，即BackFacing通过额外的Pass来实现描边。优点是可以在对应Pass中灵活处理描边的表现，如区分区域控制描边颜色、透明度、自发光等，缺点是需要额外的Pass，并且是基于顶点实现的描边，并且需要特殊处理法线。</p><p>为了更灵活地控制描边效果，我使用的是BackFacing描边。基本原理是使用一个额外的Pass，在VertexShader中将模型沿着表面法线方向进行一定程度的外扩，在FragmentShader中对其颜色进行处理，并且整个Pass剔除模型的前半面，前半面显示的是原本的模型，后半面显示的是描边。简而言之，可以理解成渲染一个CullFront的额外放大后的模型。</p><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20231204215310661.png" alt="Outline Pass渲染的结果"></p><p>但是，模型硬表面上的顶点，即具有多个法线的顶点，导入引擎后，引擎会将其视为具有各自不同法线朝向的不同顶点（模型导入引擎所见的顶点数往往比DCC软件中见到的顶点数多的原因），导致我们直接沿法线外扩后，硬边部位的描边会有断裂的情况。这个时候需要我们计算额外的平滑法线信息，用于外扩获得平滑的描边表现。我这一次处理平滑法线的方法参考（抄）了喵刀老师，借助Unity资产导入后处理来实现的，原理可以看文末链接的喵刀老师的文章，也可以去我的代码仓库的ModelSmoothNormalImporter.cs文件中查看我进一步注释后的喵刀老师的代码。</p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20231204214023274.png" alt="使用模型法线(左)和使用平滑法线(右)处理描边的效果" style="zoom:67%;" /><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20231204214313093.png" alt="使用模型法线(左)和使用平滑法线(右)处理描边的效果"></p><p>平滑法线存储在法线空间中。因为人物模型会运动，所以在处理模型时，平滑法线信息需要转换到法线空间中进行存储（原理可以进一步查询大佬的资料），进而我们需要在shader中将平滑法线信息再还原回世界空间。这一步和处理法线贴图原理相同，流程更简单，只用在Vertex中处理法线，其代码如下：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">input.vertexColor.r = input.vertexColor.r * 2.0 - 1.0;</span><br><span class="line">input.vertexColor.g = input.vertexColor.g * 2.0 - 1.0;</span><br><span class="line">float3 smoothNormalTS = normalize(float3(input.vertexColor.r, input.vertexColor.g,</span><br><span class="line">    sqrt(1 - dot(float2(input.vertexColor.r, input.vertexColor.g), float2(input.vertexColor.r, input.vertexColor.g)))</span><br><span class="line">        ));</span><br></pre></td></tr></table></figure><p>因为法线外扩在VertexShader阶段实现，他的粗细是相对于模型本身而言的，如果不做处理，会在摄像机近时显得描边特别细，在摄像机远时显得描边特别粗，视觉效果不理想。所以描边粗细需要随着摄像机的距离进行调整，处理方式我参考（抄）了NiloToon的处理方式，代码如下：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">float GetCameraFOV()</span><br><span class="line">&#123;</span><br><span class="line">    //Shader中获取相机FOV</span><br><span class="line">    //https://answers.unity.com/questions/770838/how-can-i-extract-the-fov-information-from-the-pro.html</span><br><span class="line">    float t = unity_CameraProjection._m11;</span><br><span class="line">    float Rad2Deg = 180 / 3.1415;</span><br><span class="line">    float fov = atan(1.0f / t) * 2.0 * Rad2Deg;</span><br><span class="line">    return fov;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">float ApplyOutlineDistanceFadeOut(float distanceVS)</span><br><span class="line">&#123;</span><br><span class="line">    //深度在ViewSpace中0-10范围内进行描边粗细的缩放</span><br><span class="line">    #if UNITY_REVERSED_Z</span><br><span class="line">        //DX平台</span><br><span class="line">        return max(0.0, (50.0 - distanceVS) * 0.1);</span><br><span class="line">    #else</span><br><span class="line">        //OpenGL平台（还未做测试）</span><br><span class="line">        return min(10.0, distanceVS) * 0.1;</span><br><span class="line">    #endif</span><br><span class="line">&#125;</span><br><span class="line">//距离控制描边粗细，改编自NiloToon的函数</span><br><span class="line">float GetOutlineCameraFOVAndDistanceFixMultiplier(float positionVS_Z)</span><br><span class="line">&#123;</span><br><span class="line">    float distanceFix;</span><br><span class="line">    if(unity_OrthoParams.w == 0)</span><br><span class="line">    &#123;</span><br><span class="line">        //透视相机</span><br><span class="line">        float distance = abs(positionVS_Z);</span><br><span class="line">        distanceFix = ApplyOutlineDistanceFadeOut(distance);</span><br><span class="line">        distanceFix *= GetCameraFOV();</span><br><span class="line">    &#125;</span><br><span class="line">    else</span><br><span class="line">    &#123;</span><br><span class="line">        //正交相机</span><br><span class="line">        float distance = abs(unity_OrthoParams.z);</span><br><span class="line">        distanceFix = ApplyOutlineDistanceFadeOut(distance) * 50; //参考NiloToon的数值</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    return distanceFix;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>为了实现不同区域的不同描边颜色，我们需要获取各个区域的遮罩。这个案例中我直接使用了ilmTexture贴图的A通道，即用于区分不同Ramp的灰度信息，在FragmentShader中，借Step处理并相减出不同区域的遮罩。但是处理的效果不够好，只实现了部分颜色可控，所以只把我的处理放在这里做一个参考（错误案例），代码如下：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">half4 BackFaceOutlineFragment(Varyings input) : SV_Target</span><br><span class="line">&#123;</span><br><span class="line">    //根据ilmTexture的五个分组区分出五个不同颜色的描边区域</span><br><span class="line">    half outlineColorMask = SAMPLE_TEXTURE2D(_ilmTex, sampler_ilmTex, input.uv).a;</span><br><span class="line">    half areaMask1 = step(0.003, outlineColorMask) - step(0.35, outlineColorMask);</span><br><span class="line">    half areaMask2 = step(0.35, outlineColorMask) - step(0.55, outlineColorMask);</span><br><span class="line">    half areaMask3 = step(0.55, outlineColorMask) - step(0.75, outlineColorMask);</span><br><span class="line">    half areaMask4 = step(0.75, outlineColorMask) - step(0.95, outlineColorMask);</span><br><span class="line">    half areaMask5 = step(0.95, outlineColorMask);</span><br><span class="line">    half4 finalColor = areaMask1 * _OutlineColor1 + areaMask2 * _OutlineColor2 + areaMask3 * _OutlineColor3 + areaMask4 * _OutlineColor4 + areaMask5 * _OutlineColor5;</span><br><span class="line"></span><br><span class="line">    return finalColor;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="E:/Document/Notes/Render/%E3%80%90Unity%E3%80%91GLR2.0-%E8%A7%92%E8%89%B2%E7%AF%87.assets/image-20231204222638335.png" alt="描边Pass的表示，不带有描边的效果与带有描边的效果"></p><h2 id="6-代码架构">6 代码架构</h2><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20231205233601528.png" alt="GLR2角色渲染相关代码架构"></p><h2 id="参考链接">参考链接</h2><p>YuiLu大佬的还原案例，是对我帮助最大的参考：</p><p><a href="https://zhuanlan.zhihu.com/p/511150017">https://zhuanlan.zhihu.com/p/511150017</a></p><p>这篇文章里，作者更多地从本质上去思考如何把卡通渲染做得好看，探求卡通渲染的“法”这一层面的知识：</p><p><a href="https://zhuanlan.zhihu.com/p/561494026">https://zhuanlan.zhihu.com/p/561494026</a></p><p>文如其题，作者在里面介绍了当今业界普遍使用的那些卡渲处理技术：</p><p><a href="https://zhuanlan.zhihu.com/p/508826073">https://zhuanlan.zhihu.com/p/508826073</a></p><p>喵刀老师的基于Unity资产后处理和Job System实现的高效的平滑法线处理流程：</p><p><a href="https://zhuanlan.zhihu.com/p/107664564">https://zhuanlan.zhihu.com/p/107664564</a></p><p>喵刀老师的屏幕空间深度边缘光教程：</p><p><a href="https://zhuanlan.zhihu.com/p/139290492">https://zhuanlan.zhihu.com/p/139290492</a></p><p>NiloToonURP管线案例，其中大佬对代码架构的清晰处理方式很值得学一学：</p><p><a href="https://github.com/ColinLeung-NiloCat/UnityURPToonLitShaderExample">https://github.com/ColinLeung-NiloCat/UnityURPToonLitShaderExample</a></p><h2 id="最后">最后</h2><p>再放一遍做的MMD，虽然效果还有欠缺，但是也花了很久时间去做了。如果文章对你有所帮助的话，也请转到视频点个赞和关注，救救我惨淡的数据吧！</p><p><a href="https://www.bilibili.com/video/BV1ew41187H5/">https://www.bilibili.com/video/BV1ew41187H5/</a></p><p>后续还有后处理的使用，以及整个制作工作流的分享文章。</p>]]></content>
    
    
    <summary type="html">在URP管线下探究原神的卡通渲染实现的技术分享文档，角色渲染部分</summary>
    
    
    
    <category term="Tech-Document" scheme="https://himoqiuhan.github.io/categories/Tech-Document/"/>
    
    
    <category term="Unity" scheme="https://himoqiuhan.github.io/tags/Unity/"/>
    
    <category term="URP管线" scheme="https://himoqiuhan.github.io/tags/URP%E7%AE%A1%E7%BA%BF/"/>
    
    <category term="卡通渲染" scheme="https://himoqiuhan.github.io/tags/%E5%8D%A1%E9%80%9A%E6%B8%B2%E6%9F%93/"/>
    
    <category term="技术美术" scheme="https://himoqiuhan.github.io/tags/%E6%8A%80%E6%9C%AF%E7%BE%8E%E6%9C%AF/"/>
    
    <category term="卡通角色渲染" scheme="https://himoqiuhan.github.io/tags/%E5%8D%A1%E9%80%9A%E8%A7%92%E8%89%B2%E6%B8%B2%E6%9F%93/"/>
    
  </entry>
  
  <entry>
    <title>【Unity】仿原神渲染2.0技术文档-后处理篇</title>
    <link href="https://himoqiuhan.github.io/2023/12/07/Projects-GenshinLikeRenderingInURP2-PostProcess/"/>
    <id>https://himoqiuhan.github.io/2023/12/07/Projects-GenshinLikeRenderingInURP2-PostProcess/</id>
    <published>2023-12-07T13:17:30.000Z</published>
    <updated>2024-07-27T15:13:59.657Z</updated>
    
    <content type="html"><![CDATA[<h1>【Unity】URP 中的仿原神渲染2.0-后处理篇</h1><blockquote><p>过程中使用到的模型、贴图均来自miHoYo，个人仅用作学习使用</p></blockquote><h2 id="整体效果展示">整体效果展示</h2><p>【【卡通渲染/MMD】五等分の気持ち】 <a href="https://www.bilibili.com/video/BV1ew41187H5/">https://www.bilibili.com/video/BV1ew41187H5/</a></p><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20231204162907101.png" alt="夜间、白天及加入Bloom后的效果"></p><blockquote><p>代码仓库链接：<a href="https://github.com/himoqiuhan/URPToonRenderReconstruction">https://github.com/himoqiuhan/URPToonRenderReconstruction</a></p></blockquote><h2 id="0-前言">0 前言</h2><p>这一次渲染的后处理我只自己写了Bloom，不过整体的后处理代码架构我个人认为是比较清晰的，也比较方便用于拓展。</p><h2 id="1-Bloom处理思路">1 Bloom处理思路</h2><p>Bloom处理的核心有三步，第一步提取Bloom区域，第二步对提取结果进行模糊处理，第三步合并处理结果与原图。</p><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20231205160707729.png" alt="Bloom处理核心步骤"></p><p>提取Bloom区域的方式一般有两种，一是直接根据颜色值提取，二是根据亮度提取。因为两个的处理方式都很简单，所以我在处理流程中加入了枚举，可以选择不同的提取方式。不过一般情况下，直接根据颜色值来提取得到的效果要更好一点，因为颜色值减去阈值的时候是RGB值都减去一个相同的数，降低明度的同时还能够保持颜色的饱和度。</p><p>对提取结果进行模糊处理时，最简单的方法是直接进行高斯模糊，但是那样无法获得“扩散”的感觉。我的处理方案参考了COD2014年的分享，对Bloom区域提取出来的结果进行逐级降采样，存储到一系列临时RenderTexture中，然后再逐级向上叠加混合，最后对最终的混合结果进行一次高斯模糊。其中一些提升效果的细节，我放到下面详细介绍。</p><p>合并处理结果与原图就是控制Bloom强度后与原图直接相加。</p><h2 id="2-添加Feature实现URP自定义后处理">2 添加Feature实现URP自定义后处理</h2><h3 id="2-1-RenderFeature与RenderPass">2.1 RenderFeature与RenderPass</h3><p>通常在Project视口中右键-&gt;Create-&gt;Rendering-&gt;URP Asset(with Universal Renderer)可以新建一个Render Feature模板，里面有官方给的注释，但是模板中Pass类的定义是写在RenderFeature类中的，我想要架构更清晰一点，同时想要一个Feature处理多个不同的后处理Pass，就把他们放到了不同的文件中。我没有考虑过一个Feature包含多个Pass的性能问题，如果有大佬知道，还请不吝赐教！</p><p>这样就获得了如下两个文件：CustomPostProcessFeature.cs和CustomBloomPass.cs</p><p>CustomPostProcessFeature.cs统筹管理所有后处理相关的Pass。这一个版本只写了Bloom，后续添加其他的后处理Pass同Bloom处理即可。这样处理需要在代码中考虑不同效果执行的先后顺序，以及目前有一点可能的优化空间：如果加入没有开启的Pass，是否执行Pass的逻辑是在Pass的Execute函数中判断的，也就是说该Renderer一定会执行EnqueuePass的函数。</p><p>RenderFeature的关键函数是Create和AddRenderPasses，Create用于创建对应ScriptableRenderPass的对象，并设置RenderPassEvent，即把这个Pass加入到哪个阶段，AddRenderPasses用于将Pass加入到CommandBuffer的队列中，用于后续执行。</p><figure class="highlight c#"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title">CustomPostProcessFeature</span> : <span class="title">ScriptableRendererFeature</span></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">    CustomBloomPass m_CustomBloomPass;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">override</span> <span class="keyword">void</span> <span class="title">Create</span>()</span></span><br><span class="line">    &#123;</span><br><span class="line">        m_CustomBloomPass = <span class="keyword">new</span> CustomBloomPass();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Configures where the render pass should be injected.</span></span><br><span class="line">        m_CustomBloomPass.renderPassEvent = RenderPassEvent.BeforeRenderingPostProcessing;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Here you can inject one or multiple render passes in the renderer.</span></span><br><span class="line">    <span class="comment">// This method is called when setting up the renderer once per-camera.</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">override</span> <span class="keyword">void</span> <span class="title">AddRenderPasses</span>(<span class="params">ScriptableRenderer renderer, <span class="keyword">ref</span> RenderingData renderingData</span>)</span></span><br><span class="line">    &#123;</span><br><span class="line">        m_CustomBloomPass.SetUp(renderer.cameraColorTarget);</span><br><span class="line">        renderer.EnqueuePass(m_CustomBloomPass);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>CustomBloomPass.cs执行具体的Bloom处理指令，指令存放在Execute函数中，其他的内容基本是围绕着这个函数进行处理，代码框架如下：</p><figure class="highlight c#"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title">CustomBloomPass</span> : <span class="title">ScriptableRenderPass</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">//需要使用到的成员变量，例如ShaderProperty的ID、用于执行的Shader、以及用于处理的Material等等，具体参数可以看完整的代码</span></span><br><span class="line">    <span class="comment">//...</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">CustomBloomPass</span>()</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">//基于设置的Shader创建材质</span></span><br><span class="line">        <span class="comment">//...</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">SetUp</span>(<span class="params"><span class="keyword">in</span> RenderTargetIdentifier finalRenderTarget</span>)</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">//SetUp函数由RenderFeature的AddRenderPasses调用，传递最终的Target信息</span></span><br><span class="line">        _finalRenderTarget = finalRenderTarget;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">override</span> <span class="keyword">void</span> <span class="title">Execute</span>(<span class="params">ScriptableRenderContext context, <span class="keyword">ref</span> RenderingData renderingData</span>)</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (!GetParamsAndCheckValidation(<span class="keyword">ref</span> renderingData))</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// Debug.Log(&quot;Custom Bloom Failed&quot;);</span></span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">var</span> cmd = CommandBufferPool.Get(BufferName);</span><br><span class="line">        Render(cmd, <span class="keyword">ref</span> renderingData);</span><br><span class="line">        context.ExecuteCommandBuffer(cmd);</span><br><span class="line">        CommandBufferPool.Release(cmd);</span><br><span class="line">    &#125;</span><br><span class="line">  </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">override</span> <span class="keyword">void</span> <span class="title">OnCameraCleanup</span>(<span class="params">CommandBuffer cmd</span>)</span></span><br><span class="line">    &#123;</span><br><span class="line">        ReleaseRenderTextures(cmd);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="built_in">bool</span> <span class="title">GetParamsAndCheckValidation</span>(<span class="params"><span class="keyword">ref</span> RenderingData renderingData</span>)</span></span><br><span class="line">    &#123;</span><br><span class="line">         <span class="comment">//检查Material是否被成功创建、控制参数的Volume是否有效、对应后处理是否启用</span></span><br><span class="line">    <span class="comment">//...</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">Render</span>(<span class="params">CommandBuffer cmd, <span class="keyword">ref</span> RenderingData renderingData</span>)</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">//获取相机的相关信息，并执行渲染相关的函数</span></span><br><span class="line">        <span class="keyword">ref</span> <span class="keyword">var</span> cameraData = <span class="keyword">ref</span> renderingData.cameraData;</span><br><span class="line">        RenderTargetIdentifier source = _finalRenderTarget;</span><br><span class="line">        <span class="keyword">var</span> w = cameraData.camera.scaledPixelWidth;</span><br><span class="line">        <span class="keyword">var</span> h = cameraData.camera.scaledPixelHeight;</span><br><span class="line"></span><br><span class="line">        CreateRenderTextures(cmd, w, h);</span><br><span class="line">        </span><br><span class="line">        BloomAreaExtractionPassRender(cmd, source, _bloomRTArray[<span class="number">0</span>]);</span><br><span class="line">        BloomPassRender(cmd, source, _finalRenderTarget);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">CreateRenderTextures</span>(<span class="params">CommandBuffer cmd, <span class="built_in">int</span> w, <span class="built_in">int</span> h</span>)</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">//基于需求创建临时RT</span></span><br><span class="line">        <span class="comment">//...</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">ReleaseRenderTextures</span>(<span class="params">CommandBuffer cmd</span>)</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">//释放所创建的临时RT</span></span><br><span class="line">        <span class="comment">//...</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">BloomAreaExtractionPassRender</span>(<span class="params">CommandBuffer cmd, RenderTargetIdentifier source, RenderTargetIdentifier target</span>)</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">//提取Bloom区域Pass的执行</span></span><br><span class="line">        <span class="comment">//...</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">GaussianBlurPassRender</span>(<span class="params">CommandBuffer cmd, RenderTargetIdentifier source, RenderTargetIdentifier target</span>)</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">//处理高斯模糊的双Pass执行</span></span><br><span class="line">        <span class="comment">//...</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">BloomPassRender</span>(<span class="params">CommandBuffer cmd, RenderTargetIdentifier source, RenderTargetIdentifier target</span>)</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">//对Bloom区域进行降采样、升采样与最终Bloom效果的混合</span></span><br><span class="line">        <span class="comment">//...</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-2-参数控制器">2.2 参数控制器</h3><p>参数控制器我放在了Volume中，作为后处理的一个组件，所以控制效果的参数放在了一个继承自VolumeComponent和IPostProcessCompoment的类中。这样处理的优势是，可以方便地在一个Global Volume中与URP其他的后处理结合使用，同时制作分镜所用的Cinemachine支持在Timeline的shot中使用不同的Volume Profile控制不同镜头的后处理效果。代码如下：</p><figure class="highlight c#"><table><tr><td class="code"><pre><span class="line">[<span class="meta">Serializable, VolumeComponentMenuForRenderPipeline(<span class="string">&quot;Custom Post-processing/Bloom&quot;</span>, typeof(UniversalRenderPipeline))</span>]</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title">CustomBloomParams</span> : <span class="title">VolumeComponent</span>, <span class="title">IPostProcessComponent</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">//Activity Controller</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="built_in">bool</span> <span class="title">IsActive</span>()</span> =&gt; useCustomBloom.<span class="keyword">value</span>;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="built_in">bool</span> <span class="title">IsTileCompatible</span>()</span> =&gt; <span class="literal">false</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    [<span class="meta">Serializable</span>]</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">sealed</span> <span class="keyword">class</span> <span class="title">BloomAreaExtractionModeParameter</span> : <span class="title">VolumeParameter</span>&lt;<span class="title">BloomAreaExtractionMode</span>&gt; &#123;&#125; </span><br><span class="line">    <span class="keyword">public</span> <span class="built_in">enum</span> BloomAreaExtractionMode </span><br><span class="line">    &#123;</span><br><span class="line">        Luminance,</span><br><span class="line">        Color</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Parameters</span></span><br><span class="line">    <span class="keyword">public</span> BoolParameter useCustomBloom = <span class="keyword">new</span> BoolParameter(<span class="literal">false</span>);</span><br><span class="line"></span><br><span class="line">    [<span class="meta">Header(<span class="string">&quot;Bloom Perception&quot;</span>)</span>]</span><br><span class="line">    <span class="keyword">public</span> BloomAreaExtractionModeParameter bloomAreaExtractBy = <span class="keyword">new</span> BloomAreaExtractionModeParameter();</span><br><span class="line">    <span class="keyword">public</span> ClampedFloatParameter bloomThreshold = <span class="keyword">new</span> ClampedFloatParameter(<span class="number">0.0f</span>, <span class="number">0.0f</span>, <span class="number">1.0f</span>);</span><br><span class="line">    <span class="keyword">public</span> ClampedFloatParameter bloomIntensity = <span class="keyword">new</span> ClampedFloatParameter(<span class="number">1.0f</span>, <span class="number">0.0f</span>, <span class="number">2.0f</span>);</span><br><span class="line">    <span class="keyword">public</span> ClampedFloatParameter bloomScatter = <span class="keyword">new</span> ClampedFloatParameter(<span class="number">0.5f</span>, <span class="number">0.0f</span>, <span class="number">1.0f</span>);</span><br><span class="line">    <span class="keyword">public</span> ColorParameter bloomColorTint = <span class="keyword">new</span> ColorParameter(Color.white);</span><br><span class="line"></span><br><span class="line">    [<span class="meta">Header(<span class="string">&quot;Bloom Quality&quot;</span>)</span>] </span><br><span class="line">    <span class="keyword">public</span> ClampedIntParameter gaussianBlurIterations = <span class="keyword">new</span> ClampedIntParameter(<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>);</span><br><span class="line">    <span class="keyword">public</span> ClampedFloatParameter gaussianBlurSize = <span class="keyword">new</span> ClampedFloatParameter(<span class="number">0.6f</span>, <span class="number">0.2f</span>, <span class="number">3.0f</span>);</span><br><span class="line">    <span class="keyword">public</span> ClampedIntParameter basicDownSample = <span class="keyword">new</span> ClampedIntParameter(<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>);</span><br><span class="line">    <span class="keyword">public</span> ClampedFloatParameter finalGaussianBlurBlurSizeScale = <span class="keyword">new</span> ClampedFloatParameter(<span class="number">4</span>, <span class="number">1</span>, <span class="number">10</span>);</span><br><span class="line"></span><br><span class="line">    [<span class="meta">Header(<span class="string">&quot;Bloom Details&quot;</span>)</span>] </span><br><span class="line">    <span class="keyword">public</span> ClampedFloatParameter mipIntensity1 = <span class="keyword">new</span> ClampedFloatParameter(<span class="number">0.75f</span>, <span class="number">0.5f</span>, <span class="number">1.0f</span>);</span><br><span class="line">    <span class="keyword">public</span> ClampedFloatParameter mipIntensity2 = <span class="keyword">new</span> ClampedFloatParameter(<span class="number">0.85f</span>, <span class="number">0.5f</span>, <span class="number">1.0f</span>);</span><br><span class="line">    <span class="keyword">public</span> ClampedFloatParameter mipIntensity3 = <span class="keyword">new</span> ClampedFloatParameter(<span class="number">0.95f</span>, <span class="number">0.5f</span>, <span class="number">1.0f</span>);</span><br><span class="line">    <span class="keyword">public</span> ClampedFloatParameter mipIntensity4 = <span class="keyword">new</span> ClampedFloatParameter(<span class="number">1.0f</span>, <span class="number">0.5f</span>, <span class="number">1.0f</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>基于我的处理流程，有几个特殊的参数：BloomScatter用于控制最终表现效果“扩散”程度；BloomColorTint用于调整Bloom的颜色；MipIntensity1-4是用于控制最多4个模糊Mip混合的强度（数量由高斯模糊迭代次数决定），表现在从内到外最多四层的Bloom强度的分别控制。</p><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20231205170351452.png" alt="作为PostProcess组件表现在Volume中的控制器"></p><h2 id="3-获取Bloom区域">3 获取Bloom区域</h2><p>开篇提到过，提取Bloom区域有两种方式，一是直接根据颜色值提取，二是根据亮度提取。通过简单的ShaderFeature的控制，就可以把两个处理方式简化到一个Pass中进行。对优化应该没有影响，但是这样做逻辑会清晰很多。所以需要处理的就是在BloomAreaExtractionPassRender中设置不同的宏，代码如下：</p><figure class="highlight c#"><table><tr><td class="code"><pre><span class="line">_material.SetFloat(BloomThresholdID, _params.bloomThreshold.<span class="keyword">value</span>);</span><br><span class="line"><span class="keyword">if</span> (_params.bloomAreaExtractBy.<span class="keyword">value</span> == CustomBloomParams.BloomAreaExtractionMode.Luminance)</span><br><span class="line">&#123;</span><br><span class="line">    _material.EnableKeyword(<span class="string">&quot;_EXTRACTBYLUMINANCE&quot;</span>);</span><br><span class="line">    _material.DisableKeyword(<span class="string">&quot;_EXTRACTBYCOLOR&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (_params.bloomAreaExtractBy.<span class="keyword">value</span> == CustomBloomParams.BloomAreaExtractionMode.Color)</span><br><span class="line">&#123;</span><br><span class="line">    _material.EnableKeyword(<span class="string">&quot;_EXTRACTBYCOLOR&quot;</span>);</span><br><span class="line">    _material.DisableKeyword(<span class="string">&quot;_EXTRACTBYLUMINANCE&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">&#123;</span><br><span class="line">    _material.DisableKeyword(<span class="string">&quot;_EXTRACTBYCOLOR&quot;</span>);</span><br><span class="line">    _material.DisableKeyword(<span class="string">&quot;_EXTRACTBYLUMINANCE&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Shader方面则是在FragmentShader中对不同的宏进行不同的处理：基于亮度提取的话，需要计算出画面的亮度，然后用亮度减去提取阈值；而基于颜色提取，则直接对RGB同时减去阈值即可，相关代码如下：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#if defined(_EXTRACTBYLUMINANCE)</span><br><span class="line">float luminance = 0.2126 * mainTexCol.r + 0.7152 * mainTexCol.g + 0.0722 * mainTexCol.b;</span><br><span class="line">float mask = max(luminance - _ExtractThreshold, 0.0);</span><br><span class="line">bloomAreaColor = mainTexCol * mask;</span><br><span class="line">#elif defined(_EXTRACTBYCOLOR)</span><br><span class="line">bloomAreaColor = max(mainTexCol - _ExtractThreshold, 0.0);</span><br><span class="line">#else</span><br><span class="line">bloomAreaColor = mainTexCol;</span><br><span class="line">#endif</span><br></pre></td></tr></table></figure><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20231205205509026.png" alt="通过亮度提取Bloom区域(左)与通过颜色提取Bloom区域(右)提取后的结果"></p><h2 id="4-双Pass高斯模糊">4 双Pass高斯模糊</h2><p>高斯模糊的算法原理已经有很多文章写得很清楚了，我不在此赘述，而高斯模糊通过双Pass来降低采样贴图的次数也是人尽皆知的优化方法了，项目中这一部分也只是把两个Pass的执行优化到了两次Blit完成，并且封装到函数中，方便多处使用。封装后处理高斯模糊的函数如下：</p><figure class="highlight c#"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">GaussianBlurPassRender</span>(<span class="params">CommandBuffer cmd, RenderTargetIdentifier source, RenderTargetIdentifier target</span>)</span></span><br><span class="line">&#123;</span><br><span class="line">    cmd.Blit(source, GaussianBlurTempTargetID, _material, <span class="number">1</span>);</span><br><span class="line">    cmd.Blit(GaussianBlurTempTargetID, target, _material, <span class="number">2</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Shader代码和《UnityShader入门精要》中的代码相似，借助_BlurSize控制卷积核大小，来保持模糊效果的同时降低迭代次数，以实现优化。具体代码可以看我的Git仓库，为了文章精简，我就不贴在这里了。</p><h2 id="5-降、升采样与混合">5 降、升采样与混合</h2><p>降采样的原理时使用低分辨率的图存储上一级分辨率模糊处理后的图，将贴图的filtermode设置为Blinear，降低采样率时（目标为低分辨率RT）利用双线性插值采样来获得平滑的更模糊的图片，升高采样率时（目标为高分辨率RT）平滑低分辨率到高分辨率过渡的效果。</p><p>降采样获取模糊的RTMips时，为了加强模糊效果，并保持不同层级之间模糊效果的连续性，我是对上一层的RT作为源进行模糊处理，代码如下：</p><figure class="highlight c#"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="built_in">int</span> i = <span class="number">1</span>; i &lt; _params.gaussianBlurIterations.<span class="keyword">value</span> + <span class="number">1</span>; i++)</span><br><span class="line">&#123;</span><br><span class="line">    GaussianBlurPassRender(cmd, _bloomRTArray[i - <span class="number">1</span>], _bloomRTArray[i]);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20231205205854831.png" alt="4个Mip层级的降采样"></p><p>我处理升采样的流程与COD中分享的有一点不同。因为创建Mips数组时，每一个元素在Shader中都有对应的Property “_BloomRT_n”，所以我直接在一个Pass中进行了所有Mips层级的混合，并通过_BloomScatter去控制层级之间的混合强度，用于调整”扩散“的强度。不过这一块的处理我暂时没有找到灵活的方式，所以基于高斯模糊迭代次数在shader中写了4个静态分支，也因此控制了最大高斯模糊迭代次数为4。其中最复杂的_MipsCouts为4时，Mips混合的shader如下：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">else if (_MipsCounts == 4)</span><br><span class="line">&#123;</span><br><span class="line">    float4 bloomRT1 = SAMPLE_TEXTURE2D(_BloomRT_1, sampler_BloomRT_1, input.uv);</span><br><span class="line">    float4 bloomRT2 = SAMPLE_TEXTURE2D(_BloomRT_2, sampler_BloomRT_2, input.uv);</span><br><span class="line">    float4 bloomRT3 = SAMPLE_TEXTURE2D(_BloomRT_3, sampler_BloomRT_3, input.uv);</span><br><span class="line">    float4 bloomRT4 = SAMPLE_TEXTURE2D(_BloomRT_4, sampler_BloomRT_4, input.uv);</span><br><span class="line">    return lerp(</span><br><span class="line">        lerp(</span><br><span class="line">            lerp(bloomRT1 * _BloomMipIntensity.x, bloomRT2 * _BloomMipIntensity.y, _BloomScatter),</span><br><span class="line">            bloomRT3 * _BloomMipIntensity.z, _BloomScatter),</span><br><span class="line">            bloomRT4 * _BloomMipIntensity.w,</span><br><span class="line">            _BloomScatter);</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>为了混合后不同Mips层级之间的平滑，再多加了一次高斯模糊的处理，最后再与原图像进行混合。升采样与最终混合的代码如下：</p><figure class="highlight c#"><table><tr><td class="code"><pre><span class="line"><span class="comment">//Up-Sample</span></span><br><span class="line">    <span class="comment">//Mipmaps Blend</span></span><br><span class="line">_material.SetFloat(BloomScatterID, _params.bloomScatter.<span class="keyword">value</span>);</span><br><span class="line">_material.SetFloat(Shader.PropertyToID(<span class="string">&quot;_MipsCounts&quot;</span>), _params.gaussianBlurIterations.<span class="keyword">value</span>);</span><br><span class="line">_material.SetVector(BloomMipIntensityID, <span class="keyword">new</span> Vector4(_params.mipIntensity1.<span class="keyword">value</span>,</span><br><span class="line">    _params.mipIntensity2.<span class="keyword">value</span>, _params.mipIntensity3.<span class="keyword">value</span>, _params.mipIntensity4.<span class="keyword">value</span>));</span><br><span class="line">cmd.Blit(source, _bloomRTArray[<span class="number">0</span>]);</span><br><span class="line">cmd.Blit(source, _bloomRTArray[<span class="number">1</span>], _material, <span class="number">3</span>);</span><br><span class="line">    <span class="comment">//Final Blend</span></span><br><span class="line">_material.SetFloat(BlurSizeID, _params.gaussianBlurSize.<span class="keyword">value</span> * _resolutionScale * _params.finalGaussianBlurBlurSizeScale.<span class="keyword">value</span>);</span><br><span class="line">GaussianBlurPassRender(cmd, _bloomRTArray[<span class="number">1</span>], _bloomRTArray[<span class="number">1</span>]);</span><br><span class="line">_material.SetFloat(BloomIntensityID, _params.bloomIntensity.<span class="keyword">value</span>);</span><br><span class="line">_material.SetColor(BloomColorTintID, _params.bloomColorTint.<span class="keyword">value</span>);</span><br><span class="line">cmd.Blit(_bloomRTArray[<span class="number">0</span>],target, _material, <span class="number">4</span>);</span><br></pre></td></tr></table></figure><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20231205210237925.png" alt="Mips直接叠加后的结果(左)及对其进行一次高斯模糊处理后的结果(右)"></p><p>额外有一点要注意，分辨率与卷积核大小需要做对应关系的调整。在处理BlurSize时，不同的分辨率会带来不同的效果，所以为了适配分辨率的变化，需要基于分辨率去调整BlurSize，即卷积核的大小。值直接在CPU端计算一个ResolusionScale，然后直接在C#中作用到BlurSize上，不需要在GPU中重复进行计算。</p><figure class="highlight c#"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">readonly</span> <span class="built_in">float</span> BaseResolution = <span class="number">1920f</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="built_in">float</span> _resolutionScale;</span><br><span class="line"><span class="comment">//在初始设置的函数中计算</span></span><br><span class="line">_resolutionScale = Math.Max(Screen.width, Screen.height) / BaseResolution;</span><br><span class="line"><span class="comment">//在设置BlurSize时对控制器中获取到的参数进行调整</span></span><br><span class="line">_material.SetFloat(BlurSizeID, _params.gaussianBlurSize.<span class="keyword">value</span> * _resolutionScale);</span><br></pre></td></tr></table></figure><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20231205234805145.png" alt="Bloom开启前后的效果"></p><h2 id="6-代码架构">6 代码架构</h2><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20231205234117865.png" alt="GLR2后处理相关代码架构"></p><h2 id="参考链接">参考链接</h2><p>COD在SIGGRAPH 2014上的分享，PPT第143-170页是Bloom部分：</p><p><a href="https://www.iryoku.com/next-generation-post-processing-in-call-of-duty-advanced-warfare/">https://www.iryoku.com/next-generation-post-processing-in-call-of-duty-advanced-warfare/</a></p><p>FB佬的自定义后处理教程：</p><p><a href="https://zhuanlan.zhihu.com/p/385830661">https://zhuanlan.zhihu.com/p/385830661</a></p><p>Unity官方的自定义后处理教程：</p><p><a href="https://zhuanlan.zhihu.com/p/373273390">https://zhuanlan.zhihu.com/p/373273390</a></p><h2 id="最后">最后</h2><p>再放一遍做的MMD，虽然效果还有欠缺，但是也花了很久时间去做了。如果文章对你有所帮助的话，也请转到视频点个赞和关注，救救我惨淡的数据吧！</p><p><a href="https://www.bilibili.com/video/BV1ew41187H5/">https://www.bilibili.com/video/BV1ew41187H5/</a></p><p>后续还有整个制作工作流的分享文章。</p>]]></content>
    
    
    <summary type="html">在URP管线下探究原神的卡通渲染实现的技术分享文档，后处理部分</summary>
    
    
    
    <category term="Tech-Document" scheme="https://himoqiuhan.github.io/categories/Tech-Document/"/>
    
    
    <category term="Unity" scheme="https://himoqiuhan.github.io/tags/Unity/"/>
    
    <category term="URP管线" scheme="https://himoqiuhan.github.io/tags/URP%E7%AE%A1%E7%BA%BF/"/>
    
    <category term="卡通渲染" scheme="https://himoqiuhan.github.io/tags/%E5%8D%A1%E9%80%9A%E6%B8%B2%E6%9F%93/"/>
    
    <category term="技术美术" scheme="https://himoqiuhan.github.io/tags/%E6%8A%80%E6%9C%AF%E7%BE%8E%E6%9C%AF/"/>
    
    <category term="SRP自定义后处理" scheme="https://himoqiuhan.github.io/tags/SRP%E8%87%AA%E5%AE%9A%E4%B9%89%E5%90%8E%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>《来自风平浪静的明天》浅谈</title>
    <link href="https://himoqiuhan.github.io/2023/12/01/Anima-NaginoAsukara/"/>
    <id>https://himoqiuhan.github.io/2023/12/01/Anima-NaginoAsukara/</id>
    <published>2023-12-01T13:31:58.000Z</published>
    <updated>2024-07-27T15:13:59.650Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>自此我决定放飞我的博客，想写啥就写点啥，写下了点啥就放上来啥。动漫的感想得要趁着看完的时候写，拖到“再看一遍”就基本不会到“再看一遍”的后来了。</p></blockquote><p>风平浪静，但是让我内心波涛汹涌。想要写下点东西，但是心里又空落落的，心中的体会难以言表。主角团七人，各有不同的心意，在不同看的人心里能激起不同的涟漪，而我心中所荡漾的，便是那些回忆的、或者向往的年少时的时光。总感觉这么说有点装大人，但是，当我意识到我再也无法在现实世界中，像中学时那样轻易寻找到自己的喜欢时，难免会慨叹时光所带来的尘封，而我的心意也停留在了高三。耳畔会不停传来大海拍打岸边的声音，哗哗地冲刷着沙滩。</p><p>这是一个很简单的故事，主角团七人，彼此的心意相互交织，历经时光的变迁，历经磨难的考验，有的变了，有的没变，有的不愿意承认自己发生了变化，只想要维持当初的模样。其实我一直向往着这样长大的方式，大家在一个小小的地方，彼此都从小玩到大，到成家，到最终的肉身回归土地，大家日子都简简单单，邻里之间也很熟悉，充满了生活的味道。无论是儿时的学习，还是长大后的工作，都是为了生活而去生活，为了自己一个小小的归宿努力地、为了生活而生活。在最开始，《来自风平浪静的明天》便再一次向我展示了这样的一个，我所向往但再也无法到达的世界。</p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/NaginoAsukara-1.png" style="zoom:50%;" /><p>这是一个复杂的故事，因为大家的心意交错，每个人有各自的苦衷，无法表露出自己的心意，都希望压抑自己的心情，为了自己所喜欢的人而做出牺牲，是最纯真的爱意的味道。每个人的心意我不想在这里说出来，我只想说出自己的体会，因为每个人看完之后都会陶醉于他们的故事，或者说是因为他们的故事让看故事的人想起了自己的故事，而陶醉在这份，发生在不同人身上，但都是一样的心情。</p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/NaginoAsukara-2.png" style="zoom: 50%;" /><p>喜欢，但不能说出口。</p><p>当然，没想到千咲这一开始最讨厌的人设到最后变成了我最喜欢的人设</p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/NaginoAsukara-3.png" style="zoom:50%;" />]]></content>
    
    
    <summary type="html">风平浪静来自无法传达的心意，但是，风平浪静的大海开始涌动</summary>
    
    
    
    
    <category term="动漫" scheme="https://himoqiuhan.github.io/tags/%E5%8A%A8%E6%BC%AB/"/>
    
    <category term="感想" scheme="https://himoqiuhan.github.io/tags/%E6%84%9F%E6%83%B3/"/>
    
  </entry>
  
  <entry>
    <title>【Unity】URP中的仿原神渲染2.0</title>
    <link href="https://himoqiuhan.github.io/2023/11/22/Projects-GenshinLikeRenderingInURP2/"/>
    <id>https://himoqiuhan.github.io/2023/11/22/Projects-GenshinLikeRenderingInURP2/</id>
    <published>2023-11-22T12:06:16.000Z</published>
    <updated>2024-07-27T15:13:59.657Z</updated>
    
    <content type="html"><![CDATA[<h2 id="效果展示">效果展示</h2><blockquote><p>展示视频：【【卡通渲染/MMD】五等分の気持ち】 <a href="https://www.bilibili.com/video/BV1ew41187H5/">https://www.bilibili.com/video/BV1ew41187H5/</a></p></blockquote><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/73410871d211425732b5205f15856be.png" alt=""></p><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/c29cb1345f98a9f92545947d81d692c.png" alt=""></p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/cac5b96ce08adf526486128c49df132.png" alt=""></p><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20231204162907101.png" alt="夜间、白天及加入Bloom后的效果"></p><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/GLR2_Character_GeneralShow.gif" alt=""></p><h2 id="介绍">介绍</h2><p>对之前制作的仿原神渲染的进一步优化，相比于仿原神渲染1.0，这次着重升级了：</p><blockquote><p>省流一下：优化角色渲染、加入自定义的场景渲染管线、更好看的Bloom、更清晰的代码架构</p></blockquote><ul><li><p>角色部分</p><ul><li>【漫反射】面部刘海投影</li><li>【漫反射】适配多样化Diffuse贴图使用方式</li><li>【漫反射】更灵活更美观的Ramp处理方式</li><li>【高光】基于对Mat Cap新理解的Blinn Phong + Mat Cap组合高光</li><li>【边缘光】更灵活可控的Rim Light</li><li>【描边】区分区域可控的描边颜色</li><li>【描边】更平滑的描边表现（优化平滑法线计算方式）</li><li>【流程优化】更灵活的顶点色使用方式</li><li>【代码架构优化】更加清晰的代码架构</li></ul></li><li><p>场景部分</p><ul><li>自定义的一套简易、灵活的渲染管线，可以实现：多光源的光照处理、单个方向光的Shadow Mapping、Baked Lightmap、PBR材质、PBR光照模型（Phong + Cook-Torrance）</li><li>十分清晰的代码架构（详见思维导图和文档）</li></ul></li><li><p>后处理部分</p><ul><li>效果更好的Bloom</li><li>更加清晰的代码架构</li></ul></li></ul><h2 id="思维导图（图片较大，建议下载查看）">思维导图（图片较大，建议下载查看）</h2><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/GLR2-ShadingPart.png" alt="仿原神渲染2.0渲染部分的思维导图"></p><h2 id="文档指路">文档指路</h2><ul><li><p>代码仓库：<a href="https://github.com/himoqiuhan/URPToonRenderReconstruction">https://github.com/himoqiuhan/URPToonRenderReconstruction</a></p></li><li><p>渲染部分文档</p><ul><li>角色渲染：<a href="https://zhuanlan.zhihu.com/p/670601962">https://zhuanlan.zhihu.com/p/670601962</a></li><li>后处理：<a href="https://zhuanlan.zhihu.com/p/670604475">https://zhuanlan.zhihu.com/p/670604475</a></li></ul></li><li><p>MMD-Blender-Unity MMD制作流程经验分享：（Updating…）</p></li></ul>]]></content>
    
    
    <summary type="html">在URP管线下探究原神的卡通渲染实现，对1.0从效果、代码架构上进行迭代</summary>
    
    
    
    <category term="Portfolio" scheme="https://himoqiuhan.github.io/categories/Portfolio/"/>
    
    
    <category term="Unity" scheme="https://himoqiuhan.github.io/tags/Unity/"/>
    
    <category term="URP管线" scheme="https://himoqiuhan.github.io/tags/URP%E7%AE%A1%E7%BA%BF/"/>
    
    <category term="卡通渲染" scheme="https://himoqiuhan.github.io/tags/%E5%8D%A1%E9%80%9A%E6%B8%B2%E6%9F%93/"/>
    
    <category term="技术美术" scheme="https://himoqiuhan.github.io/tags/%E6%8A%80%E6%9C%AF%E7%BE%8E%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title>《夏日重现》浅谈</title>
    <link href="https://himoqiuhan.github.io/2023/11/21/Anima-SummerTimeRendering/"/>
    <id>https://himoqiuhan.github.io/2023/11/21/Anima-SummerTimeRendering/</id>
    <published>2023-11-21T11:23:51.000Z</published>
    <updated>2024-07-27T15:13:59.650Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>距上次写动漫感悟已经过了三个月，这期间也一直在看番，但是一直想着一部番只要要看三遍之后才算理解一部番，才能够去写一些有深度的感想，所以这期间一直没有写感悟。但是！《夏日重现》让我看完之后忍不住想要写一些东西，所以就在此做一个初见的杂谈吧</p></blockquote><p>昨晚十二点打开的夏日重现，到现在晚上十一点，一天看完了《夏日重现》，酣畅淋漓！不局限于悬疑和推理，在我看来，作者在作品中给出了他对关于自我、意识与记忆、时间的理解，最后通过梦境收尾，给出安于当下的建议，加之动画团队优秀的剧本和分镜，给我忙乱的生活按上了一天的暂停。</p><p>我不太侧重于剧情的叙述，而只是在这里说一说我对其中一些算是哲学观念的浅薄理解</p><h3 id="何为自我？">何为自我？</h3><p>我是自我的观察者，这是我认同的一个观点。不管是东方的释家，还是西方的部分哲学思想，都有着传达“我是自我的观察者”这样一个思想，看到这里的你，向心底说一声你好，那个回应你声音的人其实并不是你，而是你的头脑，实际的你只是在观察着这一切。男主慎平就是这样的一个形象，他的俯瞰就是观察自我的方式，不仅仅是在动画漫画里，现实里如果能剥离出当事人的身份，修行为一个观察者去反观自己的经历、情绪、行为，能帮助我们做出更加理智（大概率更恰当）的选择</p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/SummerTimeRendering1.png" alt="慎平的俯瞰" style="zoom: 50%;" /><h3 id="如果我有一个和我包括记忆都完全一样的另一个我，那么他会是我吗？">如果我有一个和我包括记忆都完全一样的另一个我，那么他会是我吗？</h3><p>看似动画里在淡淡传达记忆相同的另一个我就是我，但是全局思考下来，我认为作者关于这一方面也还是给出一个确切的理解（不过这也确实是无法给出答案的，毕竟这样的事无法验证）。我常常会思考一个离我们更近的问题，如果失忆了，或者缺失了某段重要的记忆，那我还会是我吗？如果再一次获取了记忆，那我还会是我吗？基于观察者的视角去看待这一切，那么记忆究竟是放在意识中的还是放在身体中的？不同组合的记忆所构筑的同一个人的意识是同一个意识吗？</p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/SummerTimeRendering2.png" alt="潮和影子潮一起拍的视频" style="zoom: 50%;" /><p>这些是很有意思的、但无法得到解答的问题。我个人其实是更偏向于那个我不是我，或者说我们会构成同一个个体，但是意识会是两个。所以，结局对我来是其实是带有半份伤感的，心里会止不住地期待潮和慎平能够恢复记忆，被重绘的夏天就是那两个意识共同亲自经历的夏天。</p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/SummerTimeRendering3.png" alt="重绘的夏天" style="zoom:50%;" /><h3 id="时间与梦境">时间与梦境</h3><p>平行世界的构想其实我们接触的很多了，感觉这个作品受到了不少《星际穿越》的影响，男主时间回溯也是用的平行世界的概念去进行的衔接，所以其实大家对这个的接受度应该都会很高，甚至可能因为看多了会觉得有点老套。</p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/SummerTimeRendering4.png" alt="结局潮穿梭时间，回去重绘夏天" style="zoom:50%;" /><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/SummerTimeRendering5.png" style="zoom:50%;" /><p>但是，动画团队优秀的叙事节奏和分镜的使用，让老套的架构也能紧紧吸引住观众。关于分镜这一点，我沉淀得不够多，所以不在此做分析。</p><p>最后，结局以一场梦，结束了轮回，重绘夏天。</p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/SummerTimeRendering6.png" style="zoom:33%;" /><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/SummerTimeRendering7.png" style="zoom:33%;" /><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/SummerTimeRendering8.png" style="zoom: 33%;" /><h3 id="答案是安于当下">答案是安于当下</h3><p>关于梦境的深入阐述，我放在了这一部分，我认为与这一部分更加契合。每个人或多或少都会有既视感，在某一刻感觉自己当下所经历的事情似乎在梦境中出现过，对于每天早晨都会意识到自己昨晚有做梦的我来说，更是如此。或许真的像动漫里描述的那样，我们是活在无数平行宇宙中的，我们无法得知，自己当下的生活是不是其他平行宇宙中的自己像动画里所讲的故事那样，拼尽全力搭建起来的。</p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/SummerTimeRendering9.png" alt="最后一战" style="zoom:50%;" /><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/SummerTimeRendering10.png" alt="慎平的肉体在常夜中溃烂" style="zoom:50%;" /><p>但我可以肯定，我们所处的现在一定是最幸运的。我们需要做的，就是怀着感恩的心，用心、用力地去过好每一刻的生活。不要被那么多过去束缚住自己，也不要被那么多未来扰乱自己的心智，工作就用心做好当下的活、看番就用心融入那个奇幻世界、吃饭就一口一口品尝够食物的味道、走路就一步一步感受脚与地面的接触。</p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/SummerTimeRendering11.png" style="zoom:50%;" />]]></content>
    
    
    <summary type="html">“是想再见珍视之人一面的愿望”</summary>
    
    
    
    
    <category term="动漫" scheme="https://himoqiuhan.github.io/tags/%E5%8A%A8%E6%BC%AB/"/>
    
    <category term="感想" scheme="https://himoqiuhan.github.io/tags/%E6%84%9F%E6%83%B3/"/>
    
  </entry>
  
  <entry>
    <title>【Unity】自定义渲染管线学习笔记（Updating）</title>
    <link href="https://himoqiuhan.github.io/2023/10/28/Notes-CatlikeCoding-CustomRenderPipline/"/>
    <id>https://himoqiuhan.github.io/2023/10/28/Notes-CatlikeCoding-CustomRenderPipline/</id>
    <published>2023-10-28T11:30:11.000Z</published>
    <updated>2024-07-27T15:13:59.650Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>封面和头图均来自这个太太：<a href="https://www.pixiv.net/users/38297201">https://www.pixiv.net/users/38297201</a></p></blockquote><blockquote><p>Catlike Coding中Custom Render Pipeline的学习笔记，原链接在这里：<a href="https://catlikecoding.com/unity/tutorials/custom-srp/">https://catlikecoding.com/unity/tutorials/custom-srp/</a></p><p>代码链接（带注释）：<a href="https://github.com/himoqiuhan/CatlikeRenderPartition">https://github.com/himoqiuhan/CatlikeRenderPartition</a></p><p>尝试写了一点文档式的笔记，但是写下来后感觉，只是基于我不扎实的英语做的一个拙劣的汉化版，自己的理解不够深入。加之学习着学习着，愈发发现写管线就是个工程学的问题，更重要的是如何统筹业界已有的技术。所以在这一篇文章中持续更新我学习时写下的的思维导图，导图的框架基本是基于我写代码的框架，其中也有一些对Catlike上内容的补充，其中有不详细的内容可以去到对应的文章中查看。</p><p>注：原图过大可能无法正常显示，推荐右键下载查看</p></blockquote><h2 id="Basic-Custom-RP">Basic Custom RP</h2><p>原文链接：<a href="https://catlikecoding.com/unity/tutorials/custom-srp/custom-render-pipeline/">https://catlikecoding.com/unity/tutorials/custom-srp/custom-render-pipeline/</a></p><p>简述：创建自定义渲染管线，并实现一些基础内容绘制，入SkyBox、Unlit物体等</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/CustomRenderPipeline.png" alt="Custom Render Pipeline"></p><h2 id="Shaders-And-Batches">Shaders And Batches</h2><p>原文链接：<a href="https://catlikecoding.com/unity/tutorials/custom-srp/draw-calls/">https://catlikecoding.com/unity/tutorials/custom-srp/draw-calls/</a></p><p>简述：SRP环境下的Shader写法，以及Unity中的Batching合批技术，包含SRP Batcher、GPU Instancing、Static/Dynamic Batching</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/ShadersAndBatches.png" alt="Shaders And Batches"></p><h2 id="Directional-Lights">Directional Lights</h2><p>原文链接：<a href="https://catlikecoding.com/unity/tutorials/custom-srp/directional-lights/">https://catlikecoding.com/unity/tutorials/custom-srp/directional-lights/</a></p><p>简述：自定义管线下的基础光照实现、与光照信息解耦的BRDF处理，以及自定义的材质GUI制作</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/DirectionalLight.png" alt="Directional Light"></p><h2 id="Directional-Shadows">Directional Shadows</h2><p>原文链接：<a href="https://catlikecoding.com/unity/tutorials/custom-srp/directional-shadows/">https://catlikecoding.com/unity/tutorials/custom-srp/directional-shadows/</a></p><p>简述：基于Shadow Mapping的阴影实现，以及Cascade Shadow级联阴影、PCF软阴影的实现</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/DirectionalShadow.png" alt="Directional Shadow"></p><h2 id="Baked-Light">Baked Light</h2><p>原文链接：<a href="https://catlikecoding.com/unity/tutorials/custom-srp/baked-light/">https://catlikecoding.com/unity/tutorials/custom-srp/baked-light/</a></p><p>简述：通过自定义的Meta Pass烘焙并采样含有Diffuse Reflectivity和Emission的Lightmap，采样Light Probe以及LPPV的原理概述</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/CatlikeRenderBakedLight.png" alt="Baked Light"></p><h2 id="Shadow-Masks">Shadow Masks</h2><p>原文链接：<a href="https://catlikecoding.com/unity/tutorials/custom-srp/shadow-masks/">https://catlikecoding.com/unity/tutorials/custom-srp/shadow-masks/</a></p><p>简述：烘焙ShadowMask、采样ShadowMask并使烘焙阴影能够和实时阴影融洽融合</p><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/CatlikeRenderShadowMasks.png" alt="Shadow Mask"></p><h2 id="LOD-and-Reflections">LOD and Reflections</h2><p>原文链接：<a href="https://catlikecoding.com/unity/tutorials/custom-srp/lod-and-reflections/">https://catlikecoding.com/unity/tutorials/custom-srp/lod-and-reflections/</a></p><p>简述：LOD部分利用Unity的LOD Group Component实现LOD，同时使用dither来确保LOD之间的切换平滑；Reflection部分增加IndirectBRDF的处理，环境反射暂时只支持对Skybox的反射</p><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/CatlikeRenderLODAndReflections.png" alt="LOD and Reflections"></p><h2 id="Complex-Maps">Complex Maps</h2><p>原文链接：<a href="https://catlikecoding.com/unity/tutorials/custom-srp/complex-maps/">https://catlikecoding.com/unity/tutorials/custom-srp/complex-maps/</a></p><p>简述：在渲染管线中加入对HDRP中MODS(Metallic, Occlusion, Detail, Smoothness) Map、Detail Map、Normal Map的支持</p><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/CatlikeRenderComplexMaps.png" alt="Complex Maps"></p><h2 id="Point-and-Spot-Lights">Point and Spot Lights</h2><p>原文链接：<a href="https://catlikecoding.com/unity/tutorials/custom-srp/point-and-spot-lights/">https://catlikecoding.com/unity/tutorials/custom-srp/point-and-spot-lights/</a></p><p>简述：加入对Point Light和Spot Light的光照的计算，以及其Baked Shadow的计算</p><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/CatlikeRenderPointAndSpotLights.png" alt="Point and Spot Lights"></p><h2 id="Point-and-Spot-Shadows">Point and Spot Shadows</h2><p>原文链接：<a href="https://catlikecoding.com/unity/tutorials/custom-srp/point-and-spot-shadows/">https://catlikecoding.com/unity/tutorials/custom-srp/point-and-spot-shadows/</a></p><p>简述：同Directional Light一样使用Shadow Atlas来实现Point Light和Spot Light的实时阴影</p><p><img src="https://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/CatlikeRenderPointAndSpotShadows.png" alt="Point and Spot Shadows"></p><h2 id="Post-Processing-Updating">Post Processing (Updating)</h2><p>原文链接：<a href="https://catlikecoding.com/unity/tutorials/custom-srp/post-processing/">https://catlikecoding.com/unity/tutorials/custom-srp/post-processing/</a></p><p>简述：为自定义的渲染管线中加入后处理，并添加Bloom效果</p><p>…</p><blockquote><p>What Next：虽然现在说有点早，但是还是立一个志向：自己从零搓一个卡通渲染管线！在那个时候或许会同步写一些更多基于自己理解的文档出来</p></blockquote>]]></content>
    
    
    <summary type="html">Unity自定义渲染管线学习笔记</summary>
    
    
    
    <category term="Notes" scheme="https://himoqiuhan.github.io/categories/Notes/"/>
    
    
    <category term="TA" scheme="https://himoqiuhan.github.io/tags/TA/"/>
    
    <category term="Unity" scheme="https://himoqiuhan.github.io/tags/Unity/"/>
    
    <category term="Render Pipeline" scheme="https://himoqiuhan.github.io/tags/Render-Pipeline/"/>
    
  </entry>
  
  <entry>
    <title>《富爸爸穷爸爸》读后感</title>
    <link href="https://himoqiuhan.github.io/2023/09/20/Reading-RichDad/"/>
    <id>https://himoqiuhan.github.io/2023/09/20/Reading-RichDad/</id>
    <published>2023-09-20T07:22:46.000Z</published>
    <updated>2024-07-27T15:13:59.659Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>概述：本书对我来说最大的价值在于，理财意识的启蒙以及一些思维的培养。书中提到的具体的一些方法可能过时，但是比起方法，更多的是要去实践、理解背后的商业逻辑运行原理，也就是抓住财商的“道”。书中内容的大体逻辑可以分为两层，第一层为理论层面，内容指向现金流的一些基础知识、作者对富人穷人使用现金的分析、税收、投资等一些偏概念性的角度；第二层偏向实践，作者提供了一些树立致富意识和落地实际行动的方法论，不过对于行动在当今是否可取，就需要读者结合实际去思考与抉择。</p></blockquote><h2 id="思维和意识的建立">思维和意识的建立</h2><h3 id="面对金钱需要有理性的思考">面对金钱需要有理性的思考</h3><p>大多数人活在对金钱的欲望和恐惧之中，没有能够很好地处理对待欲望与恐惧的方式，导致自己情感战胜了思想，深陷“上班——拿工资——处理债务”的泥潭之中，无法翻身。所以在一开始，我们就需要在思想上给自己提出一个要去：<strong>克制住自己的恐惧与贪婪，不要被其控制，而是站在一个更高的角度去利用他们</strong>。书中提到了一个很有效的方法，在面对一个诱惑时，先去问问自己“我失去了什么？”，让自己抓住理性的绳子，分析利弊之后再做抉择。</p><p>思维层面的内容，不单单可以用在经济管理上，也可以用在生活中大大小小的选择中。如果想要找到自己心中理性的绳子，那么在分析问题时就需要专注于自身，不要把思考的结果归咎到环境上，而是去思考自己能够控制的事情，<strong>在希望得到转变时，先去思考自己改变自己的可能性</strong>。</p><p>同时，在自己努力的过程中，也一定要保持思考的习惯。说到底，你想要的生活，不是靠无效努力堆积出来的，如果没有深度思考，所有的勤奋都是白搭，正如这本书中作者所说的：“<strong>未经思考的努力，才是我们贫穷的根源。</strong>”</p><h3 id="现金流">现金流</h3><p>说回到钱本身，钱能满足很多欲望，可是没钱甚至会让基本生存需求受到威胁，他是我们手中可以随意支配，但在当今社会甚至可以影响到生命的东西，我们必须要认真思考它的意义和用途。但是，钱本质上只不过是一些代币，并且这一份代币本身的价值是我们完全不可控的。说到这里，就要引出书中提及的四个关键词了：<strong>收入、支出、资产、负债</strong>。收入和支出都很容易理解，就是总的现金流的输入和输出，而资产和负债就有必要仔细理解一下了。资产是我们不需要投入太多时间和精力去管理，就可以为我们提供资金来源或者未来有升值空间的东西，简单来说就是带来正收益的自动化产钱机器。负债的概念其实也算是通俗易懂，就是像贷款那样需要我们定期付出资金的东西。重点在于对资产和负债的认识和理解，书中提到了很不符合大多数人观念的一点——”房子不是你的资产，购入房子购入的是负债“。这句话说得稍微会有一点绝对，但是重点在于梳理这句话背后的逻辑关系。对大多数人来说，买房是需要贷款的，并且房屋需要定期支付物业费、维修费，这意味着买一套房子后，我们在很长一段时间内（尤其是房贷还款期内），房子对我们来说是一个负债，定期在拿走我们的资金。我在这里传达的并非是一个绝对的观点，相信能够读到这里的人也能想明白另一面是什么，也是是说能够带来正向收益的房子就是你的资产。</p><p>接着四个概念的介绍，就可以进一步介绍现金流的处置了。以下三张图从左到右分别表示穷人的现金流、中产阶级的现金流、富人的现金流。</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230921000226135.png" alt="不同资金状况类型的人有不同的现金流"></p><p>可以看到，穷人在拿到收入之后，全部的资金作为支出去弥补基础的生活需求去了；中产阶级的收入则是在处理完债务之后，流向了基础生活所需的支出上；而富人的现金流则是由资产流向收入。这一个对比图中所表现的核心在于，不同阶级的人产生收入的方式不一样。富人相对于穷人和中产阶级，他们通过资产来产生收入，而不是由工作来产生收入。</p><p>这一点很容易让我们心生疑问：“那么他们的资产是如何来的呢？”虽然说当今的背景下，想要获取巨额的资产越来越难，但是我们可以通过一点一滴的积累去逐步构建自己的资产体系，借助复利来实现一定程度上的财务自由，或者较大程度上降低自己金钱方面的压力——也就是书中提到的“<strong>构筑资产项</strong>”。在这里，需要先提一个新的与我们平常理解相异的观点，就是对事业的理解。事业与职业是有所区分的，不同于我们以往所理解的工作，事业与职业或工作有着本质上的区别。职业需要通过工作来获得报酬，而事业是不需要自己去工作就能获取报酬，结合之前提到的资产的概念，建立事业其实就是在构筑自己的资产项。</p><p>同时，还有一个值得学习的观念是，<strong>先花钱给自己</strong>。我所接受并且内化的方法是，在拿到一笔钱后，先把固定的划分好的金额用于构筑资产项，通过这样的方式去减少自己在其他价值更低方面的开销，把能自由支配的钱优先由于构筑自己的资产项。</p><p>并且要时刻警惕购买奢侈品这个举措，富人和穷人对奢饰品的处置方式天差地别。富人是后购买奢饰品，穷人先购买奢饰品。如何理解这句话呢，富人<strong>通过购买资产，基于资产的收益去购买奢侈品</strong>；而穷人往往会在刚拿到钱的一开始就拿去购买奢侈品，这样带来的结果是，自己的负债会越来越多。不是说我不去用白条、信用卡购买商品就算是后购买奢侈品，而是需要当前的资产能够保证那个奢侈品的开销以及所有后续开销后，再去购买奢侈品。我想到的一个很确切、很贴近我思想的例子是，买车（因为很想痛车）。买车需要考虑的不仅仅是买车时的支出，还必须要考虑到之后每天的油耗、每天的停车费用、每年的车审费用以及不定期的保养费用，只有在有一项资产能确保这一系列的支出，不需要额外花其他拿去购买资产的钱之后，才应该去购买这辆车。只有这样做，才会让“车”这一项自己努力的“奖励”真正成为奖励，让自己进入一个越来越富裕的阶段，千万不能为了购买一个商品，让自己购入大量的负债，陷入老鼠赛跑。</p><p>所以说，在作为一个普通的打工人，通过工作获取主要现金流的阶段下，要去重视自己资产项的构筑，并且要给自己定下一条死线，<strong>当一笔钱成为自己的资产后，就不要把他再转变回货币</strong>，这也是复利效应提到的最重要的一点，不要轻易停下。</p><h3 id="不惧失败">不惧失败</h3><p>我们需要有面对风险，和分析失败的胆量。勇气与知识对于构筑资产项是十分重要的工具，勇气需要的是在看到机会后有胆量投入，知识是能够有基础的学习与认知，投资时愿意接受失败、正面失败并且从失败中进行学习。风险是绝对存在的，只能通过增加自己的财商知识，来降低风险。但即使风险存在，也不能因为害怕失去而不敢付出。如同最近在赛马娘中看到的一个本源的道理，奇迹只会在那些相信他并且为之付出大举努力的人身上发生，在投资上，自己的努力就是克服自己对失去的恐惧、并不断地学习知识，这样才有可能会有成为富人的机会。</p><blockquote><p>有时我把投资看作网球比赛。我卖力地去打，犯了错误，然后纠正，再犯更多的错误，然后再纠正，这样水平就提高了。如果我输了，我会走向球网，和对手握手，笑着对他说：“下周六见。”——Robert</p></blockquote><h3 id="高阶思维">高阶思维</h3><p>高阶思维这个词看起来很高大上，我所理解的是“看透选项背后的选项”。面对选择时我们不能只看到它表面所带来的收益，因为大多数目前看来收益很高的选项，他的长期效益是很低的，这时就需要我们去建立高阶思维，去看到选择了这个选项之后会发生的事情。落实到书中，就是“选择那些从长远来看更能让你成长的工作，而非短期看似薪资高，但是把你专业化的工作”。除去高阶思维，抵抗短期高收益的诱惑外，这句话还有一个很重要的道理：不要把自己过度专业化。我所指的并不是学习不要学得深入，而是不要用一个词语去定义自己。</p><p>用形容词修饰自己，能够让自己在思想上就认同自己是一个可拓展性很强的人，并且基于此去不断地学习进步、广泛涉猎各个对自己有益的领域。其中有一个很重要的领域就是营销。营销能力的背后，是一个人是否有胆量面对别人的拒绝、是一个人沟通的能力、是一个人分析自身优势劣势的能力，这些能力都是成功所必需的。</p><blockquote><p>您的销售能力——沟通和自我优势定位——直接影响成功与否;</p><p>成功所必需的管理能力包括：现金流管理；系统管理；人员管理</p></blockquote><h2 id="财商的培养">财商的培养</h2><p>财商，即理财智商，书中提到财商由四点构成：<strong>会计、投资、市场、法律</strong>。还是再来重复声明一些，本书内容之于写成文章时的我，更多的是对我思维的影响和塑造，其中很大范围的内容我都还没有实践过，也就没有目前可以被参考的行之有效的具体方法，只是基于我当下所思所悟而写下的一些理解。在我具体落实一些方法，并且总结经验后我会再多补充。</p><p>接着说组成财商的四点，一是会计，指的是对当前所拥有资产的处理能力、财务知识，归结到最根本上是<strong>对数字的处理能力，与对一项生意优劣势的分析能力</strong>。会计所指向的是过去，或者说是当下的所有物，对所有物进行恰当的处理，是增长自身经济实力的坚实基础；二是投资，借用从书中学到的概念来说，投资指花钱去购买资产项，<strong>使钱生钱的科学和策略</strong>。投资着眼于未来，着眼于拥有更多，是经济实力向外拓展的表现和途径；市场和法律就不用多说，这两点我认为更多的是作为一个辅助，去保障前两者的相互增长，也是前两项的理论基础。我们需要了解市场的供给需求和市场条件，以对现有的资产做出利益尽可能高的处理方式，并寻找到更多的新机会。同时还需要有一定的法律了解基础，利用法律在合法的范围内尽可能增加利益，并且纠纷中保护好自己。其中重点需要关注的是税法和公司法，这两者可以帮助我们利用减税优惠尽可能地去降低自己的税收，并在诉讼中得到保护。</p><h2 id="克服困难，迈出一步">克服困难，迈出一步</h2><p>同样也是理念上的建成，不过这一部分的内容就会更偏实际一点。因为内容更偏向方法论，所以我的困惑和思考也更多了一些。作者在书中提到，致富面前有五大难关：恐惧、愤世嫉俗、懒惰、坏习惯、自负，下面我就一一说一下我读完书之后的的想法。</p><p>第一，恐惧：我认为这一方面本质上是要我们<strong>锻炼面对失败的态度</strong>，“有从未亏一分钱的穷人，但绝对没有从未亏钱的富人”。这一部分作者传达的核心要素是勇气，要敢于去投资，而不是简单地把钱放在银行里，追求最简单的平衡。像作者这样大力鼓动投资的行为在我一开始看来感觉是有一点激进的，但是慢慢思考后，我发现我一开始对投资和赌博之间的关系的理解是模糊的，这导致了我有点误解投资这个概念了。投资和赌博是有着明确的界限的，投资主要靠已知获得收益，而赌博更多靠的是像运气这样未知的部分。投资需要我们结合理论知识和实践经验，在变幻莫测的市场中寻找到最能获利的路径。理论知识需要的是我们主动去学习，是投资给自己的最直接的表现；实践经验需要的就是我们的勇气：迈出第一步的勇气，和敢于直面失败的勇气。这确实说起来简单，做起来难，失败对人的影响固然是很大的，希望将来的我，以及读到这里的每一个人都能够在失败后拾起勇气，认真分析一段经历所带来的收获，并在这一步的基础上有胆量迈出下一步。</p><blockquote><p>犯错是好事，前提是我们能够从每一次失败中吸取教训。</p></blockquote><blockquote><p>“风险来自于你不知道自己在做什么”——沃伦·巴菲特</p></blockquote><p>第二，愤世嫉俗。借由书中提到的“小鸡”一词来叙述这一章带给我的收获，每个人心中都有一只小鸡，为我们铺垫巨量的不愿迈出脚步的借口，同时也吵闹着让别人和自己一样不要迈出脚步。“<strong>失败者抱怨现实，成功者分析现实</strong>”，越是不好的时代，藏留在现状背后的机会越多，因为大家都在抱怨并且止步不前，只有能只身打破这一束缚，才会有向前迈进的机会。“我不想要，这是成功的关键”，面对一个不想要得到的结果时，不要把目光局限于结果之上，而是去分析如何做。对于自身成长，对于通向成功的一步步积累，过程是绝对重要的，哪怕是在一个只看重结果的时代，过程才是唯一的解。</p><blockquote><p>Don’t complain, let the other people complain. —— 马云</p></blockquote><p>第三，懒惰。作者在书中将这一点侧重在了思考方式的塑造上，也就是从“我可付不起”的思维方式，转变成“<strong>我该如何付得起</strong>”的思考模式。不要取定义或强调问题，而是去寻找问题的解决方法，只有在思想上迈出那一步，才会有行动的可能，才能迈出前进的步伐。同时，关于懒惰还有一个很深刻、实际且经常发生的事实，那就是“假努力”。很多时候我们看似很忙，一天到晚得不到一点清闲，但一段时间后感觉自己忙得非常空虚，找不到忙的任何一点意义，反观别人，努力过程更轻松的同时取得了更耀眼的成绩。这归根究底来源自我们的懒惰，我们逃避开了那些真正重要的事情，最后的结果是时间和精力都花在了做无用功，或是短期有高收益但收益很快就会消失之类的事情上。</p><p>第四，坏习惯。这一部分主要说了富爸爸和穷爸爸对待资金的处置方式。穷爸爸会先把钱花给别人，用来支付账单、缴付日常支出，再去考虑为自己花费；而富爸爸<strong>先把钱付给自己，购置资产</strong>，再去付钱给政府。通过外界必然存在的压力来逼迫自己寻求更多赚钱的方法，这是富爸爸提供的方法。如果先付钱给别人，等到付钱给自己却发现自己没钱时，自己会觉得理所当然心安理得，但如果先付钱给自己，等到发现自己没钱支付账单时，压力便会驱使自己去寻求更多方法。通过外界的压力来逼迫自己为自己花钱，这一点我也在使用，但是不知道到后续其他方面压力过大时，这一方面的压力我是否能支撑住，我更倾向于富爸爸的做法，但是需要具体情况具体分析。</p><p>第五，自负。说到底，我认为自负恰恰来源于未知，仅仅知道事物的一个小角落，对其他区域的未知立刻就给自己营造了一种我已经掌握一切的错觉。要想挣脱自负的束缚，必须要从自己心底去培养谦逊的态度，随时去思考自己对事物的理解是否有所偏差，时刻分析这一方面是否有能够做得更好，保持谦虚的态度去询问同样领域取得更高成就的人，以让自己快速找到自己在这一方面上的”盲点“。“如果你知道自己在某一问题上有所欠缺，你就应该找一位本领域的专家或是一本相关的书，马上开始教育自己。”面对自己未知的领域，请不要去胡言乱语，如果是自己需要掌握的方面，就去寻找资料进行学习；如果是自己未知，但是十分重要的领域，就通过更加专业的途径去探索它。同时，还要能够看出别人是否在自吹自擂，是否在他自己未知的领域通过一些技法表现得“侃侃而谈”</p><blockquote><p>他们试图用自吹自擂来赢得争论，而我很清楚，这是因为他们根本不知道自己在说些什么。他们并没有撒谎，只是没有讲出真相。</p></blockquote><p>然后就是本书的最后一部分内容了，开始行动。按照我的风格，对于这些过于具体的东西，我会先说一说自己能回忆起的片段以及自己对它的内化在产出，然后再一一罗列书中的原文，以供后续参考。</p><p>首先是在精神层面找到动力，寻找一个想要成为富人的理由，通过“我不想要什么”来思考出“我想要什么”，然后以此为自己精神的立足点，支撑着自己去一步一步提高财商。我觉得这样的认知有点不妥，或者说我觉得不够强大，应该找到的是内心真正想要的东西，并且大概率不是基于短期物质的，这样才能给自己的额外学习提供源源不断的动力。</p><p>第二是去控制花钱的方式，选择去为自己投资，并且养成自律的消费习惯。事实上，一个人唯一的资产是他的头脑，对自己的头脑进行投资，不断去学习新的内容，让自己能够更好地在将来做出更优的选择。同时，需要控制自己的资金流向情况，先花钱给自己。当获得一笔资金后，优先考虑花钱构筑自己的资产项（自我的教育投资也算在资产项里面），然后再去考虑付钱给他人的方面，借助别人带来的压力去进一步锻炼赚钱的思维，并为之采取行动。</p><p>第三是处理好人脉关系，尤其是处理与富人之间的人脉关系时，去关注他们为什么能成功，关注他们能给到自己的信息，用领先于市场的信息去进行投资往往能带来更有成效的收益。并且需要处理好对周围“小鸡式”人所说出的话的态度，不要随波逐流、愤世嫉俗，而是去建成自己坚信、并且切实可靠的理财思路。</p><p>第四是付出。想要获得回报，首先必须要是付出，所谓舍得，有舍才能有得，有付出才会有收获。关于这一点，Robert拆分为了两个方面，或者说由经济的付出引申出了其他方面的内容。我能够理解教育、友情、爱情的付出，先付出才能有所收获，但是我不能理解的是关于经济的一点——捐赠。文中说，富爸爸会捐赠资金给福利院、养老院，而穷爸爸把“自己有钱就会捐钱”的话挂在嘴边，却从未捐赠过一分。我现在的理解是，首先玄学一点来说是在积德，其次实际一点可能是寻求自我实现、自我价值提升，以及为未来的人际关系进行投资。“捐赠”方面的付出，我需要后续再去深入思考，并且与人讨论讨论，也欢迎读到这里的小伙伴加我好友来和我一起探索！</p><blockquote><p>轻松的道路往往会越走越艰难，而艰难的道路往往会越走越轻松。</p></blockquote><p>以下则是书中提及到的内容：</p><ol><li>我需要一个超现实的理由——精神的力量</li><li>每天做出自己的选择——选择的力量：重点在于愿意付出资产去学习，保持谦虚好学的心态，透过学习让自己一次比一次更好地选择；并且不害怕失败，因为学习路上必然有失败，失败才能带来更快的进步；</li><li>慎重地选择朋友——关系的力量：理性客观地看待别人所说的话语，不要随波逐流；并且通过关系来获得内部消息，找到下一个“赚钱点”</li><li>掌握一种模式，然后再学习一种新的模式——快速学习的力量：重点在于保持好学的态度，并且去注意要学习的内容，因为”你学到什么就会成为什么样的人“；“你不仅仅要学习更快的赚钱模式，还要更快地学习新的赚钱模式”</li><li>首先支付自己——自律的力量：这是最难掌握的一步，需要遵循以下两个法则：一是不要让自己背上数额过大地债务包袱，保持低支出，首先建立资产项，然后再用资产项购买大房子或豪车；二是当你资金短缺时，让压力去发挥作用，不要动用你的储蓄或投资，利用压力去激发你的理财天赋，想出新的赚钱方法，然后支付你的账单。这两个法则并不是鼓励自我牺牲或理财紧缩，它并不意味着首先支付自己后就饿肚子，而是唤醒自己的理财天赋，然后借助理财天赋去拥有美好的东西</li><li>给你的经纪人以优厚的报酬——好建议的力量：不吝啬于获取信息，筛选并采纳好的建议</li><li>做一个“印第安给予者”——无私的力量：让初始投资回本，并且要快速回本</li><li>用资产来购买奢侈品——专注的力量：控制住自己购买奢侈品的欲望，拥有控制金钱的坚强意志，让现金流优先流向资产项，通过资产项产生的现金流再去购买奢侈品</li><li>对英雄的崇拜——神话的力量：假装自己变成了自己最崇拜的英雄，通过这一方法去学习，挖掘出自身潜能，例如去想想”巴菲特会怎么做“</li><li>先予后取——给予的力量：如果你想要获取，就要先给予，无论金钱、微笑、爱情、友谊还是教育都是如此。</li></ol><blockquote><p>开创事业所必备的最重要的3种管理技能分别是：现金流管理、人事管理和个人时间管理</p></blockquote><h2 id="最后">最后</h2><p>以上便是我初次读完这本书，结合读书做得随记和后续的思考，所产出的感想。如果感兴趣的话，可以去找到这本书读一读，也欢迎找我聊天，无论和这本书有没有关系，都没问题。因为基本没有基于书本的框架来写读后感，所以在结构上与原书有些差异，并且部分内容会用重复，见谅！感谢你一直读到这里，希望我写下的这些能够引起你的一些思考，不管认同与否，对错与否，一次思考便是一场大脑的新奇冒险，也是我们两个灵魂之间跨越时空的一次交流。</p>]]></content>
    
    
    <summary type="html">未经思考的努力，才是我们贫穷的根源</summary>
    
    
    
    <category term="Impression" scheme="https://himoqiuhan.github.io/categories/Impression/"/>
    
    
    <category term="感想" scheme="https://himoqiuhan.github.io/tags/%E6%84%9F%E6%83%B3/"/>
    
    <category term="阅读" scheme="https://himoqiuhan.github.io/tags/%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>【Unity】URP中的仿原神渲染1.0</title>
    <link href="https://himoqiuhan.github.io/2023/08/31/Projects-GenshinLikeRenderingInURP1/"/>
    <id>https://himoqiuhan.github.io/2023/08/31/Projects-GenshinLikeRenderingInURP1/</id>
    <published>2023-08-31T09:46:40.000Z</published>
    <updated>2024-07-27T15:13:59.657Z</updated>
    
    <content type="html"><![CDATA[<h2 id="介绍">介绍</h2><blockquote><p>展示视频：【【仿原神渲染/MMD】我推的芭芭拉】<a href="https://www.bilibili.com/video/BV1F841167nb">https://www.bilibili.com/video/BV1F841167nb</a></p></blockquote><p>十分喜欢赛璐璐风格的卡通渲染，正好开始那段时间时间在学习HLSL，心想不如基于一个实际落地的效果去边学边练，于是就开启了一段美妙（并且经常痛苦）的学习旅程！</p><p>最终的效果差强人意，但也是很好地标明了一个起点，持续改进中！</p><p>像视频简介里面说的那样，这一段时间为了在已有资源的基础上做好卡通渲染，还简单学了Maya的Python，从MEL到OpenMaya都接触了一点，最后写出了一个用于计算并存储平滑法线信息的小工具。为了在URP里写后处理，去学了URP的RenderFeature，接触了一点点SRP的知识。不过这些学习大多都是基于想要实现的效果去学的，后续还要花一些时间去整理和部分地深入，我也打算一边总结一边写一些分享文章。在这篇文章里，我就简单聊一下我是如何使用手上的资源实现目前渲染效果，关于一些技术他们更深的底层原理以及我对他们运用方式的思考，不久后我也会写出来的。毕竟，卡通渲染无法一概而论，其关注的是最后的效果，对于一个优秀卡通渲染的研究，一定要深入到每一个技术细节存在的前因后果，这样才能一步步做出更好的卡渲。</p><h2 id="总览">总览</h2><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/%E3%80%90%E9%A1%B9%E7%9B%AE%E8%90%BD%E5%9C%B0%E3%80%91%E4%BB%BF%E5%8E%9F%E7%A5%9E%E6%B8%B2%E6%9F%931.0.png" alt="MMD中用到的相关技术"></p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230902100453437.png" alt="这次我用到的工作流"></p><h2 id="贴图资源简述">贴图资源简述</h2><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230831182658671.png" alt="贴图资源"></p><p>用到的贴图资源如上所示，有basecolor贴图、lightmap贴图、ramp图、matcap贴图和自制的用于存储平滑法线和描边区域信息的outline贴图，简单介绍一下各个贴图的用法</p><h3 id="lightmap-ilmTexture">lightmap - ilmTexture</h3><p>存储一些控制光照的参数信息</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230831183602219.png" alt="芭芭拉的ilmTexture各个通道显示结果"></p><ul><li>R：对高光区域进行分组，区分单独Blinn-Phong高光的区域和Matcap+Blinn-phong高光的区域</li><li>G：存储阴影权重，在这里实际上就是AO</li><li>B：存储Blinn-Phong高光的强度，同时作为高光区域遮罩</li><li>A：对材质进行分组，用于读取不同的Ramp图</li></ul><h3 id="Ramp图">Ramp图</h3><p>存储阴影以及亮暗过渡区域的Color Tint颜色</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230831184511737.png" alt="处理UV后采样Ramp图的结果"></p><h3 id="SDF面部阴影贴图">SDF面部阴影贴图</h3><p>基于光照的角度存储阴影阈值信息</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230831185012391.png" alt="Signed Distance Field阴影贴图"></p><h3 id="MatCap贴图">MatCap贴图</h3><p>存储模拟金属光泽的反射强度信息</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230831185316199.png" alt="两个视角下的MatCap模拟效果"></p><h3 id="Diffuse贴图">Diffuse贴图</h3><p>存储模型漫反射颜色</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230831185548091.png" alt="非常好看的贴图！"></p><h3 id="Outline贴图">Outline贴图</h3><p>自制的存储切线空间下顶点平滑法线信息的贴图</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230831190231498.png" alt="基于RG通道还原切线空间下的平滑法线信息"></p><ul><li>RG通道：存储切线空间下</li><li>B通道：控制描边区域</li><li>A通道：控制Ramp图影响范围（因为游戏中脖子部位和脸的下方是不受到Ramp图影响的，所以利用这个通道标记了一下不绘制ramp的区域。不过代价是在这个Pass中多读了一张贴图，后续再进行优化）</li></ul><h2 id="Diffuse漫反射">Diffuse漫反射</h2><p>Diffuse的表现效果我大概迭代了三个版本，总算是调整到了一个看上去感觉还不错的效果，不过距离游戏里的表现还相去甚远，后续再继续研究分析，做进一步的效果提升。</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230831194443935.png" alt="目前版本的Diffuse明暗表现"></p><p>几次迭代下来，我认为在Shading阶段的Diffuse关键在于，合适且美观地结合Ramp图做出出色的阴影表现，所以以下我重点说一说我对Ramp效果的处理方式。</p><h3 id="Barbara身体的Ramp">Barbara身体的Ramp</h3><p>身体的Ramp分为三层，上下两个部分分别是白天的ramp和夜间的ramp</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230823090627694.png" alt="Babala Body Ramp"></p><p>Body的Lightmap A通道同样是有三层，分为衣领、皮肤和衣服三个部分（以及还有一个胸口的蝴蝶结）</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230823090522534.png" alt="Body的Lightmap.a"></p><p>再来看看游戏中实际的效果表现：</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230831193131252.png" alt="游戏中身体的Ramp表现"></p><p>可以根据颜色的偏移得出以下判断：（其中的数字为lightmap.a中对应区域大致的灰度数值）</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230823101823412.png" alt="Barbara Body Ramp分析"></p><h3 id="Barbara头发的Ramp">Barbara头发的Ramp</h3><p>头发的Ramp分为两层，上下两个部分分别是白天的ramp和夜间的ramp</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230823084908444.png" alt="Babala Hiar Ramp"></p><p>再来看看游戏中实际的效果表现：</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230831193426363.png" alt="游戏中头发和帽子的Ramp表现"></p><p>轻易能看出来，Day/Night的Ramp第一行是头发的（对应Lightmap A通道的数值是0），Ramp第二行是帽子和丝带的（对应Lightmap A通道的数值是1）</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230823093924706.png" alt="Barbara Hair Ramp分析"></p><h3 id="RampUV-Y">RampUV.Y</h3><p>综合一下可以发现，ramp第一行对应lightmap.a&lt;0的区域，ramp第二行对应lightmap.a&gt;0.7的区域，ramp第三行对应lightmap.a&gt;0.52的区域（我开始思考是不是我的贴图错了，因为同时包含两个白天和夜晚的ramp，这也不能直接利用贴图的repeat来处理0跑到最上面的问题啊QAQ）</p><p>这一次，在百思不得其解之后，我选择了使用if来暴力解决（对不起！但是为了效果目前只能先这样了，一定还有我没摸索到的更深层次的原因，我会继续探究的！）</p><p>所以对于Ramp图UV的V轴处理就如下所示：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">//处理RampUV的V轴</span><br><span class="line">if (layers &lt;= 0.01)</span><br><span class="line">&#123;</span><br><span class="line">RampTexUV.y = 0.5 * _RampDailyMode + 9 * RampTexelY;</span><br><span class="line">    &#125;</span><br><span class="line">else if (layers &gt;= 0.6)</span><br><span class="line">&#123;</span><br><span class="line">RampTexUV.y = 0.5 * _RampDailyMode + 7 * RampTexelY;</span><br><span class="line">&#125;</span><br><span class="line">else</span><br><span class="line">&#123;</span><br><span class="line">RampTexUV.y = 0.5 * _RampDailyMode + 5 * RampTexelY;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>因为这次重点针对芭芭拉的ramp图效果进行迭代和优化，所以没有考虑其他角色的通用性，后续整理代码的时候会加上其他角色更多Ramp区域的读取方式。</p><h3 id="RampUV-X-–-Ramp表现的细节分析">RampUV.X – Ramp表现的细节分析</h3><p>迭代着迭代着，再加上一次次地原神启动，我发现了两个问题：为什么Ramp图中过渡的区域是不同的呢？为什么不是在同一个地方进行的明暗交界？</p><p>下面这张图要明显一点，在同样的光照计算中，并且仅仅是透视无法得到同样效果的前提下，可以看出来衣领的<strong>浅阴影-亮面分界</strong>与衣服的<strong>浅阴影-深阴影分界</strong>是接近的</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230823103315687.png" alt="衣领和衣服的区域分析"></p><p>再来看之前的Ramp图，在同样的光照信息采样的前提下，不同部分的ramp颜色变化区域是不同的。</p><p>所以，对于光从0到1的变化过程中，衣领的阴影变化会先到来，然后是衣服的阴影变化，最后是皮肤、蝴蝶结的阴影变化</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230823104501745.png" alt="整体的阴影过度区间"></p><p>不过再来看看阴影结束（也就是完全亮部的分界线），不同部分几乎是相同的</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230823105133188.png" alt="浅阴影和亮面的分界"></p><p>可以分析出来各个Ramp部分截取的都是相同的一段X区间上的Ramp信息，是Ramp颜色过渡部位的不同导致的阴影偏移。不过到现在我还是无法理解为什么要这么处理，等后续再去研究研究色彩和阴影的美术原理，或许能找到一些答案。</p><p>继续分析下去，再看一下游戏中的整体的阴影表现，是接近于对半平分的阴影</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230823105738804.png" alt="整体的阴影表现"></p><p>我总结出的个人理解是：阴影可以分为三层，浅阴影、过渡层、深阴影</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230823163127135.png" alt="Ramp的过度表现分析"></p><p>放大可以发现，浅阴影层和亮部的区分明显，浅阴影层和过渡层之间有一点点的平滑过渡，过渡层和深阴影层之间有平滑的过度关系，有一种颜色向深阴影层扩散的感觉。</p><p>如果直接用halflamber加step的柔化去采样，即使加上Bloom后细节效果也不够好：</p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230823163611073.png" alt="step处理halflambert后直接采样Ramp(Night)" style="zoom:90%;" /><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230823164206533.png" alt="step处理halflambert后直接采样Ramp(Day)"></p><p>肉眼可见有几个问题：</p><ol><li>浅阴影层和过渡层之间的过渡很生硬，并且有“一条一条”的感觉</li><li>过渡层到深阴影层之间，及时加了适度的Bloom也没有那种“扩散过去”的感觉</li><li>总感觉深阴影层的颜色没有那么偏紫</li></ol><p>所以感觉下来，好像深层阴影的颜色像亮部一样，并不是由ramp图控制的，如果这个假设成立，那可以有以下的解决方案：</p><ol><li>深阴影层颜色偏移由另外的参数控制，其与过渡层之间有Lerp的平滑过渡处理，并且Lerp范围比较大</li><li>过渡层只是提取了光照信息的一小个区间，读取了一整张的ramp贴图</li></ol><p>原本浅阴影和亮部之间处理二分的思路是：<code>RampTexUV.x = min(0.99, smoothstep(0.0, 0.5 + _IntermediateToShallowShadowOffset, halfLambert));</code>也就是截取了halfLambert数值<strong>0~0.5+_IntermediateToShallowShadowOffset</strong>之间数值映射到0~1之间，作为ramp的u轴去采样ramp图的横向信息。</p><p>所以直接加入一个参数来对smoothstep的左边界进行控制即可：<code>RampTexUV.x = min(0.99, smoothstep(0.5 - _IntermediateToDarkShadowOffset, 0.5 + _IntermediateToShallowShadowOffset, halfLambert));</code>同时，其中的SmoothStep处理的区间，则被用作平滑处理的遮罩rampSmoothMask，用于后续对DarkShadow区域和Intermediate区域进行融合处理，模拟“扩散过去”的感觉</p><p>最终得到的效果：</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230831200621359.png" alt="最终的Ramp表现效果"></p><p>总的用于获取RampUV信息的函数：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">//计算一个rampSmoothMask用于平滑DarkShadow与IntermediateShadow之间的分界</span><br><span class="line">float2 GetRampUV(float halfLambert, float layers, out float rampSmoothMask)</span><br><span class="line">&#123;</span><br><span class="line">float2 RampTexUV;</span><br><span class="line">float RampTexelY = 0.05;</span><br><span class="line">    //处理RampUV的V轴</span><br><span class="line">    if (layers &lt;= 0.01)</span><br><span class="line">&#123;</span><br><span class="line">    RampTexUV.y = 0.5 * _RampDailyMode + 9 * RampTexelY;</span><br><span class="line">&#125;</span><br><span class="line">else if (layers &gt;= 0.6)</span><br><span class="line">&#123;</span><br><span class="line">    RampTexUV.y = 0.5 * _RampDailyMode + 7 * RampTexelY;</span><br><span class="line">    &#125;</span><br><span class="line">else</span><br><span class="line">&#123;</span><br><span class="line">RampTexUV.y = 0.5 * _RampDailyMode + 5 * RampTexelY;</span><br><span class="line">    &#125;</span><br><span class="line">    //处理RampUV的U轴</span><br><span class="line">    rampSmoothMask = smoothstep(0.5 - _IntermediateToDarkShadowOffset, 0.5 + _IntermediateToShallowShadowOffset, halfLambert);</span><br><span class="line">    RampTexUV.x = min(0.94, rampSmoothMask);</span><br><span class="line">    return RampTexUV;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最终对diffuse color的计算：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">//Diffuse:</span><br><span class="line">float lightMapAOMask = 1 - smoothstep(saturate(lightMapTexCol.g), 0.06, 0.6);</span><br><span class="line">float rampSmoothMask;</span><br><span class="line">float2 RampTexUV = GetRampUV(halfLambert, lightMapTexCol.a, rampSmoothMask);</span><br><span class="line">half4 rampCol = SAMPLE_TEXTURE2D(_RampTex, sampler_RampTex, RampTexUV);</span><br><span class="line">//通过rampSmoothMask平滑DarkShadow与IntermediateShadow之间的分界</span><br><span class="line">half4 shadowColorTint = ifRamp * lerp(_DarkShadowColorTint, rampCol, rampSmoothMask) + (1-ifRamp)*_DarkShadowColorTint*1.5;</span><br><span class="line">//通过控制shadowMask来控制ShallowShadow的宽度</span><br><span class="line">float shadowMask = step(_ShallowShadowWidth + 0.5, halfLambert * lightMapAOMask);</span><br><span class="line">//与漫反射贴图融合得到最终的漫反射颜色</span><br><span class="line">half4 diffuseCol = diffuseTexCol * (shadowMask * _BrightAreaColorTint + (1.0 - shadowMask) * shadowColorTint);</span><br></pre></td></tr></table></figure><h2 id="Specular高光反射">Specular高光反射</h2><p>关于Specular其实没有太多自己的创新，就是Blinn-Phong高光与MatCap模拟金属高光的结合。网上也能找到很多大佬的处理思路，等之后有更新的高光处理思路，再多做记录</p><p>其中使用的是lightmap的R通道对高光类型进行区分</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230823220128347.png" alt="lightmap的r通道"></p><ul><li>&lt;10的部分是没有高光的部分</li><li>10~100的部分只有Blinn-Phong高光</li><li>&gt;100的部分有Blinn-Phong高光和采样MatCap制作的金属镜面高光</li></ul><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230823220308139.png" alt="由MatCap控制的金属镜面高光区域"></p><p>代码如下：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">//Specular:</span><br><span class="line">//通过lightmap的R通道来对高光进行分层</span><br><span class="line">const float specularLayer = lightMapTexCol.r * 255.0;</span><br><span class="line">//lightmap的b通道则是控制blinn-phong高光的强度,并同时作为Specular的遮罩</span><br><span class="line">const float blinnPhongSpecIntensity = lightMapTexCol.b;</span><br><span class="line">//先计算通用的Blinn-Phong高光</span><br><span class="line">float blinnPhongSpec = step(0.75, max(0.0, pow(ndoth, _SpecularExp))) * blinnPhongSpecIntensity;</span><br><span class="line">//specularLayer&gt;100的部分有Blinn-Phong高光和采样MatCap制作的裁边视角光，所以对此部分进行MatCap的计算</span><br><span class="line">float matcapSpec = 0.0;</span><br><span class="line">if (specularLayer &gt; 100)</span><br><span class="line">&#123;</span><br><span class="line">//MatCap需要基于视角空间的法线去读取，并且注意读取发现后需要映射到0-1区间</span><br><span class="line">matcapSpec = saturate(SAMPLE_TEXTURE2D(_MetalTex, sampler_MetalTex,</span><br><span class="line">                  mul((float3x3)UNITY_MATRIX_V, i.normalWS).xy * 0.5 + 0.5).r);</span><br><span class="line">&#125;</span><br><span class="line">//处理头发天使轮的遮罩</span><br><span class="line">matcapSpec *= step(0.25, blinnPhongSpecIntensity);</span><br><span class="line">half4 specularCol = (matcapSpec + blinnPhongSpec) * diffuseTexCol;</span><br></pre></td></tr></table></figure><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230831204549049.png" alt="最终得到的高光区域的效果"></p><h2 id="RimLight边缘光">RimLight边缘光</h2><p>边缘光是在屏幕空间下计算的等距边缘光，制作思路是，开启Depth Texture的Prepass，在Shader中拿到深度图。让模型在齐次裁剪空间下进行一定程度地法线外扩，通过比较外扩前和外扩后模型读取到的深度，如果深度插值达到阈值则判定为边缘光区域，对边缘光区域进行提亮处理。</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230901091331182.png" alt="线性深度的插值DepthDiff"></p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230901091415854.png" alt="RimMask"></p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230901091543373.png" alt="RimLight混合后的效果"></p><p>【posCS的计算】</p><p>值得提一下的是，需要获得posCS（齐次裁剪空间的坐标），但是这个posCS不能是直接由vertx shader中计算好的用于输出给SV_Target进行屏幕映射的vertexOutput.posCS（fragment中的i.posCS），因为在一定的深度范围中才会有线性的变化，并且在一个区间中会有明显的跳变。目前我还不知道原因是什么。</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/iposCSSuddenChanging.gif" alt="使用i.posCS的突变"></p><p>但是如果换作在fragment shader中利用i.posWS计算出来的就可以平滑处理</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/posCSSmoothChanging.gif" alt="在Fragment Shader中计算出posCS后过渡可以很平滑"></p><p>做一下对比探究一下：</p><p>这是vertex shader中计算posCS的方式：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">float4 TransformObjectToHClip(float3 positionOS)</span><br><span class="line">&#123;</span><br><span class="line">    // More efficient than computing M*VP matrix product</span><br><span class="line">    return mul(GetWorldToHClipMatrix(), mul(GetObjectToWorldMatrix(), float4(positionOS, 1.0)));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这是fragment shader中计算posCS的方式：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">float3 TransformObjectToWorld(float3 positionOS)</span><br><span class="line">&#123;</span><br><span class="line">    #if defined(SHADER_STAGE_RAY_TRACING)</span><br><span class="line">    return mul(ObjectToWorld3x4(), float4(positionOS, 1.0)).xyz;</span><br><span class="line">    #else</span><br><span class="line">    return mul(GetObjectToWorldMatrix(), float4(positionOS, 1.0)).xyz;</span><br><span class="line">    #endif</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// Tranforms position from world space to homogenous space</span><br><span class="line">float4 TransformWorldToHClip(float3 positionWS)</span><br><span class="line">&#123;</span><br><span class="line">    return mul(GetWorldToHClipMatrix(), float4(positionWS, 1.0));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>实际上两个方法的计算过程是一致的，只不过在vertex中计算出来的posCS直接插值传递给fragment，而在fragment中利用插值后的WS计算CS是逐fragment去计算的。但是矩阵变换是线性变换的，这些也是在线性空间完成的，按理说插值应该不会影响效果才对。那么感觉就是插值寄存器的问题</p><p>【深度的线性转化】</p><p>我们通过深度图拿到的深度信息实际上是经过透视变换之后的深度，不是线性空间的（这样可以用更高精度存近处的深度，更低精度存远处的深度），但是在进行对比时，我们需要的是线性空间的深度值来作差，所以我们需要对深度进行转化，Unity提供了Linear01Depth和LinearEyeDepth两个函数来进行转换。</p><p>Linear01Depth和LinearEyeDepth最终得到的深度值的区别在于是否压缩到0-1空间中，压缩方法也就是除以far</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// Z buffer to linear 0..1 depth (0 at camera position, 1 at far plane).</span><br><span class="line">// Does NOT work with orthographic projections.</span><br><span class="line">// Does NOT correctly handle oblique view frustums.</span><br><span class="line">// zBufferParam = &#123; (f-n)/n, 1, (f-n)/n*f, 1/f &#125;</span><br><span class="line">float Linear01Depth(float depth, float4 zBufferParam)</span><br><span class="line">&#123;</span><br><span class="line">    return 1.0 / (zBufferParam.x * depth + zBufferParam.y);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// Z buffer to linear depth.</span><br><span class="line">// Does NOT correctly handle oblique view frustums.</span><br><span class="line">// Does NOT work with orthographic projection.</span><br><span class="line">// zBufferParam = &#123; (f-n)/n, 1, (f-n)/n*f, 1/f &#125;</span><br><span class="line">float LinearEyeDepth(float depth, float4 zBufferParam)</span><br><span class="line">&#123;</span><br><span class="line">    return 1.0 / (zBufferParam.z * depth + zBufferParam.w);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>而_ProjectionParams.z的值是far</p><p>所以<code>Linear01Depth(offsetDepth, _ZBufferParams)*_ProjectionParams.z</code>和<code>LinearEyeDepth(0ffsetDepth, _ZBufferParams)</code>是等效的</p><p>【最后的代码】</p><p>计算边缘光的代码如下所示：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">//Rim Light</span><br><span class="line">float4 posCS = TransformWorldToHClip(i.posWS);</span><br><span class="line">//获取原本的深度：</span><br><span class="line">float4 orgPosCS = float4(posCS.xy, 0.0, posCS.w);</span><br><span class="line">//很有意思的一点是，这里的orgPosCS需要是由世界空间posWS经过shader内部自己计算出来的posCS，如果用</span><br><span class="line">float4 orgScreenPos = ComputeScreenPos(orgPosCS); //计算出齐次空间下的ScreenPos，注意此处是没有进行透视除法的</span><br><span class="line">float2 orgVPCoord = orgScreenPos.xy / orgScreenPos.w; //进行透视除法，获取屏幕空间坐标</span><br><span class="line">float orgDepth = SampleSceneDepth(orgVPCoord);</span><br><span class="line">//读取没有偏移时的深度信息，使用SampleSceneDepth函数需要引入DeclareDepthTexture.hlsl</span><br><span class="line">float orgLinearDepth = LinearEyeDepth(orgDepth, _ZBufferParams); //将深度信息转换到视角空间下</span><br><span class="line">//获取偏移之后的深度：</span><br><span class="line">float3 normalCS = TransformWorldToHClipDir(i.normalWS, true); //计算出齐次裁剪空间下的法线方向</span><br><span class="line">float4 rimOffsetPosCS = float4(normalCS.xy * _RimLightWidth + posCS.xy, 0.0, posCS.w); //计算出齐次裁剪空间下的偏移位置</span><br><span class="line">float4 rimOffsetScreenPos = ComputeScreenPos(rimOffsetPosCS);</span><br><span class="line">float2 offsetVPCoord = rimOffsetScreenPos.xy / rimOffsetScreenPos.w;</span><br><span class="line">float offsetDepth = SampleSceneDepth(offsetVPCoord); //得到偏移后的深度信息</span><br><span class="line">float offsetLinearDepth = LinearEyeDepth(offsetDepth, _ZBufferParams); //将深度信息转换到视角空间下</span><br><span class="line">//进行比较：</span><br><span class="line">float depthDiff = offsetLinearDepth - orgLinearDepth; //计算出偏移后和偏移前的深度差（Unity中，离摄像机越近，深度值越接近1）</span><br><span class="line">float rimFactor = 1 - smoothstep(60.0, 100.0, orgLinearDepth); //rim factor由距离控制是否需要产生边缘光，并平滑使边缘光消失</span><br><span class="line">float rimEdgeMask = smoothstep(10.0, 20.0, depthDiff) * rimFactor; //得到边缘光遮罩</span><br><span class="line"></span><br><span class="line">return diffuseCol * rimEdgeMask * 1.5 + diffuseCol * (1 - rimEdgeMask);</span><br></pre></td></tr></table></figure><h2 id="SDF制作面部阴影">SDF制作面部阴影</h2><p>【原理浅析】</p><p>为了使面部阴影效果更好（一是二分，二是鼻翼侧边的高光），开发者们根据光照的方向，制作了几张面部的lightmap，作为特定角度下面部的阴影表现。然而，直接把这几张贴图合并在一起使用的话，面部的阴影只能是跳变的效果，也就是阴影表现只有在特定角度才会切换，在某个角度区间内阴影表现是固定的。所以引入了SDF，有向距离场的技术：通过“距离场”，把每一张lightmap从单纯的0或1的区分，处理为了距离阴影分界线从0到1之间的数值，可以理解为特定角度下不同区域的阴影权重。通过“有向”来界定了该区域是在阴影内还是阴影外。最终合并后就得到如下的结果</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/Avatar_Girl_Tex_FaceLightmap.png" alt="原神中SDF面部阴影图"></p><p>【实际使用】</p><p>模型的头骨是经过旋转的，直接使用lightDir计算会有问题，不过我因为直接是用的mmd的模型，好像因此差距不是很大。所以我的处理是在用原始的light方向判断完左右信息之后，再利用旋转矩阵对光源的方向进行一定程度的旋转，通过旋转光源来进一步优化一下阴影的表现，也便于在视频中根据内容实时调整阴影。</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">//获取世界空间下的光照信息，并在世界空间下计算光源与角色forward向量之间的夹角信息，并判断光源此时是在角色的左侧还是右侧</span><br><span class="line">half2 lightDir = mainLight.direction.xz;</span><br><span class="line">float rdotl = dot(rightDirWS.xz, lightDir);</span><br><span class="line">//默认的贴图光是从角色的左侧照射向角色的，所以当点乘小于零的时候用默认uv，当点乘大于0的时候左右翻转uv</span><br><span class="line">//温馨提示：左右翻转uv本来应该是1-uv.x，但是如果把texture的Wrap Mode改为Repeat的话就可以用-uv.x来表示了</span><br><span class="line">float2 shadowmapUV = rdotl &lt; 0 ? i.uv : float2(-i.uv.x, i.uv.y);</span><br><span class="line">float faceSDF = SAMPLE_TEXTURE2D(_FaceSDFTex, sampler_FaceSDFTex, shadowmapUV).x;</span><br><span class="line">//利用SDF计算光照信息时需要对光方向进行偏移</span><br><span class="line">float sinx = sin(_FaceShadowOffset);</span><br><span class="line">float cosx = cos(_FaceShadowOffset);</span><br><span class="line">float2x2 rotationOffset1 = float2x2(cosx, sinx, -sinx, cosx); //顺时针偏移</span><br><span class="line">float2x2 rotationOffset2 = float2x2(cosx, -sinx, sinx, cosx); //逆时针偏移</span><br><span class="line">lightDir = lerp(mul(rotationOffset1, lightDir), mul(rotationOffset2, lightDir), step(0, rdotl));</span><br><span class="line">            </span><br><span class="line">float fdotl = dot(forwardDirWS.xz, lightDir);</span><br><span class="line">float lightMask = step(0.5 * fdotl, faceSDF); //需要去截取出每一个角度的SDF信息，</span><br><span class="line">//SDF信息的变化（即不同距离场贴图的区分）是按照光源与角色左右方向上的轴的夹角区分的</span><br><span class="line">//乘0.5是因为身体和头发使用的是半兰伯特光照模型，为了保持阴影分界线尽可能统一而在fdotl后乘0.5</span><br><span class="line">//原理同faceSDF &gt; 0.5 * fdotl，是同一个算法                </span><br><span class="line">//读取通用贴图</span><br><span class="line">half4 diffuseTexCol = SAMPLE_TEXTURE2D(_DiffuseTex, sampler_DiffuseTex, i.uv);</span><br><span class="line">half4 diffuseCol = lerp(diffuseTexCol * _ShadowAreaColorTint, diffuseTexCol * _BrightAreaColorTint,</span><br><span class="line">                        lightMask);</span><br></pre></td></tr></table></figure><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230901094210974.png" alt="使用_FaceShadowOffset偏移阴影"></p><h2 id="BackFace描边">BackFace描边</h2><h3 id="平滑法线处理">平滑法线处理</h3><p>因为直接使用模型的法线进行外扩的话，对于硬边上的顶点，一个位置实际上有多个顶点，每个顶点有其各自的法线朝向。当进行外扩时，就会出现描边断裂的效果。</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230901100435265.png" alt="左使用平滑法线，右使用模型法线"></p><p>硬边一个顶点有多条法线，导入引擎后实际上就是硬边把一个顶点分成了多个顶点，数量则等于硬边所区分的面的数量（这也是为什么引擎中看模型数据顶点数会大于DCC建模软件中所见的顶点数）：</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230729104633539.png" alt="硬边一个顶点有多条法线"></p><p>而软边只有一条法线：</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230729105031138.png" alt="软边只有一条法线"></p><p>关于工具的详细内容我放在另一个文章中写</p><p>最后计算出平滑法线后我将这些信息存到了一张贴图里面，也就是一开始说得那几张outline贴图</p><h3 id="描边颜色和区域控制">描边颜色和区域控制</h3><p>描边的颜色我用顶点色来进行控制，描边的区域则是在平滑法线贴图的A通道进行控制</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230901101105204.png" alt="Blender中绘制顶点色"></p><h3 id="最终效果">最终效果</h3><p>Unity中的描边内容如下：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">OutlineVaryings NPROutlineVertex(OutlineAttributes vertexInput)</span><br><span class="line">&#123;</span><br><span class="line">    OutlineVaryings vertexOutput;</span><br><span class="line">    float4 pos = TransformObjectToHClip(vertexInput.posOS.xyz);</span><br><span class="line"></span><br><span class="line">    float2 uv = TRANSFORM_TEX(vertexInput.uv, _SmoothNormalTex).xy;</span><br><span class="line">    float3 smoothNormalTexCOl = SAMPLE_TEXTURE2D_LOD(_SmoothNormalTex, sampler_SmoothNormalTex, uv, 0).xyz;</span><br><span class="line">    //构建TBN矩阵，并将平滑法线信息转换到世界空间下</span><br><span class="line">    float3 normalWS = TransformObjectToWorldNormal(vertexInput.nomralOS);</span><br><span class="line">    float3 tangentWS = TransformObjectToWorldDir(vertexInput.tangentOS.xyz);</span><br><span class="line">    float3 bitangentWS = normalize(vertexInput.tangentOS.w * abs(cross(normalWS, tangentWS)));</span><br><span class="line">    float3x3 TBN = float3x3(tangentWS, bitangentWS, normalWS);</span><br><span class="line">    float3 smoothNormalTS = float3(smoothNormalTexCOl.r, smoothNormalTexCOl.g,</span><br><span class="line">                                   sqrt(1 - (dot(smoothNormalTexCOl.r, smoothNormalTexCOl.g))));</span><br><span class="line">    float3 smoothNormalWS = mul(smoothNormalTS, TBN);</span><br><span class="line">    float3 smoothNormalCS = TransformWorldToHClipDir(smoothNormalWS, true);</span><br><span class="line"></span><br><span class="line">    //VertexColor的Blue通道用于控制描边区域</span><br><span class="line">    half toDrawOutline = saturate(1 - step(0.1, smoothNormalTexCOl.b));</span><br><span class="line"></span><br><span class="line">    //计算出屏幕的高/宽的值，用于平衡水平描边和竖直描边因屏幕映射而导致的拉伸</span><br><span class="line">    float4 screenParam = GetScaledScreenParams();</span><br><span class="line">    float HdW = screenParam.y / screenParam.x;</span><br><span class="line">    float2 offset = _OutlineWidth * clamp(0.1, 0.5, pos.w) * 0.01 * float2(</span><br><span class="line">        smoothNormalCS.x * HdW, smoothNormalCS.y);</span><br><span class="line">    pos.xy += offset * toDrawOutline;</span><br><span class="line">    vertexOutput.posCS = pos;</span><br><span class="line"></span><br><span class="line">    vertexOutput.outlineColor = vertexInput.vertexCol;</span><br><span class="line"></span><br><span class="line">    return vertexOutput;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">half4 NPROutlineFragment(OutlineVaryings i) : SV_Target</span><br><span class="line">&#123;</span><br><span class="line">    return i.outlineColor;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230901100959818.png" alt="最终的描边效果"></p><h2 id="后处理-DOF-Bloom">后处理(DOF + Bloom)</h2><p>这次后处理没有太考虑性能方面的问题，过程很暴力，后续进行优化，核心就是遮罩、模糊和混合。主要的难点在于URP的后处理比Built-in管线写起来要复杂一些。具体的制作内容我同样会再写其他文章来分享，这里只是做一个总述。</p><h3 id="DOF景深效果">DOF景深效果</h3><p>我对DOF的处理方法是，先利用深度信息去计算出模糊区域的遮罩，通过遮罩提取出模糊区域，进行高斯模糊处理。然后再基于遮罩，对原图和计算后的模糊图像进行混合（如果不利用遮罩的话，背景区域的模糊会扩散进清晰区域，得到的效果就没那么好）</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230901102726588.png" alt="DOF的过程"></p><p>然后混合后就是如下结果：</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230901101800276.png" alt="DOF最终的效果"></p><h3 id="Bloom">Bloom</h3><p>Bloom参考了COD在SIGGRAPH 2014 <a href="http://advances.realtimerendering.com/s2014/index.html">Advances in Real-Time Rendering in Games</a>课程中分享的Bloom处理思路。主要借鉴了升采样和降采样的步骤，因为一开始做Bloom都是直接模糊然后混合，得到的效果不太好，所以用升降采样的思路进行处理。</p><p>注：Bloom的输入源是DOF的输出，所以以下是在DOF景深计算后的基础上得到的内容</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230901103757021.png" alt="Bloom计算过程"></p><p>然后进行混合：</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230901103814492.png" alt="最终Bloom与DOF景深混合的结果"></p><h2 id="Timeline控制自定义后处理中的参数">Timeline控制自定义后处理中的参数</h2><p>十分感谢这位大佬的分享：<a href="https://www.bilibili.com/read/cv4828303/">https://www.bilibili.com/read/cv4828303/</a></p><p>基本就是照着这位大佬的流程制作的，后续我深入研究一下API再整理一遍思路</p><h2 id="借物表">借物表</h2><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/BarbaraMMDDisplay1.png" alt=""></p>]]></content>
    
    
    <summary type="html">在URP管线下探究原神的卡通渲染实现，并以一个MMD作为成果展示</summary>
    
    
    
    <category term="Portfolio" scheme="https://himoqiuhan.github.io/categories/Portfolio/"/>
    
    
    <category term="Unity" scheme="https://himoqiuhan.github.io/tags/Unity/"/>
    
    <category term="URP管线" scheme="https://himoqiuhan.github.io/tags/URP%E7%AE%A1%E7%BA%BF/"/>
    
    <category term="卡通渲染" scheme="https://himoqiuhan.github.io/tags/%E5%8D%A1%E9%80%9A%E6%B8%B2%E6%9F%93/"/>
    
    <category term="技术美术" scheme="https://himoqiuhan.github.io/tags/%E6%8A%80%E6%9C%AF%E7%BE%8E%E6%9C%AF/"/>
    
    <category term="SRP自定义后处理" scheme="https://himoqiuhan.github.io/tags/SRP%E8%87%AA%E5%AE%9A%E4%B9%89%E5%90%8E%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>《原子习惯》读后感</title>
    <link href="https://himoqiuhan.github.io/2023/08/20/Reading-AtomicHabits/"/>
    <id>https://himoqiuhan.github.io/2023/08/20/Reading-AtomicHabits/</id>
    <published>2023-08-20T11:34:56.000Z</published>
    <updated>2024-07-27T15:13:59.658Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>概述：本书简单地从两个维度对原子习惯进行阐述，第一层于“法”的层面向读者介绍“原子习惯”，从为什么开始，谈到真正的原子习惯是什么，再到最后简单地提了一下如何做；第二层则在“器”的维度，向读者介绍了建立习惯的四步法，并且也提供了一些在建立习惯之后如何对习惯进行维护的建议。</p></blockquote><h2 id="感想总述">感想总述</h2><p>先简单谈一下我读完本书后对“习惯”的理解。“习惯”是一个人做成事情的基石，书中有一句话这样说道：“<em>重要的不是你现在多成功或多不成功，而是你的习惯是否把你放在通往成功的道路上。比起当前拥有的成果，你应该更关注现在所处的轨道。</em>“ 养成一个好的习惯，或者培养出一个习惯的过程，能够让人更好地把握住当下，确保自己朝着对自己来说正确的方向努力。这里的重点有两个：一是自己想要走向的方向，二是持续的投入和努力。</p><p>由”方向“这个概念，作者阐述了一个观点——<em>”改变习惯最有效的方法是改变身份认同 “</em>。养成习惯的过程，就是在强化自己想要成为的那个身份的身份认同，让自己一步一步去接近自己想要成为的形象，这也正是很好地给自己的前向指明了方向。如同书中提到的，<em>”建立习惯的过程，其实就是成为自己的过程“</em>，每一次为形成习惯做出努力而带来的小成功，就是在一点一滴向自己证明自己就是那样的人。</p><p>”持续努力“这个概念，意味着习惯的养成重点在于”养成“，并非目标的完成。很多时候，我们会掉入”畅想未来“的陷阱，在给自己制定目标时总能感觉到精力充沛，相信着自己能够完成目标并且依此获得成功，但事实上，很多时候我们根本做不到我们所设想的那样。习惯的养成，可以从根本上调和由目标制定带来的焦虑与盲目自信。养成习惯在于“做”，只有持续做下去，才会有成功的可能，如果只是沉浸于对未来无边无际的遐想，那也只是自欺欺人罢了。</p><blockquote><p>“当一切努力看似无用，我会去看石匠敲打石头。可能敲了一百下，石头上连一条裂缝都没有，但就在第一百零一下，石头断裂为两半。然后我了解到，把石头劈成两半的不是最后那一下，而是先前的每一次敲击。“——雅各·里斯</p></blockquote><p>由”习惯“这个概念落实到一个更加具象的层面上，我认为就转化为了每个人独属于自己的”系统“，于我而言就是我在努力构建的属于自己的”思维闭环“。书中说到过，<em>”决定你成功或失败的，不是你的目标，而是你的系统。“</em> 我认为，制定一个系统的重要性远高于完成一个目标，想要不断进步的人需要的是一个能够支撑自己不停朝着目标实现、身份改变、行为塑造去努力的系统。一个个习惯的堆叠，属于自我的系统就能由小到大地发展壮大，驱动一个人去不断努力。书中大量篇幅讲述了如何去建立习惯，但是我们也该主要到书中只用几百字提及到的这个系统的自我迭代、更新和修复的必要性。</p><blockquote><p>“你生命中最大的挑战是什么？”在某个论坛上，埃隆·马斯克面对这个问题，足足想了30秒，给出了一个非常精彩的回答：<strong>确保你有一个可纠错的反馈闭环</strong>。</p></blockquote><p>当一个习惯被养成，当一个个小小的习惯累积起来，当一个系统准确运行了很长时间，<strong>复利效应</strong>便会对一个人产生巨大冲击。<em>”复利过程的标志：强大的成果总是姗姗来迟。“</em> 这一点便是“原子习惯”中“原子”二字的由来，从时间和空间上进行复利计算，重点在于养成一个个小小的习惯，并且坚持下去，其所带来的影响虽前期不够显著，但随着时间的推移，成效一定会越来越大。</p><blockquote><p>“习惯不过是环境中反复出现的问题的可靠解决之道。”——杰森·瑞哈</p></blockquote><p><em>“当你建立了习惯，搞定生活中的基本事务，你的心智就能自由地聚焦于新的挑战，掌握下一组问题。现在把习惯建立好，你就得以在未来做更多想做的事。”</em> 如同这句话所说的一样，习惯带来的另一个好处就是，他能自动化处理很多事情，让我们自己能把精力留在那些更加重要的方面上。</p><p>本书中还传达了一个很重要的观点：<strong>选对自己的赛道</strong>。我相信每个人各有千秋，每个人一定会有能够被称为天才的领域，只不过大多数的人，包括我在内，可能都暂时没有真正找到独属于自己的赛道，或者还没有意识到自己所走的路是属于自己的赛道。越有成就的人，他努力的方向越是接近那条属于他的赛道，这一切似乎从基于中就已经形成了。</p><p>基因无法被轻易改变，这有利有弊，一方面这代表它们会在有利的条件下提供强大的优势，另一方面基于也会在不利的环境中带来严重的劣势。所以，<strong>基因所决定的不是命运，而是我们在哪个领域会有机会</strong>。</p><p>这一个思维模式转移到习惯上，我们就可以理解为性格影响一个人习惯的养成。书中提到，<em>“不需要养成每个人叫你养成的习惯，选择最适合你的习惯，而不是最受欢迎的习惯 “</em>，只有能让我们自己个人获得愉悦的习惯，才更容易被养成。这时候或许会有另外一个想法说：”如果我就是为了挑战自己而建立习惯呢？“那我认为给自己一步一步加挑战并且实现它是一定能给你带来愉悦的，并且你乐于依靠这样的愉悦和挑战去一步一步提升自己。</p><blockquote><p>要判别自己是不是做某件事的料，不是看自己爱不爱，而是看你能不能比多数人更不费力地处理那项任务带来的痛苦。<strong>大家都在抱怨，而你却乐在其中的工作，就是你生来适合的工作</strong>。</p></blockquote><p>同样的，正确理解基因也能让我们更好地看清自己，把握当下并且去做到最好。我们一定要清楚，基因不会消除努力的必要性，它只是让努力的方向更加清楚。并且，与其与别人在基因层面上去做比较，抱着“他比我更有天赋“的思想去自暴自弃，不如把焦点放在你<strong>是否发挥尽了自身潜能</strong>。</p><blockquote><p>人们太执著于自己“有极限”这件事，以致很少真正尽力去接近那些极限。<strong>在付出跟你佩服的那些人同等的努力之前，不要把他们的成功归因于幸运</strong>。</p></blockquote><p>提到了”比较“，接着就能引申出习惯系统的自我反馈迭代方法：与自己做比较。<strong>与今天的别人做比较远远不如把今天的自己和昨天的自己进行比较</strong>，要经常从一个旁观者的角度来看待自己，分析甚至批评自己，努力让自己的习惯系统比昨天更好。</p><p>即使到习惯养成之后，也要常常把自己从身份认同中剥离出来，不要用一个陈述句去定义自己是一个什么样的人，而是通过一系列的形容词去描述自己是一个什么样的人，这样能够更好地寻找出自己的上身空间。</p><blockquote><p>改善不只关乎学习习惯，还关乎微调习惯。反省与复查确保你把时间花在对的事情上，并在需要的时候调整方向。解决方法之一，就是<strong>不要让身分的任何单一面向决定你是谁</strong>，一个身分抓得太紧，你就会变得脆弱易碎。失去那一样东西，你就失去自己。要减轻身分丧失造成的影响，关键就是重新定义自己。如此一来，才能在特定角色改变时，仍保住你身分认同的重要面向。</p></blockquote><h2 id="具体方法">具体方法</h2><p>接下来就该提到习惯的建立方法了，书中借助行为发起的四个过程，将习惯的养成分成了四个步骤：一，让提示显而易见；二，让习惯有吸引力；三，让行为轻而易举；四，让奖赏令人满足。</p><p>我读完之后做出的提炼是，把握”<strong>开始</strong>“两个字。想要养成一个好的习惯，最为重要的就是<strong>给自己一个开始的契机，并且在执行的过程中让下一次的开始轻松一点</strong>。同样，如果是想要破除一个坏习惯，也是通过一系列的方法，让坏习惯的开始变难，让个人在坏习惯进行过程中获得的体验变差进而不想开启下一次。全书中给了很多距离落实的方法，我就挑几个我个人认为最有用，并且正在使用的方法来分享，剩余的可以到书中学习。</p><h3 id="1-营造环境">1 营造环境</h3><p>环境是一双隐形的手，形塑着人的行为。环境的影响力十分之大，想要让习惯成为生活的一大部分，就让提示成为环境的一大部分，我们需要创造明显的可以把注意力引至我们想养成的习惯的视觉提示。同时，需要给每个习惯都安一个家，一个凡事<strong>各安其位、各司其职的稳定环境</strong>，就是可以轻松养成习惯的环境。</p><p>所谓的“自律者”只是擅长<strong>建构生活</strong>，好让自己不需要展现超凡的意志力与自我控制力。换言之，他们不常让自己处于充满诱惑的情境。所以，最有自制力的，通常是最少用到自制力的人，我们需要通过环境的构建来减少对自制力的消耗，让一切习惯的执行变得自然而然。</p><p>今年暑假的时候我并没有回家，而是选择留在宿舍和学校中学习，一定程度上利用这个方法在没有舍友的一段时间坚持住了自己的学习习惯。通常早上起床我就会去学校的教室里自习，晚上跑完步十点回到宿舍洗完澡基本就是打会儿游戏，然后直接睡觉。不过这一点也确实有一点弊端，就是我在一段时间把宿舍完全当作了一个休息的地方，突发大雨那几天，我在宿舍的学习效率就很低，基本学习一两个小时就开始打游戏了QAQ。</p><h3 id="2-执行意向和习惯堆叠">2 执行意向和习惯堆叠</h3><p>这两点在书中是两个方面，但是于我个人实际使用而言，他们是相互捆绑的。</p><p>执行意向很简单地来说，就是**“当Ｘ情境发生时，我就会执行Ｙ回应。”<strong>这样一个公式。提前做好大概的计划是很重要的，书中说过：“对</strong>何时何地执行一项新习惯<strong>做出</strong>确切计划**的人，比较可能真的去执行。”我体验下来确实是这样的。简单拿我自己举个例子，同样是暑假期间，我大多数时间会提前规划当天的时间分配情况，偶尔会有几天因为太忙或者太懒没有去做时间的分配，一天结束去做总结的时候，明显发现自己不规划的那一天过的会很没有头绪，并且会有点脱离自己每周制定的目标。</p><p>所以，执行意向在我个人身上的用法就是，每一天大概去规划自己的时间分配情况。我不会细致到每一分钟做什么，也不会可以去强求自己完完全全按照计划来走，但是<strong>一个大概的方向确定在那里，无论我如何迈出下一步，那个目标多多少少都会吸引着我向那个方向前进</strong>。</p><p>再就是习惯堆叠，它和执行意向同根同源，只不过是把前提条件换成了某项习惯：”<strong>当某项习惯被执行后，我会去执行某项习惯</strong>“。我体验最深的就是我跑步习惯的维持。在舍友回来前，我基本每天都会九点左右去到操场跑步，因为我把”晚上在学校学习“与跑步堆叠在了一起。反观现在，舍友回来后我去跑步的频率降低到了一周一两次，目前正在想办法调整这个频率。舍友回到宿舍后，我转为“宿舍工作制”，也同样用了习惯堆叠的方法，如今至少说我上午的行为模式是很固定了，基本不需要我花太多时间去考虑：起床后，我会尽快去洗漱，洗漱完就泡一杯燕麦，然后冥想十多分钟，吃完早饭，立刻开始学习。</p><p>所以说，这两个方法是真的让人欲罢不能！</p><h3 id="3-习惯追踪器和建立一套反省与复查的系统">3 习惯追踪器和建立一套反省与复查的系统</h3><p>这两个方法一看名字就能知道是什么意思，记录自己的习惯，并且时刻反省自己。</p><p>“<strong>吾日三省吾身</strong>”，当一个习惯建成的时候，我们千万要注意不能踏入自满的陷阱，要找到一个可以对自我表现长久保持觉察的方法。如果少了反省，我们就会找借口，试图合理化，并且欺骗自己。所以不仅仅要注意养成习惯，还要去注重微调习惯，通过一定的方法确保我们把时间花在对的事情上，并在需要的时候调整方向。</p><p>于我个人而言，我会每天做自己的记录，类似一个日记的形式。每天的结束或者第二天的早晨。我会努力站在一个旁观者的视角去分析自己一天的行为，记录下自己的思考以及想到的能够优化自己系统的建议。在每周日的时候，我会花一点时间去回顾这一周发生的事情，同步一下自己的进展并且依据这个阶段的大目标去给下一周做一个任务规划，同时收集之前记录下的建议，思考决定是否加入到我自我的”思维闭环“中。</p><h2 id="后语">后语</h2><p>总的来说，这本书很值得读，尤其对于这个阶段的我来说，给了我很多”思维闭环”构建方面的建议，并且经过实践之后我也找到了几个会被我纳入自我精神世界的方法。</p><p>所以，不要停下，一直向前，哪怕慢一点也要一直努力！</p>]]></content>
    
    
    <summary type="html">小习惯带来巨大改变</summary>
    
    
    
    <category term="Impression" scheme="https://himoqiuhan.github.io/categories/Impression/"/>
    
    
    <category term="感想" scheme="https://himoqiuhan.github.io/tags/%E6%84%9F%E6%83%B3/"/>
    
    <category term="阅读" scheme="https://himoqiuhan.github.io/tags/%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>技术美术百人计划学习笔记（图形5.1.3 基于物理的灯光）</title>
    <link href="https://himoqiuhan.github.io/2023/08/12/Notes-TA100-T5130/"/>
    <id>https://himoqiuhan.github.io/2023/08/12/Notes-TA100-T5130/</id>
    <published>2023-08-12T06:01:11.000Z</published>
    <updated>2024-07-27T15:13:59.655Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T5100.png" alt=""></p><h2 id="辐射度量学基本理论">辐射度量学基本理论</h2><p>定义：辐射度量学是一门以整个电磁波段的电磁辐射能测量为研究的科学。而计算机图形学中涉及的辐射度学，则集中对于整个电磁波普中光学谱段中的<strong>可见光谱段的辐射能的计算</strong>。</p><ul><li>光学谱段范围：指从波长0.1cm的红外线到波长0.1mm的x射线这一距离</li><li>可见光谱段：波长0.38mm到0.76mm范围内，能对人眼产生目视刺激，从而形成亮光感和色感的谱段</li></ul><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230910155110107.png" alt=""></p><h3 id="立体角-Solid-Angle">立体角 Solid Angle</h3><p>概念：单位球体上的一块区域对应的球面部分的面积</p><p>假设我们给定一个正球体，半径为R，再给定一个正圆锥体，正圆锥体的顶点与球心重合，到圆锥底面圆任意一点的连线，即正圆锥体的斜高（斜边），它的长度也为R。由正圆锥体的底面圆S所截取的那一部分球面的面积A和球体半径R的平方的比值称为立体角（球面度）</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230910155323759.png" alt=""></p><p>单位立体角：一个立体弧度为半径1 米的球面上1 平方米面积所张的立体角。</p><p>立体角的微分形式定义如下(以Ω表示立体角)：</p><p><img src="https://cdn.nlark.com/yuque/__latex/b996f967bd70eb23923994776341c205.svg" alt="img"></p><p>如果在球面坐标系下对立体角进行定义，面积微元dA的公式为：</p><p><img src="https://cdn.nlark.com/yuque/__latex/510ea9781c331fee9ff5195801fba7a1.svg" alt="img"></p><p>由上式可得，整个球面的立体角可以写为关于θ和φ的二重积分公式，其中φ表示维度，θ表示经度，所以整个球面的立体角为4Π；对于一个正方体的面，从该正方体的中心点测量的立体角为(2/3)Π；半球的立体角为2Π</p><p><img src="https://cdn.nlark.com/yuque/__latex/f75629cbe76c3a177242462213c2af25.svg" alt="img"></p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230910155346801.png" alt=""></p><h3 id="辐射通量-Radiation-Flux">辐射通量 Radiation Flux</h3><p>辐射通量定义为以辐射的形式发射，传输或者接受的功率，即单位时间内的辐射能，单位为W，记为ɸ。是为了研究<strong>单位时间</strong>内，<strong>通过某表面的各个光子</strong>所携带的<strong>能量之和是多少</strong>，物理学中引入了辐射通量（Radiation Flux）的概念</p><p><strong>不同的波长</strong>的光引入人对<strong>不同颜色的感知</strong>，<strong>不同的辐射通量</strong>则引起人对光的<strong>不同亮度的感知</strong></p><p>光子虽然在物体表面不停流过，但总体上讲，光子的分布保持一个常数——恒定光源，且开灯瞬间光速很快</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T5130-1.png" alt=""></p><p>如果知道了任意一点p在任意一个方向w上的辐射通量的值Φ(p,w)，就能得到计算机图形学中的光照问题的完整解决方案。因为知道辐射通量，就能计算出单位时间内的能量，从而计算出光波，根据光波重构颜色。不过因为以上公式计算量极大，所以图形学中更多的是在用近似公式</p><h3 id="辐射强度-Radiation-Intensity">辐射强度 Radiation Intensity</h3><p>定义：在给定的传输方向上，<strong>单位立体角内</strong>光源发出的<strong>辐射通量</strong></p><p>记辐射通量为Φ，立体角为I，辐射亮度为L，则以微分形式定义光的辐射强度公式为：</p><p><img src="https://cdn.nlark.com/yuque/__latex/001b208f605ab71edc8e50f3a4f59048.svg" alt="img"></p><p>对一个面A的区域求积分得到：</p><p><img src="https://cdn.nlark.com/yuque/__latex/d67eb8510f7796bfd50ad890a5100564.svg" alt="img"></p><p>I称为表面面积微元dA在方向(α, β)上的辐射强度，与距离无关，但是与发射面dA的面积有关</p><h3 id="辐射亮度-Radiance">辐射亮度 Radiance</h3><p>定义：辐射表面在其单位投影面积的单位立体角内发出的辐射通量</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230910155522508.png" alt=""></p><p>辐射亮度L的微分定义：</p><p><img src="https://cdn.nlark.com/yuque/__latex/4b875145b544c208d8a5c3f1c71d24cf.svg" alt="img"></p><p>辐射亮度描述的是<strong>光源的表面面积微元在垂直传输方向上的辐射强度特性</strong>。好比单去描述一个白炽灯的某一块区域的发射特性是没有实际意义的，应该将它视作一个点光源整体，来描述在某个给定的观察方向上的辐射强度</p><h3 id="辐射照度-Irradiance">辐射照度 Irradiance</h3><p>辐射照度被用来测量光进入一个单位面积的强度，也可以表示光离开一个表面的强度，我们称为入射度与出射度</p><ul><li>入射度定义：单位面积被照射的辐射通量</li><li>出射度定义：离开光源表面的单位面积的辐射通量</li></ul><p>对于点光源，给定辐射通量为Φ，则在一个半径为R的球面某一面积微元上的入射度遵循平方反比定律（距离越近，亮度越大，为距离平方的倒数）</p><h2 id="光度学基本理论">光度学基本理论</h2><p>人眼的灵敏度用CIE光度拉姆达曲线表示，代表我们眼睛对某些光波长的接收效率。人眼的灵敏度在555nm初达到峰值，在我们看来是绿色。这个波长上，灵敏度函数值为1个单位，意味着100%的效率</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230910155605529.png" alt=""></p><p>和辐射度量学相比，光度量学只限于可见光的范围内，并且要以人眼的视觉特性为基础。光通量和辐射通量的转换公式为：</p><p><img src="https://cdn.nlark.com/yuque/__latex/040e5c1727677353c44785496786d30c.svg" alt="img"></p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T5130-2.png" alt="链接辐射度量学和光度学的桥梁，是视见函数"></p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230910155705346.png" alt=""></p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230910155722003.png" alt="辐射度量和光度量的名称、符号和定义"></p><h2 id="灯光类型">灯光类型</h2><h3 id="精确光">精确光</h3><p>Frostbite和Unity3D 只支持两种类型的精确光：<strong>点光源和聚光灯</strong>。为了使精确光在物理上正确，它必须遵循反平方定律，如下图。从恒定亮度的光源观察到的光强与物体距离的平方成正比下降。</p><p>平方反比定律只对点光源有效，不适用于：</p><ol><li>窄分布的泛光灯和探照灯，因为光束是高度聚光的</li><li>区域灯，或像菲涅尔透镜这样的特殊灯</li></ol><p>将反平方定律转化为方程，可得：</p><p>光照度E = I / 距离</p><p><img src="https://cdn.nlark.com/yuque/__latex/92d0b3dcf9aaecac5b50a4a40e06916c.svg" alt="img"></p><p>该方程要求在整个照明计算中，距离单位是均匀的（米，厘米，毫米）</p><p>在Frostbite和Unity3D中，1个单位=1米，定义精确光的尺寸为1厘米。光通量（也叫光功率）总是换算成光强度（ luminous intensity ）进行照明计算。通过对光强度在光照立体角上的积分而得到光照度</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230910155812993.png" alt=""></p><p>光通量到光强度的转换：</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230910155839824.png" alt=""></p><p>【Attenuation衰减】</p><p>平方反比定律的一个问题是它永远不会到达零，但是由于性能原因，渲染器必须实现有限的光照范围，使得光强度为0，以剔除灯光。在某个极限下，光照度应该平滑地达到零，解决这个问题的一种方法就是以这样的方式对falloff进行窗口化的处理，使大部分功能不受到影响，所以使用基于距离的lerp插值到零：</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T5130-3.png" alt=""></p><p>这个简单的方法可行，但会造成硬性的切断，看起来不自然。第二种方法是用一个threshold对函数进行重置到1，并将其重新映射到初始范围0到1：(参考：<a href="https://imdoingitwrong.wordpress.com/2011/02/10/improved-light-attenuation/">https://imdoingitwrong.wordpress.com/2011/02/10/improved-light-attenuation/</a>)</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T5130-4.png" alt=""></p><p>结果较好，但这种方法的缺点是在0处有一个非零的梯度，这会导致一个可见的不连续。更好的方法是对函数进行窗口化处理，并确保lightRadius处的梯度为零。这可以通过提高窗口函数的power来实现</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230910155959371.png" alt=""></p><p>第一张图显示了不同的窗口函数，窗口2是一个不连续的问题，窗口3是一个平滑步长后的 。第二和第三张图显示了分别应用于光半径（Distance）为10和40的范围的窗口函数。这表现了随着光半径的增加，曲线的拟合程度</p><p>寒霜和Unity中都是适用的Window1的衰减函数</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T5130-5.png" alt=""></p><h3 id="IES">IES</h3><p>使用光度轮廓来描述其强度分布，这些分布文件被存储在一个光度测量文件中。存在两种常见的格式：IES(.ies)和eulumdat(.ldt) 。在计算机图形学中，大多数都只支持IES格式，Unity和寒霜也不例外。</p><p>从IES文件创建的光度轮廓可以直接应用在一个点或聚光灯上。IES轮廓可用于描述光强，并可用一个乘法进行调整。这是控制具有发光强度的灯的唯一方法。第二种选择是使用IES配置文件作为Mask，通过配置文件的最大强度进行归一化</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230910160051231.png" alt=""></p><p>为了用光强度点光方程处理这两种情况，我们根据其最大强度对轮廓进行归一化，光亮度计算如下</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T5130-6.png" alt=""></p><p>当创建一个新的光轮廓时，球形光度函数被重建并采样，以填充一个球形参数化(θ，cos（φ）)。我们存储按最大强度的倒数缩放的归一化值，来处理MASK和未Mask的使用。在着色器中，2D纹理被计算并作为衰减使用</p><p>【IES配置文件的使用】</p><p>IES配置文件在室内设计中比在游戏中更有用。能够使用光配置文件作为Mask带来了有趣的效果，IES配置文件可以使用对应的工具创建，并可用于模拟复杂的光影效果，类似于cookie</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230910160137397.png" alt=""></p><h3 id="平行光Sun">平行光Sun</h3><p>太阳是非常重要的光源，特别是对于户外环境。太阳对方向和照度变化非常敏感。将这样的光源作为材质漫射部分的精确光是可接受的近似，但对镜面材质这样做会有问题。在寒霜引擎中，为了部分缓解这些问题，将太阳视为一个总是垂直于外半球的圆盘区域光。艺术家为垂直于太阳方向的表面指定太阳照度（以勒克斯为单位）。</p><p>下图为他们直接使用的光表</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230910160233811.png" alt=""></p><h2 id="灯光参数">灯光参数</h2><p>常用灯光参考值：<a href="https://docs.unity3d.com/Packages/com.unity.render-pipelines.high-definition@13.1/manual/Physical-Light-Units.html">Physical light units - High Definition RP - 13.1.9</a></p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T5130-7.png" alt=""></p>]]></content>
    
    
    <summary type="html">介绍了辐射度量学、光度量学的理论知识(烧脑)，以及游戏引擎中的灯光类型和灯光参数</summary>
    
    
    
    <category term="Notes" scheme="https://himoqiuhan.github.io/categories/Notes/"/>
    
    
    <category term="TA" scheme="https://himoqiuhan.github.io/tags/TA/"/>
    
    <category term="百人计划" scheme="https://himoqiuhan.github.io/tags/%E7%99%BE%E4%BA%BA%E8%AE%A1%E5%88%92/"/>
    
    <category term="Shader" scheme="https://himoqiuhan.github.io/tags/Shader/"/>
    
  </entry>
  
  <entry>
    <title>技术美术百人计划学习笔记（图形5.1.2 基于物理的相机）</title>
    <link href="https://himoqiuhan.github.io/2023/08/12/Notes-TA100-T5120/"/>
    <id>https://himoqiuhan.github.io/2023/08/12/Notes-TA100-T5120/</id>
    <published>2023-08-12T05:01:11.000Z</published>
    <updated>2024-07-27T15:13:59.654Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T5100.png" alt=""></p><h2 id="曝光三角形">曝光三角形</h2><h3 id="曝光主要由三个参数控制">曝光主要由三个参数控制</h3><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230910152658446.png" alt=""></p><ul><li><strong>快门速度（Shutter Speed）</strong>：快门开启的有效时间长度的衡量方式，代表了这个相机拍摄一个图像记录多少光线信息。----可以获得<strong>运动模糊（Motion Blur）</strong></li><li><strong>光圈（Aperture）</strong>：控制一个瞬间进光量的多少。----可以获得<strong>景深（Depth of Field）</strong></li><li><strong>感光度（ISO）</strong>：衡量底片对光的敏感程度。----可以获得<strong>噪点（Grain）</strong></li></ul><p>以上三个参数协同工作，调节进入传感器（Sensor）的光量（由光圈和快门速度控制）和该表面的感光度（传统的胶片或现代的数字ISO）</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230910152719116.png" alt="光先通过光圈(Aperture)控制后，再通过快门（Shutter），最后再到达传感器（ Sensor ）上"></p><h3 id="摄像机流水线">摄像机流水线</h3><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T5120-1.png" alt="摄像机流水线"></p><ul><li>由场景亮度（Scene Luminance）光传递到镜头的光圈（Lens），由光圈（Lens）传递到传感器亮度（Sensor illuminance）</li><li>由传感器亮度传递到快门 （Shutter），由快门 （Shutter）传递到传感器曝光（Sensor Exposure）</li><li>由传感器曝光（Sensor Exposure）传递到电耦合元件（CCD），由电耦合元件（CCD）传递到模拟电压（Analog Voltages），模拟电压（Analog Voltages）再传递到模拟数字转换器（ADC），模拟数字转换器（ADC）再输出数字值（Digital Values）</li><li>对数字值（Digital Values）进行重映射（Remapping）最后输出像素值（Pixel Values）</li></ul><p>摄影师艺术家则可以控制<strong>相对光圈，快门时间，传感器灵敏度/增益三个主要参数来设置曝光</strong></p><p>以人眼为例：</p><ul><li>光圈(Aperture) 的功能就像眼睛的虹膜一样，可以张开并收缩其开口的直径来限制允许进入眼睛的光量</li><li>快门速度(Shutter Speed) 类似于眨眼，只是在我们清醒时眼睑通常是打开的。想象一下，你的眼皮在闭上之前瞬间张开以捕捉单个图像，那就像是相机的快门</li><li>感光度(ISO) 类似于眼睛后部视杆和视锥细胞的灵敏度</li></ul><h3 id="曝光补偿">曝光补偿</h3><p>曝光补偿是一种曝光控制方式，一般常见在±2~3EV。以前胶片时代只能调1个EV，而现在数码能调动1~3个EV。<strong>EV值每增加1.0，相当于进光量增加一倍。EV值每减小1.0，相当于进光量减小一倍</strong>。如果环境光源偏暗调节上述三个参数（快门速度、光圈、感光度）达不到效果或者手动挡调参失败，即可增加曝光值(如调整为+1EV、+2EV)以突显画面的清晰度</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230910153030474.png" alt=""></p><p>曝光补偿就是有意识地变更相机自动演算出的“合适”曝光参数，让照片更明亮或者更昏暗的拍摄手法。拍摄者可以根据自己的想法调节照片的明暗程度，创造出独特的视觉效果等。一般会使用变更光圈值或者快门速度来进行曝光值的调节。<strong>曝光补偿则放在最后</strong></p><h3 id="直方图">直方图</h3><p>直方图是用以表示数字图像中亮度分布的直方图，标绘了图像中每个亮度值的像素数。可以借助观察该直方图了解需要如何调整亮度分布</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230910153110824.png" alt=""></p><p>这种直方图中，横坐标的左侧为纯黑、较暗的区域，而右侧为较亮、纯白的区域。因此，一张较暗图片的图像直方图中的数据多集中于左侧和中间部分；而整体明亮、只有少量阴影的图像则相反</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230910153154292.png" alt=""></p><p>直方图是用来观察照片是否曝光的重要利器，现代数码相机内应该都会内置直方图。直方图主要是为了帮助我们在拍摄的过程中有一个参考，以确保我们所拍摄的照片不会丢失高光或者阴影中的任何细节，帮助我们在拍摄照片的时候就能够分析这张照片有没有丢失细节，或者是否过曝</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T5120-2.png" alt=""></p><h2 id="快门速度">快门速度</h2><h3 id="快门">快门</h3><p>快门 (Shutter) 类似于眨眼，只是在我们清醒时眼睑通常是打开的。想象一下，你的眼皮在闭上之前瞬间张开以捕捉单个图像，那就像是相机的快门</p><blockquote><p>大部分相机的快门需要半按才能对焦，对焦完成可以全按拍摄</p></blockquote><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230910153323590.png" alt=""></p><p>几乎所有现代单反和数码单反相机都使用<strong>焦平面快门</strong>。焦平面快门的工作方式更像车库的门。但有第二扇门，称为帘幕。在第一个帘幕打开后，帘幕紧跟在第一个帘幕后面以关闭图像传感器或胶片。这种双帘设计可实现传感器或胶片上的光线平衡，从而使曝光均匀。这种设计允许在当今的单镜头反光（SLR） 和数码单反相机（DSLR） 相机上实现极快的操作，最高可达 1/8000 秒。</p><p>如今的快门已经数字化了。许多<strong>现代数码相机操作电子快门</strong>，我们只需在按下的瞬间为数字传感器供电。所以电子快门速度可以非常快。快门打开和关闭到整个传感器的过程一次完成，滚动电子快门在横跨其宽度的时间内激活一行像素</p><h3 id="快门速度-2">快门速度</h3><p>定义：快门速度是衡量相机快门打开的时间——允许光线照射到感光传感器的时间</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T5120-3.png" alt=""></p><p>焦平面快门和滚动电子快门，由于它们的设计和功能，当图像平面上有快速运动时，都会导致图像产生有趣的失真</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230910153438492.png" alt=""></p><p>所以当我们拍摄高度运动的物体，如汽车，奔跑的人时，<strong>快门速度应该调快，才可以弥补这种视觉残影的效果。但是这意味着感光器收到的光线不足</strong>，也就是接下来要解决的问题</p><h2 id="光圈">光圈</h2><p>光圈是一个用来<strong>控制光线透过镜头，进入机身内传感器光量</strong>的装置，通常是在镜头内。已经制造好的镜头，我们不可能随意改变镜头的直径，但是我们可以通过在镜头内部加入多边形或者圆形，且面积可变的孔状光栅片来达到控制镜头通光量，这个装置就叫做光圈</p><p>光圈(Aperture) 的功能就像眼睛的虹膜一样，可以张开并收缩其开口的直径来<strong>限制允许进入眼睛的光量</strong></p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230910153542931.png" alt=""></p><h3 id="光圈大小">光圈大小</h3><p>表达光圈大小我们是用F数表示通常称为f-number(N)，记作F/。透镜的焦距(F)与孔径(D)的比，我们记为N= F/D ,孔径(D)越大，进入相机的光线就越多。光圈不等同于F数，恰恰相反，<strong>光圈大小与F数大小成反比</strong>，F数又称光圈数。如大光圈的镜头，F数小，光圈数小；小光圈的镜头，F数大</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230910153627625.png" alt="不同F值光圈的图形表示"></p><p>比如F/1.2、F/1.8、F/2.8这三组数值都表示大光圈（一般F/+一个数值特指光圈数值），其中F/1.2的光圈最大，而F/18、F/22、F/32则表示小光圈，其中F/32最小</p><p>【光圈的计算】</p><p>光圈可以理解为一个圆形，圆的面积公式是：面积 =  π 乘以半径的平方</p><blockquote><p>光圈大小的相邻间隔都是以根号二为倍数的（也就是以面积为两倍的相邻间隔进行区分）</p></blockquote><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230910153718679.png" alt="光圈的计算方式"></p><p>所以如果将相机光圈设置为 F/8，拍摄一张图，然后将光圈调整到 F/5.6，那么通过镜头的光量则增加了一倍（x2）。从 F/8 改为F/4 会使光量增加三倍（x4）。从 F/11 到 F/16 则减少了一半的光量（x1/2）。遵循<strong>反平方定律</strong></p><p>【光圈与景深】</p><p>如果将光圈从 f/16 扩大到 f/11，进光面积增加为原来2倍， 照片将获得 +1 EV （曝光值）结果，f/16 到 f/8 使光圈开口尺寸加倍，进光面积增加为原来四倍，则表示 +2 EV 偏移。回到上面，曝光部分，如果我们光圈小了，则可以通过控制EV来增强。</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230910153757032.png" alt=""></p><p>同时，在光圈大的图像中，拍摄主体的边缘会有明显的模糊效果，这个效果被称为<strong>景深</strong></p><blockquote><p>景深定义：在图像中<strong>看起来清晰对焦的最近和最远对象之间的一个距离量</strong>。例如摄影过程中，主物体清晰对焦，但其余部分会模糊。<strong>景深起到了感知深度的作用</strong>。</p></blockquote><p>【衍射现象】</p><p>光圈不仅控制通过镜头的光量，还影响光线穿过镜头时的角度。<strong>光线经过孔径光栅时被轻微弯曲</strong>，光的这种弯曲称为“<strong>衍射</strong>”，是光的波动特性</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230910153849851.png" alt=""></p><p>缩小镜头的孔径光栅时，会使衍射更接近图像的中心。很多摄影师在刚开始了解光圈的时候，因为光圈对景深的影响，认为最大化锐度的关键是小光圈。然而这是不正确的。<strong>由于存在衍射，尽管通过缩小光圈来去除景深，但图像中的衍射量也在增加，这就会导致图像失去清晰度。<strong>光圈的中间地带，即</strong>景深且衍射可控的区域</strong>，被称为镜头的“最佳位置” ，<strong>一般是在 f/4 和 f/11 之间的区域，具体数值取决于镜头的设计</strong></p><h2 id="ISO感光度">ISO感光度</h2><p>感光度(ISO) 类似于眼睛后部视杆和视锥细胞的灵敏度，与快门速度一样是线性的。ISO 为 200 对光的敏感度是 ISO 等级为 400 的一半。ISO 加倍，感光度加倍。一半的 ISO，一半的感光度</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230910154023733.png" alt="不同的ISO会使画面亮度不一样"></p><p>因此，假设所有其他条件都不变，如果将相机的 ISO 从 400 更改为 200，则得到-1的EV （曝光）偏移，ISO 800 更改为 ISO 1600 ，感光度加倍，则+1 EV偏移</p><h2 id="胶片颗粒Grain">胶片颗粒Grain</h2><p>提升数码相机的ISO是通过两种方式实现的：</p><ol><li>强行提高每个像素点的亮度和对比度；</li><li>使用多个像素点共同完成原来只要一个像素点来完成的任务。</li></ol><p>这也导致了一个效果——数字噪点（Grain）ISO 越高，图像中引入的数字噪点就越多。少量的胶片颗粒感可以改善照片的感觉和质感，但是这并不是可以为之的</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230910154120666.png" alt="image-20230910154120666"></p><h2 id="相机与引擎">相机与引擎</h2><h3 id="构建HDRP下基于物理的摄像机环境">构建HDRP下基于物理的摄像机环境</h3><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T5120-4.png" alt=""></p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T5120-5.png" alt=""></p><p>HDRP渲染管线环境，我们在Camera组件下找到Camera Body和 Lens 以及Aperture，勾选上Physical Camera同时将Volume组件挂上Exposure和Depth of field并且设置Mode为Camera。即可启用基于物理的相机。</p><p>这里的基于物理是根据相机和摄影知识理论构建的，但是目前阶段不算是很完善，比如Grain并不会因为ISO过高而产生效果，需要自己额外再Volume组件设置。也不会因为光圈越小产生衍射</p><h3 id="快门速度、光圈、感光度的调节">快门速度、光圈、感光度的调节</h3><p>快门速度：其他属性都不变的情况下，快门速度越快，曝光就越低，呈<strong>线性下降</strong></p><p>光圈：其他属性都不变的情况下，光圈数越大，曝光就越小，呈<strong>指数下降</strong> ，景深效果越弱</p><p>感光度：其他属性都不变的情况下，感光度越小，曝光值越小。因为没有Grain效果 ，所以高感光度所产生的Grain效果并没有增加上</p><h3 id="焦距-Focus-Distance-和焦长-Focal-Lengh">焦距(Focus Distance)和焦长(Focal Lengh)</h3><p>焦距主要用来对焦，单反相机一般会自动对焦，也可以手动对焦</p><p>焦长则模拟了不同类型的镜头，从拍摄花草鱼虫的微距镜头一直到打鸟射月的长焦镜头的模拟</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T5120-6.png" alt=""></p><h2 id="自发光与Bloom">自发光与Bloom</h2><p>【Bloom的原因】</p><ol><li>高亮的值使得数码相机的传感器饱和，并且<strong>泄漏到临近的传感器单元</strong></li><li>光线在摄像机镜头内反射</li><li>相机镜头内或者表面有灰尘</li></ol><p>像素的最终亮度取决于相机的设置，EV值决定了是否产生Bloom的效果。当入射的亮度超过了传感器本身最大的亮度值，就会产生Bloom</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230910154411206.png" alt=""></p><p>【难题：特效设计师想要白天和夜晚效果一致的Bloom】</p><p>–对于自发光表面：可以给设计师提供工具控制Bloom：</p><ol><li>给自发光表面定义“曝光补偿”，用于调整它的强度，确保该强度超过饱和点。</li><li>该项是场景全局值，额外的曝光被注入场景内，使得自发光表面变量或者变暗。</li></ol><p>【HDRP对于自发光的强度控制】</p><p><code>_EmissiveIntensity</code> 不是一个独立的属性。 着色器仅使用 <code>_EmissiveIntensity</code> 来序列化 UI 中的属性，并将最终结果存储在 <code>_EmissiveColor</code> 属性中</p><h2 id="Sunny-16法则">Sunny-16法则</h2><ul><li><p>是摄影术中不借助电子测光表来<strong>估计照相机的光圈大小和快门长短的方式</strong></p></li><li><p><strong>阳光16法则示例：在室外阳光下，光圈为F/16 ，ISO为100，快门时间为1/125s</strong></p></li><li><p>在<strong>室外阳光下</strong>，如果光圈是 f/16，则快门速度应是所用胶片的国际感光度指数的倒数</p><ul><li>例如，在<strong>室外阳光下</strong>，如用光圈是 f/16，而ISO为100，则快门应为 1/100s（或者1/125）。</li></ul></li><li><p>“16”，并不是光圈非f/16不可，如选用f/11,则快门速度当提高一倍成为 1/200秒，以此类推。</p><ul><li>其次，<strong>要根据天气状况作调整</strong>，如并非阳光普照，而是多云，ISO为100，如果快门速度仍旧取 1/100s，则光圈应从1/16开大一倍到1/11；依此类推。</li></ul></li></ul>]]></content>
    
    
    <summary type="html">介绍了真实世界中相机的运作原理、各个参数的影响、Bloom的物理原理，以及Unity的基于物理的相机</summary>
    
    
    
    <category term="Notes" scheme="https://himoqiuhan.github.io/categories/Notes/"/>
    
    
    <category term="TA" scheme="https://himoqiuhan.github.io/tags/TA/"/>
    
    <category term="百人计划" scheme="https://himoqiuhan.github.io/tags/%E7%99%BE%E4%BA%BA%E8%AE%A1%E5%88%92/"/>
    
    <category term="Shader" scheme="https://himoqiuhan.github.io/tags/Shader/"/>
    
  </entry>
  
  <entry>
    <title>技术美术百人计划学习笔记（图形5.1.1 基于物理的光照模型）</title>
    <link href="https://himoqiuhan.github.io/2023/08/12/Notes-TA100-T5110/"/>
    <id>https://himoqiuhan.github.io/2023/08/12/Notes-TA100-T5110/</id>
    <published>2023-08-12T04:01:11.000Z</published>
    <updated>2024-07-27T15:13:59.654Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T5100.png" alt=""></p><h2 id="PBR框架概述">PBR框架概述</h2><p>PBR框架包括：</p><ul><li>基于物理的材质（光照模型）</li><li>基于物理的灯光</li><li>基于物理的相机</li><li>以及美术的PBR全流程。</li></ul><p>PBR背后更重要的是PBR带来的<strong>框架理念：轻松地利用一个框架去保证项目的整体画面统一</strong></p><p>“基于物理”是对现实世界的近似，需要满足三个条件：</p><ul><li>基于微平面（Microfacet）的表面模型</li><li>能量守恒</li><li>应用基于物理的BRDF</li></ul><h2 id="微平面理论">微平面理论</h2><p>概念：将物体表面建模成做无数微观尺度上有<strong>随机朝向</strong>的理想<strong>镜面反射</strong>的小平面（microfacet）的理论</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T5110-1.png" alt=""></p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T5110-23.png" alt="TA100T5110-23"></p><h2 id="能量守恒">能量守恒</h2><p>概念：出射光线的能量永远不能大于入射光线的能量</p><p>表现：随着粗糙度的上升，镜面反射区域的面积会增加，作为平衡，镜面反射区域的平均亮度则会下降</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T5110-2.png" alt=""></p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T5110-3.png" alt=""></p><h3 id="如何能量守恒">如何能量守恒</h3><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230910151314019.png" alt=""></p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T5110-4.png" alt="TA100T5110-4"></p><p>简单理解就是：</p><p>反射光的强度 = 入射光的强度 * 反射比例 + 入射光的衰减</p><p>最终出射光的强度 = 反射光的强度 + 自发光的强度</p><p>入射光的衰减 = 半角向量和法线的点积</p><p>【唯一的难点就是知道<strong>反射比例</strong>是多少】</p><h3 id="BRDF-–-计算反射比例">BRDF – 计算反射比例</h3><p>Bidirectional Reflectance Distribution Function，双向反射分布函数。“双向”意为相机方向和光源方向调换之后，他们所计算出来的能量等级是一致的</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T5110-5.png" alt=""></p><p>BTDF：双向透射分布函数，用于描述光线透过物体的表现，其也是包含高光反射和漫反射</p><p><strong>BSDF = BRDF + BTDF（BSDF双向散射分布函数）</strong></p><p>BRDF<img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T5110-24.png" alt="TA100T5110-24"></p><p>BRDF函数作用是：通过入射光的方向和出射光的方向，得到反射比例</p><h2 id="应用基于物理的BRDF">应用基于物理的BRDF</h2><h3 id="BRDF的计算">BRDF的计算</h3><p>一般将反射拆分为漫反射和高光反射：</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T5110-6.png" alt=""></p><h3 id="漫反射">漫反射</h3><p>算法基本采用Lambert光照模型（经验模型）</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T5110-7.png" alt=""></p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230910151549710.png" alt=""></p><ul><li>Kd：漫反射系数，与高光反射系数Ks加和为1</li><li>（I / r ^2）：描述光线能量的衰减</li></ul><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T5110-8.png" alt=""></p><h3 id="高光反射">高光反射</h3><ul><li><p>经验模型</p><ul><li><p>Phong模型</p></li><li><p>Blinn-Phong模型</p></li></ul></li></ul><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T5110-9.png" alt=""></p><ul><li>基于物理的高光反射：Cook-Torrance反射率方程</li></ul><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230910151657659.png" alt=""></p><h4 id="D：NDF法线分布函数">D：NDF法线分布函数</h4><p>“Normal Distrubution Function”</p><p>GGX算法：更接近于物理</p><p>此处使用的是Trowbridge-Reitz GGX法线分布函数</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230910151730565.png" alt=""></p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T5110-12.png" alt=""></p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T5110-13.png" alt="绿色为GGX"></p><h4 id="G：几何遮蔽函数">G：几何遮蔽函数</h4><p>我们使用的微平面理论每个面的计算是互不干扰的，但实际中物体表面凹凸存在相互遮蔽的情况。</p><p>因此，几何函数从<strong>统计学上近似的求得了微平面间相互遮蔽的比率</strong>。这种相互遮蔽会损耗光线的能量。（除了被吸收，还有被自身遮蔽带来的能量损耗）</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230910151853240.png" alt=""></p><p>传统的几何项模型中，需要考虑光线方向和视线方向两种情况的遮蔽效应，所以最终写为：</p><p><img src="https://cdn.nlark.com/yuque/__latex/8f568f8403f69833e0b8fde074d3c597.svg" alt="img"></p><p>使用史密斯法与Schlick-GGX作为Gsub可以得到如下所示不同粗糙度的视觉效果</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T5110-15.png" alt="0：没有微平面阴影 -- 1：微平面彻底被遮蔽"></p><h4 id="F：菲涅尔方程">F：菲涅尔方程</h4><p>被反射的光线对比光线被折射的部分所占的比率（物体的边缘的更亮一些）</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230910152002086.png" alt=""></p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T5110-17.png" alt=""></p><h3 id="总结">总结</h3><p><strong>BRDF的核心算法是Cook-Torrance反射率方程</strong></p><p><img src="https://cdn.nlark.com/yuque/__latex/7bbad6903073c85946379ebb8a7aec5f.svg" alt="img"></p><p>漫反射：Lambert</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T5110-18.png" alt=""></p><p>D：</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T5110-19.png" alt=""></p><p>G：</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T5110-20.png" alt=""></p><p>F：</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T5110-21.png" alt=""></p><h2 id="URP中的PBR">URP中的PBR</h2><p>Specular工作流的高光颜色由Specular Map控制；而Metallic工作流的高光颜色与Base Color相同</p><p>直接看源码就好，顺着URP默认材质的Pass，找到Lit.shader和Lighting.hlsl研究就行</p><h2 id="迪士尼原则的BRDF">迪士尼原则的BRDF</h2><ul><li>使用直观的参数，而不是物理类的晦涩参数</li><li>参数应尽可能少</li><li>参数在其合理范围内应该为0到1</li><li>允许参数在有意义时超出正常的合理范围</li><li>所有参数组合应尽可能健壮和合理</li></ul><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T5110-22.png" alt="用了11个参数即可非常真实地模拟出金属、非金属以及不同粗糙度的材质光照结果"></p>]]></content>
    
    
    <summary type="html">简单介绍了PBR的思想，以及Cook-Torrance反射率方程</summary>
    
    
    
    <category term="Notes" scheme="https://himoqiuhan.github.io/categories/Notes/"/>
    
    
    <category term="TA" scheme="https://himoqiuhan.github.io/tags/TA/"/>
    
    <category term="百人计划" scheme="https://himoqiuhan.github.io/tags/%E7%99%BE%E4%BA%BA%E8%AE%A1%E5%88%92/"/>
    
    <category term="Shader" scheme="https://himoqiuhan.github.io/tags/Shader/"/>
    
  </entry>
  
  <entry>
    <title>技术美术百人计划学习笔记（图形4.5 DOF景深算法）</title>
    <link href="https://himoqiuhan.github.io/2023/08/11/Notes-TA100-T4500/"/>
    <id>https://himoqiuhan.github.io/2023/08/11/Notes-TA100-T4500/</id>
    <published>2023-08-11T06:01:11.000Z</published>
    <updated>2024-07-27T15:13:59.654Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T4500.png" alt=""></p><h2 id="什么是景深">什么是景深</h2><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T4500-1.png" alt=""></p><p>景深的原理：离散圈</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T4500-2.png" alt=""></p><h2 id="景深的作用">景深的作用</h2><p>选择性的<strong>突出或者强调</strong>画面中的一部分，例如某个物体或者某个人物，吸引观察者的注意力到画面中清晰对焦的部分，而忽略其他的模糊部分的细节。</p><p>强调所拍摄<strong>场景的深度</strong>，增加画面的<strong>层次立体感</strong>。</p><p><strong>艺术意境</strong>的表达。摄影师可以利用景深效果，营造出虚幻、梦境、或者神奇等意境。表示主观的视线。</p><p>在电影学中，通过调节浅景深的镜头，使之对焦在不同位置上，来表示某个人的主观<strong>视线的转移</strong>，<strong>交代人物之间的关系</strong>。在电影学中，通过景深聚焦位置的变化，来表达前景和背景人物之间的关系。</p><h2 id="景深的制作">景深的制作</h2><h3 id="制作思路">制作思路</h3><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T4500-3.png" alt=""></p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T4500-4.png" alt=""></p><h3 id="截取景深区域">截取景深区域</h3><p>【深度图获取】</p><p>先获取深度图，如果需要让远裁剪面不影响实际的景深效果，则需要对读取深度图后的深度值进行处理：乘上ProjectionParams.z</p><figure class="highlight glsl"><table><tr><td class="code"><pre><span class="line">depth = Linear01Depth(tex2D(_CameraDepthTexture, i.uv)) * _ProjectionParams.z;</span><br></pre></td></tr></table></figure><p>【通过深度值提取景深范围】</p><p>定义一个_FocusDistance和一个_DepthOfDield，景深的范围就是[_FocusDistance - _DepthOfField, _FocusDistance + _DepthOfField]</p><p>通过景深值与深度值进行比较可获得景深的模糊区域遮罩</p><figure class="highlight glsl"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span>(depth &lt; focusNear)</span><br><span class="line">&#123;</span><br><span class="line">  final_depth = saturate(<span class="built_in">abs</span>(focusNear - depth) * _SmoothRange);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span>(depth &gt; focusNear)</span><br><span class="line">&#123;</span><br><span class="line">  final_depth = saturate(<span class="built_in">abs</span>(depth - focusFar) * _SmoothRange);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="模糊处理与贴图合并">模糊处理与贴图合并</h3><p>与高斯模糊、bloom效果制作相似</p><h2 id="高级景深效果思路拓展">高级景深效果思路拓展</h2><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T4500-5.png" alt=""></p><ul><li>(a) p在背景区域</li><li>(b) p在前景区域</li><li>© p在聚焦区域</li></ul><p>不同区域可以使用不同滤波，甚至使用不同滤波方法</p><h3 id="颜色泄露缺陷">颜色泄露缺陷</h3><p>在对后处理的对焦区域之外进行模糊处理的过程中，将模糊的背景色叠加在聚焦区域之上，或者前景聚焦区域的颜色混合到了模糊背景之中（类似于Bloom的“扩散”效果）</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T4500-6.png" alt=""></p><p>为了解决这个问题，可以使用<strong>扩散滤波</strong></p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T4500-7.png" alt=""></p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T4500-13.png" alt=""></p><p>原理是将每个像素点的颜色扩散到这个像素点的<strong>模糊圈范围中</strong>，这样由于聚焦区域以外的像素有大量的模糊圈，所以被模糊了；而聚焦区域的模糊圈直径小于一个像素，所以颜色就不会扩散，保持清晰</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T4500-8.png" alt="左为原图，右为通过扩散滤波得到的结果"></p><h3 id="模糊的不连续缺陷">模糊的不连续缺陷</h3><p>产生原因是焦点的像素为零，而前景区域大于0</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T4500-9.png" alt=""></p><p>解决方法：单独取出前景和后景进行区分，把前景进行单独的模糊处理，然后再和背景进行融合</p><h2 id="散景的模拟（Bokeh）">散景的模拟（Bokeh）</h2><p><strong>焦外成像</strong></p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T4500-10.png" alt=""></p><p>在背景滤波的基础上可以通过<strong>点函数来模拟散景效果</strong></p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T4500-11.png" alt="一个单色光点光源在不同的参数下成像"></p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T4500-12.png" alt="(a): 原始图像 (b): 真实相机的景深模糊效果 (c): 高斯分布的点扩散函数的景深模糊效果(d): 均匀分布的圆形点扩散函数的景深模糊效果 (e): 均匀分布的六边形点扩散函数的景深模糊效果"></p><h2 id="UE在18年的景深效果">UE在18年的景深效果</h2><p><a href="https://epicgames.ent.box.com/s/s86j70iamxvsuu6j35pilypficznec04">https://epicgames.ent.box.com/s/s86j70iamxvsuu6j35pilypficznec04</a></p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230910144904055.png" alt=""></p>]]></content>
    
    
    <summary type="html">DOF景深效果的基础实现</summary>
    
    
    
    <category term="Notes" scheme="https://himoqiuhan.github.io/categories/Notes/"/>
    
    
    <category term="TA" scheme="https://himoqiuhan.github.io/tags/TA/"/>
    
    <category term="百人计划" scheme="https://himoqiuhan.github.io/tags/%E7%99%BE%E4%BA%BA%E8%AE%A1%E5%88%92/"/>
    
    <category term="Shader" scheme="https://himoqiuhan.github.io/tags/Shader/"/>
    
  </entry>
  
  <entry>
    <title>技术美术百人计划学习笔记（图形4.1 Bloom算法）</title>
    <link href="https://himoqiuhan.github.io/2023/08/11/Notes-TA100-T4100/"/>
    <id>https://himoqiuhan.github.io/2023/08/11/Notes-TA100-T4100/</id>
    <published>2023-08-11T03:01:11.000Z</published>
    <updated>2024-07-27T15:13:59.654Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T4100.png" alt=""></p><h2 id="Bloom算法">Bloom算法</h2><h3 id="Bloom算法介绍">Bloom算法介绍</h3><h4 id="Bloom的效果">Bloom的效果</h4><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T4100-1.png" alt=""></p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230910142602489.png" alt=""></p><p>Bloom，也称辉光效果。用于模拟摄像机的一种图像效果（光向周围扩散的效果，原理是高亮的值使得数码相机的传感器饱和，并且<strong>泄漏到临近的传感器单元</strong>），让物体具有真实的明亮效果</p><h3 id="实现思路">实现思路</h3><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T4100-2.png" alt=""></p><h3 id="HDR与LDR">HDR与LDR</h3><p>LDR：Low Dynamic Range，低动态范围</p><ul><li>JPG、PNG格式图片</li><li>RGB范围在[0,1]之间，造成精度丢失</li></ul><p>HDR：High Dynamic Range，高动态范围</p><ul><li>HDR、EXR格式图片</li><li>RGB范围可超过1，提取亮度大于1的区域作为辉光区域（在LDR中，一些不应该有Bloom效果的区域也可能因为亮度过高而带有不合适的Bloom效果，比如如下的地面反射的区域不应该有Bloom）</li></ul><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T4100-3.png" alt=""></p><h3 id="高斯模糊">高斯模糊</h3><p>一种图像模糊处理方法，目的是减少图像噪声、降低细节层次。通过高斯函数得到的高斯核，对图像信息进行卷积运算。</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230910142750916.png" alt=""></p><p>【计算高斯核】</p><p>核中心(0,0)，核大小3x3，标准方差σ为1.5</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T4100-5.png" alt=""></p><p>【二维高斯核的优化】</p><p>二维高斯核计算量大，需要N*N*W*H次纹理采样</p><p>但是二维高斯核具有可分离性，可拆成两个一维高斯核，横竖方向分别进行两次高斯模糊，只需要2*N*W*H</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T4100-6.png" alt=""></p><h2 id="Bloom算法实现">Bloom算法实现</h2><p>思路：</p><ul><li>C#：URP需要用Render Feature增加Pass，Built-in管线调用OnRenderImage函数。核心就是处理图像存为RT，再将RT传给shader作为处理的源图像进行下一步处理。</li><li>shader：使用4个Pass完成Bloom效果（Pass1用于提取亮度，Pass2和Pass3进行不同方向的高斯模糊，Pass4用于混合），<strong>实际上在脚本流中将uv偏移值传入shader可以用一个Pass完成处理</strong><ul><li>提取亮度时，用相减做clamp比用step更省性能，因为clamp只需要进行一次比较，而step需要进行两次比较</li></ul></li></ul><h2 id="Bloom的应用">Bloom的应用</h2><h3 id="配合自发光">配合自发光</h3><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T4100-7.png" alt=""></p><h3 id="配合特效">配合特效</h3><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T4100-8.png" alt=""></p><h3 id="实现GodRay效果">实现GodRay效果</h3><p>实现思路与Bloom效果类似，通过一个设定好的亮度阈值去提取原图像中比较亮的区域，区别在于不用高斯模糊进行模糊处理，而是使用径向模糊Radial Blur，对提取后的图像进行多次模糊处理来模拟光线扩散的效果，最后进行图像混合</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T4100-9.png" alt="="></p><h3 id="配合Tonemapping">配合Tonemapping</h3><p>以下两图的右侧都是使用ACES模式的色调映射效果，配合了ACES模式的Bloom效果饱和度偏低，视觉上更加柔和，较好的保留暗部和亮部的细节</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T4100-10.png" alt=""></p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T4100-11.png" alt=""></p>]]></content>
    
    
    <summary type="html">初级Bloom的基础实现原理</summary>
    
    
    
    <category term="Notes" scheme="https://himoqiuhan.github.io/categories/Notes/"/>
    
    
    <category term="TA" scheme="https://himoqiuhan.github.io/tags/TA/"/>
    
    <category term="百人计划" scheme="https://himoqiuhan.github.io/tags/%E7%99%BE%E4%BA%BA%E8%AE%A1%E5%88%92/"/>
    
    <category term="Shader" scheme="https://himoqiuhan.github.io/tags/Shader/"/>
    
  </entry>
  
  <entry>
    <title>技术美术百人计划学习笔记（图形3.4 延迟渲染原理介绍）</title>
    <link href="https://himoqiuhan.github.io/2023/08/10/Notes-TA100-T3400/"/>
    <id>https://himoqiuhan.github.io/2023/08/10/Notes-TA100-T3400/</id>
    <published>2023-08-10T06:01:11.000Z</published>
    <updated>2024-07-27T15:13:59.654Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T3400.png" alt=""></p><h2 id="延迟渲染原理介绍">延迟渲染原理介绍</h2><h3 id="渲染路径-Rendering-Path">渲染路径 Rendering Path</h3><p>渲染路径是决定光照的实现方式，简而言之就是当前渲染目标使用<strong>光照的流程</strong></p><h3 id="渲染方式">渲染方式</h3><ul><li><p>前向渲染（Forward Rendering）</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T3400-1.png" alt=""></p></li><li><p>延迟渲染（Deferred Rendering）</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T3400-2.png" alt=""></p></li></ul><h3 id="前向渲染">前向渲染</h3><p>在渲染每一帧时，每个顶点/片元都要执行一次片元着色器代码，这时需要将所有的光照信息都传递到片元着色器中。虽然大部分情况下的光源都趋向于小型化，而其照亮的区域也不大，但即便是光源离这个像素所对应的世界空间中的位置很远，但计算光照时，还是会把所有的光源都考虑进去，会造成极大的浪费</p><p>例如，物体受n个光源影响，那么在每一个片元执行着色器代码时，都必须吧这n个光源都传递进着色器中执行光照计算</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T3400-2.png" alt="待渲染几何体→顶点着色器→片元着色器 →渲染目标"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">For Each light:</span><br><span class="line">For each object affected by light:</span><br><span class="line">framebuffer += brdf(object, light)</span><br></pre></td></tr></table></figure><h3 id="延迟渲染">延迟渲染</h3><h4 id="延迟渲染概述">延迟渲染概述</h4><p>主要解决<strong>大量光照渲染</strong>的方案</p><p>延迟渲染的实质，是先不要做迭代三角形做光照计算，而是<strong>先找出来你能看到的所有像素，再去迭代光照</strong>。直接迭代三角形的话，由于大量三角形你是看不到的，无疑是极大的浪费</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T3400-4.png" alt=""></p><p>上图中，前向渲染下，计算红绿两个不同距离的光照计算的开销是相同的（因为都是在世界空间或切线空间进行的光照计算，无法知道在屏幕空间上会显示多少像素），但是明显能看出来近处的绿光有效像素更多，说明红光造成的浪费更多；而对于延迟渲染来说，远处光照的计算开销是要远远小于近处的光照计算开销，因为<strong>延迟渲染是将所有的顶点信息都渲染到了一张基于屏幕的RT上</strong>，然后再进行光照计算</p><h4 id="延迟渲染详细说明">延迟渲染详细说明</h4><p>将渲染过程拆分成两个渲染通路（pass）：</p><ul><li>第一个pass称为<strong>几何处理通路</strong>。首先将场景渲染一次，获取到待渲染对象的<strong>各种几何信息</strong>存储到名为<strong>G-buffer</strong>的缓冲区中，这些缓冲区将会在之后用作更复杂的光照计算。由于有深度测试，所以最终写入G-buffer中的各个数据都是离摄像机**最近的片元(一定可见的)**的几何属性，这意味着最后在G-buffer中的片元必定要进行光照计算的。</li><li>第二个pass称为<strong>光照处理通路</strong>。该pass会遍历所有G-buffer中的位置、颜色、法线等参数，执行<strong>一次</strong>光照计算。</li></ul><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T3400-5.png" alt="待渲染几何体→顶点着色器→MRT→光照计算→渲染目标"></p><ul><li>实际上Fragment Shader也是输出了片元颜色的，只不过没有进行光照计算</li><li>延迟渲染无法支持半透明物体的渲染，在延迟渲染管线下渲染半透明物体，只能是在延迟渲染处理完成之后，最后再用前向渲染的方式去渲染半透明物体</li><li>G-Buffer中的数据都是2D的，所以<strong>最终的光照计算相当于2D的光照后处理</strong></li></ul><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">For each object:</span><br><span class="line">  Render to multiple targets</span><br><span class="line">For each light:</span><br><span class="line">  Apply light as a 2D postprocess</span><br></pre></td></tr></table></figure><h3 id="不同渲染路径的特性">不同渲染路径的特性</h3><h4 id="后处理方式不同">后处理方式不同</h4><p>如果需要深度信息来进行后处理，前向渲染就需要单独渲染出一张深度图，而延迟渲染直接从G-Buffer中拿深度图即可</p><h4 id="着色计算不同-Shader">着色计算不同(Shader)</h4><p>由于延迟渲染光照计算统一是在LightPass中完成的，所以<strong>只能计算一个光照模型</strong>。<strong>如果需要其他的光照模型，只能切换Pass</strong></p><h4 id="抗锯齿方式不同">抗锯齿方式不同</h4><p>（后面细说）</p><h3 id="不同渲染路径的优劣">不同渲染路径的优劣</h3><p>前向渲染的缺点：</p><ol><li>光源数量对计算复杂度影响大</li><li>访问深度数据时需要额外计算</li></ol><p>前向渲染的优点：</p><ol><li>支持半透明渲染</li><li>支持使用多个光照Pass</li><li>支持自定义光照计算方式</li></ol><p>延迟渲染的缺点：</p><ol><li>对MSAA支持不友好</li><li>透明物体渲染存在问题（深度的问题）</li><li>占用大量的显存带宽（每一帧都需要几张RT在显存中传输、清理）</li></ol><p>延迟渲染的优点：</p><ol><li>大量光照场景优势明显</li><li>只渲染可见像素，节省计算量</li><li>对后处理支持良好</li><li>用更少的shader</li></ol><h2 id="其他">其他</h2><h3 id="渲染路径设置">渲染路径设置</h3><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T3400-6.png" alt=""></p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T3400-7.png" alt=""></p><h3 id="移动端优化">移动端优化</h3><ul><li><p>两个TBDR的优化方式</p><ul><li><p>SIGGRAPH2010上提出的，通过分块来解决降低带宽内存用量</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T3400-8.png" alt=""></p></li><li><p>PowerVR基于手机GPU的TBR架构提出的，通过HSR减少overdraw（做一些可见性测试）</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T3400-9.png" alt=""></p></li></ul></li></ul><h3 id="其他渲染路径">其他渲染路径</h3><p>参考资料：<a href="https://zhuanlan.zhihu.com/p/54694743">https://zhuanlan.zhihu.com/p/54694743</a></p><h4 id="延迟光照（Lighting-Pre-Pass-Deferred-Lighting">延迟光照（Lighting Pre-Pass / Deferred Lighting)</h4><p>减少G-Buffer占用过多开销，支持多种光照模型</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T3400-10.png" alt=""></p><h4 id="Forward-（即Tiled-Forward-Rendering，分块前向渲染）">Forward+（即Tiled Forward Rendering，分块前向渲染）</h4><p>减少带宽，支持多光源，强制需要一个Pre-Z进行深度预计算的pass</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T3400-11.png" alt=""></p><h4 id="群组渲染（Cluster-Rendering）">群组渲染（Cluster Rendering）</h4><p>带宽相对减少，多光源下效率提升</p><h3 id="MSAA与延迟渲染的不兼容">MSAA与延迟渲染的不兼容</h3><p>MSAA在延迟渲染中的问题是：像素在进行光照计算前已经被光栅化了，MSAA需要基于sub-pixel数据进行处理，光栅化后的每个像素的sub-pixel们是一样的，处理无效，所以没有办法用精度更高的像素来进行渲染。</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T3400-12.png" alt="https://catlikecoding.com/unity/tutorials/rendering/part-13/"></p><p>但是如果非要做MSAA的话，有如下参考资料：</p><p><a href="https://docs.nvidia.com/gameworks/index.html#gameworkslibrary/graphicssamples/d3d_samples/antialiaseddeferredrendering.htm">Anti-aliased Deferred Rendering – NVIDIA</a></p><h3 id="不同Path下光源shader的编写">不同Path下光源shader的编写</h3><p>由shader中的&quot;<strong>LightMode</strong>&quot;进行控制</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T3400-13.png" alt=""></p><h3 id="PreZ-Zprepass">PreZ/Zprepass</h3><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T3400-14.png" alt=""></p><p>实际上就是一个深度计算，与深度图一样都是用一个pass去计算深度。PreZ与深度图的区别在于，深度图是将深度信息绘制到了一张RT上，为了记录数据并方便进行数据间的传输；而PreZ/Zprepass是硬件自动进行的，只是计算深度然后在shader中使用（做屏幕等距边缘光的时候用到了），或者透明排序的时候会涉及三段深度排序的方式，来让透明的物体深度正确。</p><blockquote><p>如果在shader中要用到DepthOnlyPass获取到的深度图的话，需要勾选Depth Texture的生成——屏幕空间等距边缘光</p></blockquote><h2 id="游戏引擎中的光照算法（文中知乎链接笔记）">游戏引擎中的光照算法（<a href="https://zhuanlan.zhihu.com/p/54694743">文中知乎链接</a>笔记）</h2><h3 id="Forwar-Render">Forwar Render</h3><p>处理多光源有限，基本算法为：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">For Each Light:</span><br><span class="line">  For Each Object Affected By Light:</span><br><span class="line">      framebuffer += brdf(object, light)</span><br></pre></td></tr></table></figure><ul><li><p>不可见的面会造成shading的浪费</p></li><li><p>Batch数量高：每个物体的每个光源就会有一个batch(1 batch/object/light)</p><ul><li>如果有shadow-casting，batch会更多</li></ul></li><li><p>在每个pass内有大量重复的工作</p><ul><li><p>顶点初始化和空间变化</p></li><li><p>各向异性过滤</p></li></ul></li></ul><p>一般游戏引擎为了减少batch和处理光照的次数，会<strong>让每个物体受影响的光源有上限</strong>（3个或4个），并且<strong>一般是在一个shader中直接处理多个光源</strong></p><h3 id="Single-Pass-Lighting">Single-Pass Lighting</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">For Each Object：</span><br><span class="line">  Render Object， apply all lighting in one shader</span><br></pre></td></tr></table></figure><ul><li><p>不可见的面会造成shading的浪费</p></li><li><p>难以管理多光源的情况</p><ul><li>不同光源情况就要单独写一个shader，费时费力</li></ul></li><li><p>难以和shadow合并</p><ul><li>如果使用Shadow Maps：容易爆显存</li></ul></li></ul><h3 id="Deferrd-Shading">Deferrd Shading</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">For Each Object:</span><br><span class="line">  Render lighting properties to &quot;G-buffer&quot;</span><br><span class="line">For Each Light:</span><br><span class="line">  framebuffer += brdf(G-buffer, light)</span><br></pre></td></tr></table></figure><ul><li>便于引擎管理，可以极大简化batch</li><li>与常见阴影技术融合度高</li><li>光照计算的复杂度达到“完美的$$O_{(1)}$$</li><li>可以处理从许许多多的小光源到一个大光源的情况</li></ul><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T3400-15.png" alt=""></p><p>目前大多数引擎为了支持多材质，会有一个字节用于写入Material ID或者Shading Model ID(UE4)来区分不同的着色模型</p><h3 id="Tiled-Deferred-Shading">Tiled Deferred Shading</h3><p>（TBDR看后续的百人计划吧，感觉那篇知乎这里写得怪怪的）</p><p>在Deferred Shading的基础上按一定像素大小分块，计算每一块中光源的数量，这样我们可以对多个光源计算光照时只读取一次GBuffer信息，节省了带宽</p><h3 id="Deferred-Lighting">Deferred Lighting</h3><p>算法和Deferred Shading差不多，CryEngine早期版本中使用过该技术，目前基本没有引擎使用</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T3400-16.png" alt=""></p><h3 id="Forward-（Tiled-Forward-Rendering）">Forward+（Tiled Forward Rendering）</h3><p>最初是从AMD的论文Forward+: Bringing Deferred Lighting to the Next Level开始流行起来的</p><p>【Forward+管线总览】</p><ul><li><p>Depth prepass</p><ul><li><p>Fill Z Buffer</p></li><li><p>避免shading时的overdraw</p></li><li><p>用于为light culling而进行的像素位置重构</p></li></ul></li><li><p>Lighting culling</p><ul><li><p>逐个tile进行cull light</p></li><li><p>输入：z buffer，light buffer</p></li><li><p>输出：逐tile的light</p></li></ul></li><li><p>Shading</p><ul><li><p>几何信息已经被渲染了</p></li><li><p>Pixel Shader：</p><ul><li><p>迭代在light culling中计算得到的light list</p></li><li><p>计算这些光照的材质表现</p></li></ul></li></ul></li></ul><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T3400-11.png" alt="Forward+管线总览"></p><p><strong>【Light Culling的细节】</strong></p><p>使用Compute Shader来实现，细节还是后续想要去写的时候看看论文吧</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T3400-17.png" alt=""></p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T3400-18.png" alt=""></p><h3 id="Clustered-Forward-Rendering">Clustered Forward Rendering</h3><p>在Forward+ Rendering的基础上又沿着相机深度的方向切了很多片</p><ul><li>Tiles被替换为了3d空间中的cluster</li><li>深度的分布不是线性的（猜测是信息优先级的问题，就好像屏幕空间的深度信息也不是线性的一样，用更多信息去存储高精度的近区域的深度，用更少信息去存远区域的深度）</li><li>比起forward+来说，逐个cluster有更少的光照</li><li>但是cluster的数量大于tiles的数量</li></ul><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T3400-19.png" alt=""></p><p><strong>【Clusters的形成（Fill Clusters）】</strong></p><ul><li><p>由异步的compute shader来fill clusters</p></li><li><p>每个cluster都被所有light进行测试</p></li><li><p>3个pass</p></li><li><p>两个等级的层级</p><ul><li><p>18x10x32</p></li><li><p>36x20x64</p></li></ul></li></ul><p><strong>【Clustering与深度】</strong></p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T3400-20.png" alt=""></p><h3 id="Clustered-Deferred-Rendering">Clustered Deferred Rendering</h3><p>是延迟渲染的管线，只不过用了和forward passes一样的光照解决方案</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T3400-21.png" alt=""></p><h3 id="总结">总结</h3><p>整体上我只是做了一个了解，并且纸上得来终觉浅，等之后去细实现的时候，再去好好读一读论文，实际落地并做出笔记。渲染管线的发展总结就借用文中大佬的总结来概述一下：最开始只有传统的Forward Rendering，为了解决<strong>多光源的问题</strong>，引入了Deferred Rendering，带来了很多好处，包括各种后期效果。但是<strong>带宽是个问题</strong>，于是出现了Forward Plus(Tiled) Rendering解决带宽问题，也就有了Tiled Deferred Shading，但是还可以<strong>进一步优化</strong>，那就是在Tile的基础上再切片，于是就有了Clustered Forward Rendering和Clustered Deferred Rendering</p><p>整个技术都是在基于需求进行的推进，学习亦然。</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T3400-22.png" alt="大佬整理的各个管线的优劣"></p>]]></content>
    
    
    <summary type="html">延迟渲染与前向渲染的简述，以及额外的其他光照算法的概述</summary>
    
    
    
    <category term="Notes" scheme="https://himoqiuhan.github.io/categories/Notes/"/>
    
    
    <category term="TA" scheme="https://himoqiuhan.github.io/tags/TA/"/>
    
    <category term="百人计划" scheme="https://himoqiuhan.github.io/tags/%E7%99%BE%E4%BA%BA%E8%AE%A1%E5%88%92/"/>
    
    <category term="Shader" scheme="https://himoqiuhan.github.io/tags/Shader/"/>
    
  </entry>
  
  <entry>
    <title>技术美术百人计划学习笔记（图形3.3 曲面细分与几何着色器）</title>
    <link href="https://himoqiuhan.github.io/2023/08/10/Notes-TA100-T3300/"/>
    <id>https://himoqiuhan.github.io/2023/08/10/Notes-TA100-T3300/</id>
    <published>2023-08-10T05:01:11.000Z</published>
    <updated>2024-07-27T15:13:59.654Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T3300.png" alt=""></p><h2 id="应用">应用</h2><h3 id="曲面细分着色器">曲面细分着色器</h3><ul><li><p>海浪、雪地等</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T3300-1.png" alt=""></p></li><li><p>与DP贴图结合</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T3300-2.png" alt=""></p></li></ul><p>相较于直接增加模型的面数，曲面细分着色器可以在游戏进行时，根据自定义的规则（距离等），动态调整模型的复杂度，可以带来更好的性能。</p><h3 id="几何着色器">几何着色器</h3><ul><li><p>几何动画</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T3300-3.png" alt=""></p></li><li><p>草地（与曲面细分着色器结合，动态调整草地的疏密）</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T3300-4.png" alt=""></p></li></ul><h2 id="着色器执行顺序">着色器执行顺序</h2><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T3300-5.png" alt=""></p><p>Hull Shader：可编程，定义细分的参数</p><p>Tessellation Primitive Generator：不可编程不可控制</p><p>Domain Shader：曲面细分得到的点是在重心空间的，需要在Domain Shader中将点转换到我们要用的空间中</p><h2 id="TESS">TESS</h2><h3 id="输入和输出">输入和输出</h3><ul><li>输入：Patch，可以看作是多个顶点的集合，包含每个顶点的属性，可以指定一个Patch包含的顶点数以及自己的属性</li><li>功能：将图元细分（可以是三角形、矩形等）</li><li>输出：细分后的顶点</li></ul><h3 id="TESS流程">TESS流程</h3><ul><li><p>Hull Shader</p><ul><li><p>决定细分的数量（设定Tessellation Factor和Inside Tessellation Factor）</p></li><li><p>对输入的Patch参数进行改变（按需）</p></li></ul></li><li><p>Tessellation Primitive Generation</p><ul><li>进行细分操作</li></ul></li><li><p>Domain Shader</p><ul><li>对细分后的点进行处理，从重心空间（Barycentric Coordinate System）转换到屏幕空间</li></ul></li></ul><h3 id="Hull-Shader参数解析">Hull Shader参数解析</h3><ul><li><p>Tessellation Factor：决定将一条边分成几个部分，切分方法有：</p><ul><li><p>equal_spacing：进行等分</p></li><li><p>fractional_even_spacing，fractional_odd_spacing：从两端开始细分，到中间部分逐渐平均。目的是为了让细分更平滑。两者区别在于，even一定在中点部位有一个点（点的数量是奇数），odd则没有中点（点的数量是偶数）</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T3300-6.png" alt=""></p></li></ul></li><li><p>Inner Tessellation Factor：控制内部图形的切割方式</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T3300-7.png" alt=""></p></li></ul><h2 id="GS">GS</h2><h3 id="输入与输出">输入与输出</h3><ul><li>输入：图元（三角形、矩形、线等），据图元的不同，shader中会出现对应不同数量的顶点</li><li>输出：输出同样为图元，一个或多个，需要自己从定点构建，顺序很重要；同时需要定义最大输出的顶点数</li></ul>]]></content>
    
    
    <summary type="html">介绍曲面细分着色器与几何着色器，并介绍对两个着色器可配置阶段的控制</summary>
    
    
    
    <category term="Notes" scheme="https://himoqiuhan.github.io/categories/Notes/"/>
    
    
    <category term="TA" scheme="https://himoqiuhan.github.io/tags/TA/"/>
    
    <category term="百人计划" scheme="https://himoqiuhan.github.io/tags/%E7%99%BE%E4%BA%BA%E8%AE%A1%E5%88%92/"/>
    
    <category term="Shader" scheme="https://himoqiuhan.github.io/tags/Shader/"/>
    
  </entry>
  
  <entry>
    <title>技术美术百人计划学习笔记（图形3.2 混合模式及剔除）</title>
    <link href="https://himoqiuhan.github.io/2023/08/10/Notes-TA100-T3200/"/>
    <id>https://himoqiuhan.github.io/2023/08/10/Notes-TA100-T3200/</id>
    <published>2023-08-10T04:01:11.000Z</published>
    <updated>2024-07-27T15:13:59.654Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T3200.png" alt=""></p><h2 id="什么是混合模式">什么是混合模式</h2><h3 id="混合的定义">混合的定义</h3><p>混合就是把两种颜色混在一起，具体就是某一像素原位置的颜色与将要画上去的颜色，通过某种方式或算法混在一起，从而实现新的效果。例如PS中的叠加、正片叠底、滤色</p><h3 id="混合模式">混合模式</h3><p><strong>最终颜色 = Shader计算后的颜色值 * ScrFactor + 累计颜色 * DstFactor</strong></p><p>累计颜色可以理解为G-buffer中的像素，混合模式控制的就是ScrFactor和DstFactor</p><p>脚本中会看到的是<code>Blend SrcFactor DstFactor</code></p><p><strong>混合模式是Material层面的控制</strong></p><h2 id="混合模式的类型">混合模式的类型</h2><h3 id="PS中的混合模式">PS中的混合模式</h3><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T3200-1.png" alt=""></p><h3 id="ShaderLab的混合">ShaderLab的混合</h3><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T3200-2.png" alt=""></p><ol><li>如果颜色某一分量超过1，则会被自动截取为1，不需要考虑越界的问题</li><li>在所有着色器执行完毕，所有纹理都被应用，所有像素准备被呈现到屏幕之后，使用Blend命令来操作这些像素进行混合</li><li>语法：</li></ol><ul><li><p>Blend Off：关闭Blend（默认）</p></li><li><p>Blend SrcFactor DstFactor</p><ul><li><p>配置并启用混合</p></li><li><p>shader计算的颜色 * SrcFactor + 已经在target buffer的颜色 * DstFactor</p></li></ul></li><li><p>Blend SrcFactor DstFactor, SrcFactorA DstFactorA</p><ul><li>与上同理，但是使用不同的factor来混合alpha</li></ul></li><li><p>BlendOp Value</p><ul><li>设置Blend的计算公式，默认为Add</li></ul></li><li><p>BlendOp OpColor， OpAlpha</p><ul><li>同上，但是对Alpha进行不同处理</li></ul></li><li><p>AlphaToMaskOn</p><ul><li>常用在开启多重渲染（MSAA）的地表植被的渲染</li></ul></li></ul><p>官方文档：<a href="https://docs.unity3d.com/cn/2018.4/Manual/SL-Blend.html">https://docs.unity3d.com/cn/2018.4/Manual/SL-Blend.html</a></p><h3 id="总结">总结</h3><ol><li>Blend命令：启用会禁用GPU上的一些优化（主要是隐藏表面去除Early-Z），会导致GPU开销增加</li><li>混合操作默认为Add，如果使用BlendOp命令，则混合操作将设置为该方式</li><li>混合方程：FinalValue = SrcFactor * SrcValue [operation] DstFactor * DstValue</li><li>单独的RGB和Alpha混合与高级OpenGL混合操作不兼容</li></ol><p>【常用非高级混合命令】：</p><ul><li><p>Alpha混合：Blend SrcAlpha OneMinusSrcAlpha</p></li><li><p>Addtive相加混合：Blend One One</p></li><li><p>柔和相加混合：Blend One OneMinusDstFactor</p></li><li><p>相乘混合：Blend DstColor Zero</p></li><li><p>2倍相乘混合：Blend DstColor SrcColor</p></li><li><p>预乘透明度混合：Blend One OneMinusSrcAlpha</p></li></ul><h2 id="混合模式的实现方式">混合模式的实现方式</h2><h3 id="Unity中使用自带枚举控制">Unity中使用自带枚举控制</h3><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/TA100T3200-3.png" alt=""></p><h3 id="PS混合模式实现方式">PS混合模式实现方式</h3><p>【以下是最常用的】</p><ul><li><p>alpha：使用当前颜色的alpha值进行混合</p></li><li><p>darken：对两个颜色乘1取两者的最小值</p><ul><li>语法：BlendOp Min，Blend One One</li></ul></li><li><p>Multipy正片叠底：当前颜色 * 缓存颜色 + 缓存颜色 * 0</p><ul><li>语法：Blend Add， Blend DstColor Zero</li></ul></li><li><p>Screen滤色：当前颜色 * （1 -  缓存颜色） + 缓存颜色 * 1</p><ul><li>语法：Blend Add，Blend OneMinusDstColor One</li></ul></li><li><p>Lighten变亮：对两个颜色乘1取最大值</p><ul><li>语法：BlendOp Max，Blend One One</li></ul></li><li><p>LinearDodge线性减淡：缓存颜色 * 1 + 当前颜色 * 1</p><ul><li>语法：Blend One One</li></ul></li><li><p>ColorBurn颜色加深：（高级OpenGL混合，只支持OpenGL平台）</p><ul><li>目前只在具有GL_KHR_blend_equaion_advanced或GL_NV_blend_equation_advanced扩展支持的OpenGL硬件上可用</li></ul></li></ul><h2 id="剔除">剔除</h2><h3 id="法线剔除">法线剔除</h3><p>也被称为<strong>背面消隐</strong>，根据法线朝向判断哪个面被剔除掉，可以控制是否双面渲染。</p><p><strong>也是Material层面的控制</strong></p><p>【语法】</p><p><code>Cull + &lt;Off/Front/Back&gt;</code>（默认为Back）</p><p>【Unity内置的剔除模式的枚举】</p><p><code>[Enum(UnityEngine.Rendering.CullMode)]_CullMode(&quot;Cull Mode&quot;, float) = 2</code></p><h3 id="面裁切">面裁切</h3><p>Clip函数会将参数小于值的片元直接在片元阶段丢弃掉，常用于制作溶解、裁剪等效果</p><p>【语法】</p><p><code>Clip();</code>（默认会切掉0.5的部分）</p><p>【Clip背后的命令实际上是if】</p><figure class="highlight glsl"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (input.posInObjectCoords.y &gt; <span class="number">0.5</span>)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="keyword">discard</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="总结-2">总结</h3><ol><li>开启双面渲染（Cull Off）相当于绘制了两次–Overdraw</li><li>Clip函数在某些PowerVR的机型上效率很低</li><li>面裁切Clip最好是使用AlphaTest的Queue（2450）</li></ol>]]></content>
    
    
    <summary type="html">介绍Unity及PS中的混合模式，以及剔除相关的知识</summary>
    
    
    
    <category term="Notes" scheme="https://himoqiuhan.github.io/categories/Notes/"/>
    
    
    <category term="TA" scheme="https://himoqiuhan.github.io/tags/TA/"/>
    
    <category term="百人计划" scheme="https://himoqiuhan.github.io/tags/%E7%99%BE%E4%BA%BA%E8%AE%A1%E5%88%92/"/>
    
    <category term="Shader" scheme="https://himoqiuhan.github.io/tags/Shader/"/>
    
  </entry>
  
  <entry>
    <title>【Maya/Python】调用OpenMaya制作平滑法线工具</title>
    <link href="https://himoqiuhan.github.io/2023/08/03/Projects-Maya-PyForNormalSmoother/"/>
    <id>https://himoqiuhan.github.io/2023/08/03/Projects-Maya-PyForNormalSmoother/</id>
    <published>2023-08-03T01:27:17.000Z</published>
    <updated>2024-07-27T15:13:59.658Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>因为目前只是个人使用，所以还没有打包成一个插件，目前还是几段代码。等后续完善一下功能，再将其打包到一个插件中</p></blockquote><blockquote><p>使用的是Maya2023，基于Python3写的代码，不同Maya版本之间可能会不通用</p></blockquote><h2 id="作用效果">作用效果</h2><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230903094249292.png" alt="存入顶点色RGB三通道的切线空间平滑法线信息"></p><h2 id="代码源码">代码源码</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> maya.cmds <span class="keyword">as</span> cmds</span><br><span class="line"><span class="keyword">import</span> maya.api.OpenMaya <span class="keyword">as</span> om</span><br><span class="line"><span class="keyword">import</span> baseWindow</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SmoothNormalProcessor</span>(baseWindow.MainGui):</span><br><span class="line">    NormalOS = om.MColorArray()</span><br><span class="line">    NormalTS = om.MColorArray()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, name=<span class="string">&quot;SmoothNormal Processor&quot;</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SmoothNormalProcessor, <span class="variable language_">self</span>).__init__(name)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">buildUI</span>(<span class="params">self</span>):</span><br><span class="line">        cmds.columnLayout(adj=<span class="literal">True</span>)</span><br><span class="line">        cmds.button(label=<span class="string">&#x27;Smooth Normal To VertCol&#x27;</span>, ann=<span class="string">&#x27;Smooth Normal To VertCol&#x27;</span>, h=<span class="number">60</span>, w=<span class="number">300</span>, command=<span class="variable language_">self</span>.CalculateSmoothNormalToVerCol)</span><br><span class="line">        cmds.button(label=<span class="string">&#x27;Smooth Normal To Texture&#x27;</span>, ann=<span class="string">&#x27;Smooth Normal To Texture&#x27;</span>, h=<span class="number">60</span>, w=<span class="number">300</span>, command=<span class="variable language_">self</span>.CalculateSmoothNormalToTexture)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">CalculateSmoothNormalToVerCol</span>(<span class="params">self, *args</span>):</span><br><span class="line">        <span class="variable language_">self</span>.NormalOS.clear()</span><br><span class="line">        <span class="variable language_">self</span>.NormalTS.clear()</span><br><span class="line">        <span class="variable language_">self</span>.Execute(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">CalculateSmoothNormalToTexture</span>(<span class="params">self, *args</span>):</span><br><span class="line">        <span class="variable language_">self</span>.NormalOS.clear()</span><br><span class="line">        <span class="variable language_">self</span>.NormalTS.clear()</span><br><span class="line">        <span class="variable language_">self</span>.Execute(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Execute</span>(<span class="params">self, isToVertex</span>):</span><br><span class="line">        SelectionModels = cmds.ls(sl=<span class="literal">True</span>, l=<span class="literal">True</span>)</span><br><span class="line">        selectList = om.MGlobal.getActiveSelectionList()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> selectList.isEmpty():</span><br><span class="line">            om.MGlobal.displayError(<span class="string">&quot;You Need To Select A Target&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">if</span> selectList.length() &gt; <span class="number">1</span>:</span><br><span class="line">            om.MGlobal.displayError(<span class="string">&quot;You Can Just Handle One Target Once&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        uvSet = cmds.polyUVSet(SelectionModels[<span class="number">0</span>], query=<span class="literal">True</span>, currentUVSet=<span class="literal">True</span>)</span><br><span class="line">        colorSet = cmds.polyColorSet(SelectionModels[<span class="number">0</span>], query=<span class="literal">True</span>, currentColorSet=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">if</span> uvSet == <span class="literal">None</span>:</span><br><span class="line">            om.MGlobal.displayError(<span class="string">&quot;You Need To Creat A UV Set For Selected Model&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            uvSet = uvSet[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> colorSet == <span class="literal">None</span>:</span><br><span class="line">            om.MGlobal.displayError(<span class="string">&quot;You Need To Apply A Color Set For Selected Model&quot;</span>)</span><br><span class="line">            om.MGlobal.displayInfo(<span class="string">&quot;Method: Select Target -&gt; Mesh Display/Apply Color&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            colorSet = colorSet[<span class="number">0</span>]</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Color Set Exist: %s&quot;</span> % (colorSet))</span><br><span class="line"></span><br><span class="line">        dagPath = selectList.getDagPath(<span class="number">0</span>)</span><br><span class="line">        fnMesh = om.MFnMesh(dagPath)</span><br><span class="line">        fnMesh.setCurrentColorSetName(colorSet)</span><br><span class="line">        normals = fnMesh.getNormals()</span><br><span class="line">        itVerts = om.MItMeshVertex(dagPath)</span><br><span class="line">        <span class="variable language_">self</span>.NormalOS.setLength(fnMesh.numFaceVertices)</span><br><span class="line">        <span class="variable language_">self</span>.NormalTS.setLength(fnMesh.numFaceVertices)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.CalculateSmoothNormal(itVerts, normals, colorSet)</span><br><span class="line">        fnMesh.setColors(<span class="variable language_">self</span>.NormalOS, colorSet)</span><br><span class="line"></span><br><span class="line">        itFaceVerts = om.MItMeshFaceVertex(dagPath)</span><br><span class="line">        <span class="variable language_">self</span>.TransformToTangentAndStoreToVertCol(itFaceVerts, uvSet, colorSet)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> isToVertex:</span><br><span class="line">            fnMesh.setColors(<span class="variable language_">self</span>.NormalTS,colorSet)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;To Texture is Not Available&quot;</span>)</span><br><span class="line">            white = om.MColorArray()</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(fnMesh.numFaceVertices):</span><br><span class="line">                white.append(om.MColor([<span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>]))</span><br><span class="line">            fnMesh.setColors(white, colorSet)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">CalculateSmoothNormal</span>(<span class="params">self, itVerts, normals, colorSet</span>):</span><br><span class="line">        averageNormal = om.MFloatVector()</span><br><span class="line">        averageColor = om.MColor()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> itVerts.isDone():</span><br><span class="line">            associatedNormalIndices = itVerts.getNormalIndices()</span><br><span class="line">            <span class="keyword">for</span> normalIndex <span class="keyword">in</span> associatedNormalIndices:</span><br><span class="line">                averageNormal += normals[normalIndex]</span><br><span class="line">            averageNormal.normalize()</span><br><span class="line"></span><br><span class="line">            colorIndices = itVerts.getColorIndices(colorSet)</span><br><span class="line">            <span class="keyword">for</span> colorIndex <span class="keyword">in</span> colorIndices:</span><br><span class="line">                averageColor.r = averageNormal.x</span><br><span class="line">                averageColor.g = averageNormal.y</span><br><span class="line">                averageColor.b = averageNormal.z</span><br><span class="line">                averageColor.a = <span class="number">1.0</span></span><br><span class="line">                <span class="variable language_">self</span>.NormalOS.__setitem__(colorIndex, averageColor)</span><br><span class="line">            averageNormal = om.MFloatVector(<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>)</span><br><span class="line">            itVerts.<span class="built_in">next</span>()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;ObjectSpace Smooth Normal Calculation Finished&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">TransformToTangentAndStoreToVertCol</span>(<span class="params">self, itFaceVerts, uvSetName, colorSetName</span>):</span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> itFaceVerts.isDone():</span><br><span class="line">            tangent = itFaceVerts.getTangent(uvSet=uvSetName)</span><br><span class="line">            bitangent = itFaceVerts.getBinormal(uvSet=uvSetName)</span><br><span class="line">            normal = itFaceVerts.getNormal()</span><br><span class="line"></span><br><span class="line">            col = itFaceVerts.getColor(colorSetName)</span><br><span class="line">            colIndex = itFaceVerts.getColorIndex(colorSet=colorSetName)</span><br><span class="line"></span><br><span class="line">            averageColor = om.MColor()</span><br><span class="line">            averageNormal = om.MVector(col[<span class="number">0</span>], col[<span class="number">1</span>], col[<span class="number">2</span>])</span><br><span class="line">            x = averageNormal * tangent</span><br><span class="line">            y = averageNormal * bitangent</span><br><span class="line">            z = averageNormal * normal</span><br><span class="line">            averageColor.r = <span class="number">0.5</span> * (x + <span class="number">1</span>)</span><br><span class="line">            averageColor.g = <span class="number">0.5</span> * (y + <span class="number">1</span>)</span><br><span class="line">            averageColor.b = <span class="number">0.5</span> * (z + <span class="number">1</span>)</span><br><span class="line">            averageColor.a = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="variable language_">self</span>.NormalTS.__setitem__(colIndex, averageColor)</span><br><span class="line">            itFaceVerts.<span class="built_in">next</span>()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;ObjectSpace To TangentSpace Transformation Finished&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="使用方法">使用方法</h2><ol><li><p>首先，需要给模型手动添加一个ColorSet，直接通过Maya自带的Apply Color即可完成（这一步是代码的Bug，但是目前我如果在代码中新建ColorSet会无法正常将信息赋到顶点色中，所以暂时需要手动解决一下）</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230903095630589.png" alt="为模型Apply Color"></p></li><li><p>复制以下代码到Python的代码窗口中</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> maya.cmds <span class="keyword">as</span> cmds</span><br><span class="line"><span class="keyword">import</span> maya.api.OpenMaya <span class="keyword">as</span> om</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MainGui</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, name=<span class="string">&#x27;QiuH Toolkit&#x27;</span></span>):</span><br><span class="line">        <span class="keyword">if</span> cmds.window(name, query=<span class="literal">True</span>, exists=<span class="literal">True</span>):</span><br><span class="line">            cmds.deleteUI(name)</span><br><span class="line"></span><br><span class="line">        cmds.window(name)</span><br><span class="line">        <span class="variable language_">self</span>.buildUI()</span><br><span class="line">        cmds.showWindow()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">buildUI</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;No UI Building&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SmoothNormalProcessor</span>(<span class="title class_ inherited__">MainGui</span>):</span><br><span class="line">    NormalOS = om.MColorArray()</span><br><span class="line">    NormalTS = om.MColorArray()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, name=<span class="string">&quot;SmoothNormal Processor&quot;</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SmoothNormalProcessor, <span class="variable language_">self</span>).__init__(name)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">buildUI</span>(<span class="params">self</span>):</span><br><span class="line">        cmds.columnLayout(adj=<span class="literal">True</span>)</span><br><span class="line">        cmds.button(label=<span class="string">&#x27;Smooth Normal To VertCol&#x27;</span>, ann=<span class="string">&#x27;Smooth Normal To VertCol&#x27;</span>, h=<span class="number">60</span>, w=<span class="number">300</span>, command=<span class="variable language_">self</span>.CalculateSmoothNormalToVerCol)</span><br><span class="line">        cmds.button(label=<span class="string">&#x27;Smooth Normal To Texture&#x27;</span>, ann=<span class="string">&#x27;Smooth Normal To Texture&#x27;</span>, h=<span class="number">60</span>, w=<span class="number">300</span>, command=<span class="variable language_">self</span>.CalculateSmoothNormalToTexture)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">CalculateSmoothNormalToVerCol</span>(<span class="params">self, *args</span>):</span><br><span class="line">        <span class="variable language_">self</span>.NormalOS.clear()</span><br><span class="line">        <span class="variable language_">self</span>.NormalTS.clear()</span><br><span class="line">        <span class="variable language_">self</span>.Execute(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">CalculateSmoothNormalToTexture</span>(<span class="params">self, *args</span>):</span><br><span class="line">        <span class="variable language_">self</span>.NormalOS.clear()</span><br><span class="line">        <span class="variable language_">self</span>.NormalTS.clear()</span><br><span class="line">        <span class="variable language_">self</span>.Execute(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Execute</span>(<span class="params">self, isToVertex</span>):</span><br><span class="line">        SelectionModels = cmds.ls(sl=<span class="literal">True</span>, l=<span class="literal">True</span>)</span><br><span class="line">        selectList = om.MGlobal.getActiveSelectionList()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> selectList.isEmpty():</span><br><span class="line">            om.MGlobal.displayError(<span class="string">&quot;You Need To Select A Target&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">if</span> selectList.length() &gt; <span class="number">1</span>:</span><br><span class="line">            om.MGlobal.displayError(<span class="string">&quot;You Can Just Handle One Target Once&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        uvSet = cmds.polyUVSet(SelectionModels[<span class="number">0</span>], query=<span class="literal">True</span>, currentUVSet=<span class="literal">True</span>)</span><br><span class="line">        colorSet = cmds.polyColorSet(SelectionModels[<span class="number">0</span>], query=<span class="literal">True</span>, currentColorSet=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">if</span> uvSet == <span class="literal">None</span>:</span><br><span class="line">            om.MGlobal.displayError(<span class="string">&quot;You Need To Creat A UV Set For Selected Model&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            uvSet = uvSet[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> colorSet == <span class="literal">None</span>:</span><br><span class="line">            om.MGlobal.displayError(<span class="string">&quot;You Need To Apply A Color Set For Selected Model&quot;</span>)</span><br><span class="line">            om.MGlobal.displayInfo(<span class="string">&quot;Method: Select Target -&gt; Mesh Display/Apply Color&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            colorSet = colorSet[<span class="number">0</span>]</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Color Set Exist: %s&quot;</span> % (colorSet))</span><br><span class="line"></span><br><span class="line">        dagPath = selectList.getDagPath(<span class="number">0</span>)</span><br><span class="line">        fnMesh = om.MFnMesh(dagPath)</span><br><span class="line">        fnMesh.setCurrentColorSetName(colorSet)</span><br><span class="line">        normals = fnMesh.getNormals()</span><br><span class="line">        itVerts = om.MItMeshVertex(dagPath)</span><br><span class="line">        <span class="variable language_">self</span>.NormalOS.setLength(fnMesh.numFaceVertices)</span><br><span class="line">        <span class="variable language_">self</span>.NormalTS.setLength(fnMesh.numFaceVertices)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.CalculateSmoothNormal(itVerts, normals, colorSet)</span><br><span class="line">        fnMesh.setColors(<span class="variable language_">self</span>.NormalOS, colorSet)</span><br><span class="line"></span><br><span class="line">        itFaceVerts = om.MItMeshFaceVertex(dagPath)</span><br><span class="line">        <span class="variable language_">self</span>.TransformToTangentAndStoreToVertCol(itFaceVerts, uvSet, colorSet)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> isToVertex:</span><br><span class="line">            fnMesh.setColors(<span class="variable language_">self</span>.NormalTS,colorSet)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;To Texture is Not Available&quot;</span>)</span><br><span class="line">            white = om.MColorArray()</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(fnMesh.numFaceVertices):</span><br><span class="line">                white.append(om.MColor([<span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>]))</span><br><span class="line">            fnMesh.setColors(white, colorSet)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">CalculateSmoothNormal</span>(<span class="params">self, itVerts, normals, colorSet</span>):</span><br><span class="line">        averageNormal = om.MFloatVector()</span><br><span class="line">        averageColor = om.MColor()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> itVerts.isDone():</span><br><span class="line">            associatedNormalIndices = itVerts.getNormalIndices()</span><br><span class="line">            <span class="keyword">for</span> normalIndex <span class="keyword">in</span> associatedNormalIndices:</span><br><span class="line">                averageNormal += normals[normalIndex]</span><br><span class="line">            averageNormal.normalize()</span><br><span class="line"></span><br><span class="line">            colorIndices = itVerts.getColorIndices(colorSet)</span><br><span class="line">            <span class="keyword">for</span> colorIndex <span class="keyword">in</span> colorIndices:</span><br><span class="line">                averageColor.r = averageNormal.x</span><br><span class="line">                averageColor.g = averageNormal.y</span><br><span class="line">                averageColor.b = averageNormal.z</span><br><span class="line">                averageColor.a = <span class="number">1.0</span></span><br><span class="line">                <span class="variable language_">self</span>.NormalOS.__setitem__(colorIndex, averageColor)</span><br><span class="line">            averageNormal = om.MFloatVector(<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>)</span><br><span class="line">            itVerts.<span class="built_in">next</span>()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;ObjectSpace Smooth Normal Calculation Finished&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">TransformToTangentAndStoreToVertCol</span>(<span class="params">self, itFaceVerts, uvSetName, colorSetName</span>):</span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> itFaceVerts.isDone():</span><br><span class="line">            tangent = itFaceVerts.getTangent(uvSet=uvSetName)</span><br><span class="line">            bitangent = itFaceVerts.getBinormal(uvSet=uvSetName)</span><br><span class="line">            normal = itFaceVerts.getNormal()</span><br><span class="line"></span><br><span class="line">            col = itFaceVerts.getColor(colorSetName)</span><br><span class="line">            colIndex = itFaceVerts.getColorIndex(colorSet=colorSetName)</span><br><span class="line"></span><br><span class="line">            averageColor = om.MColor()</span><br><span class="line">            averageNormal = om.MVector(col[<span class="number">0</span>], col[<span class="number">1</span>], col[<span class="number">2</span>])</span><br><span class="line">            x = averageNormal * tangent</span><br><span class="line">            y = averageNormal * bitangent</span><br><span class="line">            z = averageNormal * normal</span><br><span class="line">            averageColor.r = <span class="number">0.5</span> * (x + <span class="number">1</span>)</span><br><span class="line">            averageColor.g = <span class="number">0.5</span> * (y + <span class="number">1</span>)</span><br><span class="line">            averageColor.b = <span class="number">0.5</span> * (z + <span class="number">1</span>)</span><br><span class="line">            averageColor.a = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="variable language_">self</span>.NormalTS.__setitem__(colIndex, averageColor)</span><br><span class="line">            itFaceVerts.<span class="built_in">next</span>()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;ObjectSpace To TangentSpace Transformation Finished&quot;</span>)</span><br><span class="line"></span><br><span class="line">window = smoothNormal.SmoothNormalProcessor()</span><br></pre></td></tr></table></figure></li><li><p>按下Crtl + Enter执行代码</p></li><li><p>选中模型，选择想要运行的模式（目前只确保了存入顶点色的程序的稳定性，存入贴图的版本还在完善中，暂不可用）</p><p><img src="http://qiuhanblog-imgsubmit.oss-cn-beijing.aliyuncs.com/img/image-20230903100501179.png" alt="简易窗口"></p></li><li><p>等待几秒即可计算并写入完毕，后续导出的模型顶点色就带有平滑法线信息</p></li></ol><h2 id="代码解释-带注释版本的源码">代码解释(带注释版本的源码)</h2><blockquote><p>代码是在学习这位大佬源码的基础上进行一些自己的改编完成的，十分推荐大家去学习这位大佬的文章：<a href="https://zhuanlan.zhihu.com/p/538660626">https://zhuanlan.zhihu.com/p/538660626</a></p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> maya.cmds <span class="keyword">as</span> cmds</span><br><span class="line"><span class="keyword">import</span> maya.api.OpenMaya <span class="keyword">as</span> om</span><br><span class="line"><span class="keyword">import</span> baseWindow</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SmoothNormalProcessor</span>(baseWindow.MainGui):</span><br><span class="line">    NormalOS = om.MColorArray()</span><br><span class="line">    NormalTS = om.MColorArray()</span><br><span class="line">    <span class="comment"># 类变量，用openmaya的MColorArray类来存储对象空间和切线空间下的平滑法线</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, name=<span class="string">&quot;SmoothNormal Processor&quot;</span></span>):</span><br><span class="line">        <span class="comment"># override MainGui类中的构造函数，以SmoothNormal Processor作为窗口名称回调父类的构造函数</span></span><br><span class="line">        <span class="built_in">super</span>(SmoothNormalProcessor, <span class="variable language_">self</span>).__init__(name)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">buildUI</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 窗口中UI（两个Button）的创建</span></span><br><span class="line">        cmds.columnLayout(adj=<span class="literal">True</span>)</span><br><span class="line">        cmds.button(label=<span class="string">&#x27;Smooth Normal To VertCol&#x27;</span>, ann=<span class="string">&#x27;Smooth Normal To VertCol&#x27;</span>, h=<span class="number">60</span>, w=<span class="number">300</span>, command=<span class="variable language_">self</span>.CalculateSmoothNormalToVerCol)</span><br><span class="line">        cmds.button(label=<span class="string">&#x27;Smooth Normal To Texture&#x27;</span>, ann=<span class="string">&#x27;Smooth Normal To Texture&#x27;</span>, h=<span class="number">60</span>, w=<span class="number">300</span>, command=<span class="variable language_">self</span>.CalculateSmoothNormalToTexture)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">CalculateSmoothNormalToVerCol</span>(<span class="params">self, *args</span>):</span><br><span class="line">        <span class="variable language_">self</span>.NormalOS.clear()</span><br><span class="line">        <span class="variable language_">self</span>.NormalTS.clear()</span><br><span class="line">        <span class="variable language_">self</span>.Execute(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">CalculateSmoothNormalToTexture</span>(<span class="params">self, *args</span>):</span><br><span class="line">        <span class="comment"># 先清空两个类变量，然后再执行程序</span></span><br><span class="line">        <span class="variable language_">self</span>.NormalOS.clear()</span><br><span class="line">        <span class="variable language_">self</span>.NormalTS.clear()</span><br><span class="line">        <span class="variable language_">self</span>.Execute(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Execute</span>(<span class="params">self, isToVertex</span>):</span><br><span class="line">        SelectionModels = cmds.ls(sl=<span class="literal">True</span>, l=<span class="literal">True</span>)</span><br><span class="line">        selectList = om.MGlobal.getActiveSelectionList()</span><br><span class="line">        <span class="comment"># Return an MSelectionList containing the nodes, components and plugs currently selected in Maya</span></span><br><span class="line">    <span class="comment"># 返回的是一个选择列表MSelectionList，MSelectionList是一个MObject、MPlug、MDagPath的异构列表</span></span><br><span class="line">    <span class="comment"># 需要通过遍历选择列表，通过利用遍历的index获取DAGPath，然后利用DAGPath来获取到MFnMesh</span></span><br><span class="line">    <span class="comment"># MFnMesh类中存储了许多网格体的信息</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 限制同时处理的对象（有且只能有1个）</span></span><br><span class="line">        <span class="keyword">if</span> selectList.isEmpty():</span><br><span class="line">            om.MGlobal.displayError(<span class="string">&quot;You Need To Select A Target&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">if</span> selectList.length() &gt; <span class="number">1</span>:</span><br><span class="line">            om.MGlobal.displayError(<span class="string">&quot;You Can Just Handle One Target Once&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 检查选中的对象是否有Colorset和UVset</span></span><br><span class="line">        uvSet = cmds.polyUVSet(SelectionModels[<span class="number">0</span>], query=<span class="literal">True</span>, currentUVSet=<span class="literal">True</span>)</span><br><span class="line">        colorSet = cmds.polyColorSet(SelectionModels[<span class="number">0</span>], query=<span class="literal">True</span>, currentColorSet=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">if</span> uvSet == <span class="literal">None</span>:</span><br><span class="line">            om.MGlobal.displayError(<span class="string">&quot;You Need To Creat A UV Set For Selected Model&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            uvSet = uvSet[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> colorSet == <span class="literal">None</span>:</span><br><span class="line">            om.MGlobal.displayError(<span class="string">&quot;You Need To Apply A Color Set For Selected Model&quot;</span>)</span><br><span class="line">            om.MGlobal.displayInfo(<span class="string">&quot;Method: Select Target -&gt; Mesh Display/Apply Color&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            colorSet = colorSet[<span class="number">0</span>]</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Color Set Exist: %s&quot;</span> % (colorSet))</span><br><span class="line"></span><br><span class="line">        dagPath = selectList.getDagPath(<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># DAG是Directed Acyclic Graph的缩写</span></span><br><span class="line">    <span class="comment"># DAG节点是Maya场景中的节点，包括transform和shape两种类型的节点。</span></span><br><span class="line">    <span class="comment"># transform节点提供位置、旋转、缩放等信息，并且可以有子节点；shape节点只提供几何信息，并且没有子节点。</span></span><br><span class="line">    <span class="comment"># 可以把dagPath理解为指向一个特定模型对象的路径，通过这个路径可以获取有关这个物体的很多信息，并以此填充到特定数据结构（类）中</span></span><br><span class="line">        fnMesh = om.MFnMesh(dagPath)</span><br><span class="line">        <span class="comment"># 通过dagPath路径获得多边形网格模型</span></span><br><span class="line">        fnMesh.setCurrentColorSetName(colorSet)</span><br><span class="line">        normals = fnMesh.getNormals()</span><br><span class="line">        <span class="comment"># Returns a copy of the mesh&#x27;s normals. The normals are the per-polygon per-vertex normals.</span></span><br><span class="line">    <span class="comment"># 即获取模型的法线信息，返回值是MFloatVectorArray，如果要获得特定顶点的法线，需要通过getFaceNormalIds()来获得数组的索引</span></span><br><span class="line">        itVerts = om.MItMeshVertex(dagPath)</span><br><span class="line">        <span class="comment"># 获取多边形网格模型的顶点迭代器vertex iterator</span></span><br><span class="line">        <span class="variable language_">self</span>.NormalOS.setLength(fnMesh.numFaceVertices)</span><br><span class="line">        <span class="variable language_">self</span>.NormalTS.setLength(fnMesh.numFaceVertices)</span><br><span class="line">        <span class="comment"># 基于顶点数量来设置之前创建的数组的长度</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 因为只有Vertex才能读取到与顶点相关联的法线的索引，所以由vertex来计算模型空间下的平滑法线信息：</span></span><br><span class="line">        <span class="variable language_">self</span>.CalculateSmoothNormal(itVerts, normals, colorSet)</span><br><span class="line">        fnMesh.setColors(<span class="variable language_">self</span>.NormalOS, colorSet)</span><br><span class="line">        <span class="comment"># 通过遍历顶点来计算模型空间下的顶点平滑法线，并写入顶点色中</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 因为只有FaceVertex才能读取到切线、副切线的信息，所以由FaceVertex来将模型空间下的法线信息转换到切线空间：</span></span><br><span class="line">        itFaceVerts = om.MItMeshFaceVertex(dagPath)</span><br><span class="line">        <span class="comment"># 获取FaceVertex的迭代器</span></span><br><span class="line">        <span class="variable language_">self</span>.TransformToTangentAndStoreToVertCol(itFaceVerts, uvSet, colorSet)</span><br><span class="line"><span class="comment"># 将之前计算好的平滑法线信息转换到切线空间中，并最终存储到顶点色的RGB通道中</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 区分是将结果写入顶点色还是写入贴图</span></span><br><span class="line">        <span class="keyword">if</span> isToVertex:</span><br><span class="line">            fnMesh.setColors(<span class="variable language_">self</span>.NormalTS,colorSet)</span><br><span class="line">            <span class="comment"># 设置顶点色</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;To Texture is Not Available&quot;</span>)</span><br><span class="line">            white = om.MColorArray()</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(fnMesh.numFaceVertices):</span><br><span class="line">                white.append(om.MColor([<span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>]))</span><br><span class="line">            fnMesh.setColors(white, colorSet)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">CalculateSmoothNormal</span>(<span class="params">self, itVerts, normals, colorSet</span>):</span><br><span class="line">        averageNormal = om.MFloatVector()</span><br><span class="line">        averageColor = om.MColor()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> itVerts.isDone():</span><br><span class="line">            <span class="comment"># 逐个顶点进行计算和顶点色写入</span></span><br><span class="line">            associatedNormalIndices = itVerts.getNormalIndices()</span><br><span class="line">            <span class="comment"># This method returns the normal indices of the face/vertex associated with the current vertex</span></span><br><span class="line">        <span class="comment"># 返回与这个顶点相关联的顶点、面的法线索引（平均值这不仅来了吗！）</span></span><br><span class="line">            <span class="keyword">for</span> normalIndex <span class="keyword">in</span> associatedNormalIndices:</span><br><span class="line">                averageNormal += normals[normalIndex]</span><br><span class="line">            averageNormal.normalize()</span><br><span class="line">            <span class="comment"># 求和取平均，直接使用MFloatVector类带的normalize即可</span></span><br><span class="line"></span><br><span class="line">            colorIndices = itVerts.getColorIndices(colorSet)</span><br><span class="line">            <span class="comment"># This method returns the colorIndices into the color array</span></span><br><span class="line">        <span class="comment"># 获取当前顶点的全部顶点色索引（一个顶点可能有多个顶点色索引，指向不同的颜色集合，这一点我认为是和一个顶点在Maya中可能有多个法线同理，导出后会变成多个顶点，但会与信息一一对应）</span></span><br><span class="line">            <span class="keyword">for</span> colorIndex <span class="keyword">in</span> colorIndices:</span><br><span class="line">                averageColor.r = averageNormal.x</span><br><span class="line">                averageColor.g = averageNormal.y</span><br><span class="line">                averageColor.b = averageNormal.z</span><br><span class="line">                averageColor.a = <span class="number">1.0</span></span><br><span class="line">                <span class="variable language_">self</span>.NormalOS.__setitem__(colorIndex, averageColor)</span><br><span class="line">                <span class="comment"># 写入NormalOS的Color数组，因为这套逻辑中设置顶点是直接统一用一个数组去Copy设置的，所以需要保证这里的索引值与顶点色中的索引值相同</span></span><br><span class="line">            <span class="comment"># 同样也可以一个一个去setColors</span></span><br><span class="line">            averageNormal = om.MFloatVector(<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>)</span><br><span class="line">            <span class="comment"># 重置一下averageNormal，然后继续计算下一个平滑法线颜色</span></span><br><span class="line">            itVerts.<span class="built_in">next</span>()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;ObjectSpace Smooth Normal Calculation Finished&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">TransformToTangentAndStoreToVertCol</span>(<span class="params">self, itFaceVerts, uvSetName, colorSetName</span>):</span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> itFaceVerts.isDone():</span><br><span class="line">            <span class="comment"># 使用面顶点迭代器遍历每个面顶点</span></span><br><span class="line">            tangent = itFaceVerts.getTangent(uvSet=uvSetName)</span><br><span class="line">            bitangent = itFaceVerts.getBinormal(uvSet=uvSetName)</span><br><span class="line">            normal = itFaceVerts.getNormal()</span><br><span class="line"><span class="comment"># 得到当前面顶点的切线和副切线、法线，用于构建TBN矩阵（只有FaceVertex才能获取到切线信息）</span></span><br><span class="line">            col = itFaceVerts.getColor(colorSetName)</span><br><span class="line">            colIndex = itFaceVerts.getColorIndex(colorSet=colorSetName)</span><br><span class="line">            <span class="comment"># 得到当前 面顶点的顶点色和顶点色索引，用于读取之前计算出的模型空间下的平滑法线信息</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 左乘TBN矩阵，将法线信息从模型空间转换到切线空间</span></span><br><span class="line">        <span class="comment"># （   tangent   ）    normal.x</span></span><br><span class="line">        <span class="comment"># （  bitangent  ） *  normal.y</span></span><br><span class="line">        <span class="comment"># （    normal   ）    normal.z</span></span><br><span class="line">            averageColor = om.MColor()</span><br><span class="line">            averageNormal = om.MVector(col[<span class="number">0</span>], col[<span class="number">1</span>], col[<span class="number">2</span>])</span><br><span class="line">            x = averageNormal * tangent</span><br><span class="line">            y = averageNormal * bitangent</span><br><span class="line">            z = averageNormal * normal</span><br><span class="line">            <span class="comment"># 因为计算出来的法线信息是在[-1,1]区间上的，而颜色信息是在[0,1]区间上的，所以存储前需要进行一次映射</span></span><br><span class="line">            averageColor.r = <span class="number">0.5</span> * (x + <span class="number">1</span>)</span><br><span class="line">            averageColor.g = <span class="number">0.5</span> * (y + <span class="number">1</span>)</span><br><span class="line">            averageColor.b = <span class="number">0.5</span> * (z + <span class="number">1</span>)</span><br><span class="line">            averageColor.a = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="variable language_">self</span>.NormalTS.__setitem__(colIndex, averageColor)</span><br><span class="line">            itFaceVerts.<span class="built_in">next</span>()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;ObjectSpace To TangentSpace Transformation Finished&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="学习心得">学习心得</h2><p>起初看了一些写Python代码的教程，是通过学习Python调用Maya.cmds来入门的Python For Maya，也跟着教程简单的写了几个小工具。到后面实际去做自己想要的工具时，发现cmds能够实现的功能很有限制，同时找到了那位大佬的文章，就去跟着他的代码学习了一下OpenMaya的库，基本上就是结合它的代码，一行一行地读，不会的就去官方文档查，并借助NewBing去寻找更多的资料。在全部理解完大佬的代码后，就参考着他的源码，结合之前学的一些创建可视化窗口和面向对象的知识，写出了这个工具</p><h2 id="学习资料">学习资料</h2><blockquote><p>只列出了一些重点资料，更多的知识自己基于需求去查询和学习</p></blockquote><p>【python+maya 脚本语法编写全面基础入门中文字幕视频教程】 <a href="https://www.bilibili.com/video/BV1d4411a7aH">https://www.bilibili.com/video/BV1d4411a7aH</a></p><p>【Maya工作流的平滑法线描边小工具 - 鲜虾云吞面的文章】 <a href="https://zhuanlan.zhihu.com/p/538660626">https://zhuanlan.zhihu.com/p/538660626</a></p><p>【Maya官方API文档 - OpenMaya】 <a href="https://help.autodesk.com/view/MAYAUL/2023/CHS/?guid=MAYA_API_REF_py_ref_namespace_open_maya_html">https://help.autodesk.com/view/MAYAUL/2023/CHS/?guid=MAYA_API_REF_py_ref_namespace_open_maya_html</a></p><p>【Maya官方API文档 - PyForMaya】 <a href="https://help.autodesk.com/view/MAYAUL/2023/CHS/?guid=__CommandsPython_index_html">https://help.autodesk.com/view/MAYAUL/2023/CHS/?guid=__CommandsPython_index_html</a></p>]]></content>
    
    
    <summary type="html">为了制作平滑描边，在Maya内制作了一个计算顶点平滑法线信息，并转换到切线空间，最后存入顶点色的小工具</summary>
    
    
    
    <category term="Portfolio" scheme="https://himoqiuhan.github.io/categories/Portfolio/"/>
    
    
    <category term="Maya" scheme="https://himoqiuhan.github.io/tags/Maya/"/>
    
    <category term="Python" scheme="https://himoqiuhan.github.io/tags/Python/"/>
    
    <category term="OpenMaya" scheme="https://himoqiuhan.github.io/tags/OpenMaya/"/>
    
    <category term="Maya.cmds" scheme="https://himoqiuhan.github.io/tags/Maya-cmds/"/>
    
  </entry>
  
</feed>
